{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkhar21/ML_4/blob/main/FER_Data_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_c0CQyLCKSS"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "3T7KxaYvByjT",
        "outputId": "c31637d9-9ead-476e-fa29-ccba0718bdb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1e511ba8-216c-4ec7-b93d-4d51ebf8b6da\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1e511ba8-216c-4ec7-b93d-4d51ebf8b6da\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!mkdir -p ~/.kaggle/\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBPjjhwlCRJc",
        "outputId": "3ee840bb-ab5e-43ea-b998-a1b1738858da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "challenges-in-representation-learning-facial-expression-recognition-challenge.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "replace example_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "!unzip challenges-in-representation-learning-facial-expression-recognition-challenge.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wz_aAV3VCUo8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "example_submission_df = pd.read_csv('example_submission.csv')\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XE3h_TB2CXzw",
        "outputId": "a328cd81-4c6a-4564-fbc8-0964640d819f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7178, 48, 48, 1), (28709, 48, 48, 1), (28709,))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert pixel strings to numpy arrays\n",
        "def process_pixels(pixels_str):\n",
        "    return np.array(pixels_str.split(), dtype='float32')\n",
        "\n",
        "# For training\n",
        "train_df['pixels'] = train_df['pixels'].apply(process_pixels)\n",
        "x_train = np.stack(train_df['pixels'].values)\n",
        "y_train = train_df['emotion'].values\n",
        "\n",
        "# For testing\n",
        "test_df['pixels'] = test_df['pixels'].apply(process_pixels)\n",
        "x_test = np.stack(test_df['pixels'].values)\n",
        "\n",
        "# Normalize and reshape\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "x_train = x_train.reshape(-1, 48, 48, 1)\n",
        "x_test = x_test.reshape(-1, 48, 48, 1)\n",
        "\n",
        "x_test.shape, x_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5KTBZfICce8",
        "outputId": "715602ca-83e5-4d88-fc71-4007f2f8c621"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((22967, 48, 48, 1), (7178, 48, 48, 1), (22967,), (5742, 48, 48, 1), (5742,))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into train and validation sets (e.g. 80% train, 20% val)\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    x_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "x_train.shape, x_test.shape, y_train.shape, x_val.shape, y_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "o4jUHrb2Cgh6",
        "outputId": "4d44e862-611b-4a47-f71d-1917aa5c9ece"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "idx = 4\n",
        "\n",
        "\n",
        "emotion_dict = {\n",
        "    0: \"Angry\",\n",
        "    1: \"Disgust\",\n",
        "    2: \"Fear\",\n",
        "    3: \"Happy\",\n",
        "    4: \"Sad\",\n",
        "    5: \"Surprise\",\n",
        "    6: \"Neutral\"\n",
        "}\n",
        "\n",
        "plt.imshow(x_train[idx].squeeze(), cmap='gray')\n",
        "plt.title(f'Emotion: {emotion_dict[y_train[idx]]}')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gx7iVdxUCiz7",
        "outputId": "a93ca473-11d7-4bd4-c466-04e36d90cd4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGksoNEOFnzC",
        "outputId": "1ed65cae-76a9-48d1-e500-6b69ee8e8fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.29.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# f8a227b42dc881e037b25911fa86b8a491fc0581\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DioidVEAI-l"
      },
      "outputs": [],
      "source": [
        "# === 0. YOUR HELPER EVALUATE FUNCTION ===\n",
        "def evaluate_model(\n",
        "    x_data, y_data, model, loss_fn, device,\n",
        "    train_mode=False, optimizer=None, batch_size=64\n",
        "):\n",
        "    # 1. Convert x_data → FloatTensor on CPU, then permute to (N,1,48,48)\n",
        "    if isinstance(x_data, torch.Tensor):\n",
        "        x_tensor = x_data.float()\n",
        "    else:\n",
        "        x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
        "    x_tensor = x_tensor.permute(0, 3, 1, 2)  # (N,1,48,48)\n",
        "\n",
        "    # ────────── NEW: normalize exactly like train_transforms ──────────\n",
        "    # train_transforms did: ToTensor() then Normalize(mean=0.5,std=0.5).\n",
        "    # Since x_tensor is now in [0,1], do (x–0.5)/0.5 to match.\n",
        "    x_tensor = (x_tensor - 0.5) / 0.5\n",
        "\n",
        "    # 2. Convert y_data → LongTensor of shape (N,)\n",
        "    if isinstance(y_data, torch.Tensor):\n",
        "        y_tensor = y_data.long()\n",
        "    else:\n",
        "        y_tensor = torch.tensor(y_data, dtype=torch.long)\n",
        "\n",
        "    # 3. Build DataLoader\n",
        "    dataset = TensorDataset(x_tensor, y_tensor)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=train_mode)\n",
        "\n",
        "    # 4. Set model mode\n",
        "    model.to(device)\n",
        "    if train_mode:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # 5. Loop over batches\n",
        "    for x_batch, y_batch in dataloader:\n",
        "        x_batch = x_batch.to(device)   # shape: (B,1,48,48)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        if train_mode:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(x_batch)       # (B,7)\n",
        "        loss = loss_fn(outputs, y_batch)\n",
        "        total_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "        if train_mode:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        preds = outputs.argmax(dim=1)  # (B,)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (preds == y_batch).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kmm8tL8_Hua"
      },
      "source": [
        "# Training 1 - Data aug 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh-O2fLn_LAw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# === 1. MODEL DEFINITION ===\n",
        "class FERCNN_With_BatchNorm(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_rate=0.3):\n",
        "        super(FERCNN_With_BatchNorm, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Calculate flattened size dynamically\n",
        "        dummy_input = torch.zeros(1, 1, 48, 48)\n",
        "        with torch.no_grad():\n",
        "            x = self.pool(torch.relu(self.bn1(self.conv1(dummy_input))))\n",
        "            x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "            x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "            flattened_size = x.view(1, -1).shape[1]\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.dropout(torch.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtDscznQ_OpN"
      },
      "outputs": [],
      "source": [
        "# === 1. CUSTOM DATASET WITH AUGMENTATION ===\n",
        "class FER2013Dataset(Dataset):\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        \"\"\"\n",
        "        images: NumPy array of shape (N, 48, 48, 1)\n",
        "        labels: NumPy array of shape (N,)\n",
        "        transform: torchvision.transforms to apply to each PIL image.\n",
        "        \"\"\"\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]                     # shape (48, 48, 1)\n",
        "        img = img.squeeze(-1)                      # shape (48, 48)\n",
        "        img = Image.fromarray(img.astype(np.uint8), mode=\"L\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)              # now a tensor (1, 48, 48)\n",
        "        label = int(self.labels[idx])\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgNMmGyo_hy7"
      },
      "outputs": [],
      "source": [
        "# === 2. DEFINE AUGMENTATION TRANSFORMS ===\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(10),                # ±10°\n",
        "    transforms.RandomResizedCrop(48, scale=(0.8, 1.0)),\n",
        "    transforms.ToTensor(),                        # → [0,1]\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])   # → roughly [-1,1]\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2XNQPoS_mvw"
      },
      "outputs": [],
      "source": [
        "# === 3. TRAINING FUNCTION (uses evaluate_model for validation) ===\n",
        "def train_model_with_batchnorm_aug(\n",
        "    x_train, y_train, x_val, y_val,\n",
        "    epochs=20,\n",
        "    lr=0.001,\n",
        "    dropout_rate=0.3,\n",
        "    weight_decay=1e-5,\n",
        "    patience=5,\n",
        "    batch_size=64\n",
        "):\n",
        "    run = wandb.init(\n",
        "        project=\"ML_4\",\n",
        "        entity=\"nkhar21-student\",\n",
        "        name=f\"Augmented_1_Dropout_{dropout_rate}_LR_{lr}_Epochs_{epochs}\",\n",
        "        config={\n",
        "            \"dropout_rate\": dropout_rate,\n",
        "            \"pool_kernel\": (2, 2),\n",
        "            \"epochs\": epochs,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"early_stopping_patience\": patience,\n",
        "            \"augmentation\": True\n",
        "        }\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # 4a. Build train/val datasets & loaders\n",
        "    train_dataset = FER2013Dataset(x_train, y_train, transform=train_transforms)\n",
        "    val_dataset = FER2013Dataset(x_val, y_val, transform=val_transforms)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "    # For validation, we'll NOT use evaluate_model's DataLoader but pass raw x_val/y_val into evaluate_model\n",
        "\n",
        "    # 4b. Build model, optimizer, loss, scheduler\n",
        "    model = FERCNN_With_BatchNorm(num_classes=7, dropout_rate=dropout_rate).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience, verbose=True)\n",
        "    wandb.watch(model)\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ---- TRAIN LOOP ----\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device)            # (B, 1, 48, 48)\n",
        "            labels = labels.to(device)         # (B,)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(imgs)              # (B, 7)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item() * imgs.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct_train += (preds == labels).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        train_loss = total_train_loss / total_train\n",
        "        train_acc = correct_train / total_train\n",
        "\n",
        "        # ---- VALIDATION LOOP via evaluate_model ----\n",
        "        val_loss, val_acc = evaluate_model(\n",
        "            x_val, y_val,\n",
        "            model, loss_fn, device,\n",
        "            train_mode=False,\n",
        "            optimizer=None,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        # ---- SCHEDULER & EARLY STOPPING ----\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(f\"  ⛔ Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # ---- OVERFITTING METRICS & LOGGING ----\n",
        "        loss_gap = train_loss - val_loss\n",
        "        acc_gap = train_acc - val_acc\n",
        "        loss_ratio = train_loss / val_loss if val_loss != 0 else float(\"inf\")\n",
        "        acc_ratio = train_acc / val_acc if val_acc != 0 else float(\"inf\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "        print(f\"  Loss Gap:   {loss_gap:.4f} | Acc  Gap:  {acc_gap:.4f}\")\n",
        "        print(f\"  Loss Ratio: {loss_ratio:.4f} | Acc Ratio: {acc_ratio:.4f}\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"loss_gap\": loss_gap,\n",
        "            \"acc_gap\": acc_gap,\n",
        "            \"loss_ratio\": loss_ratio,\n",
        "            \"acc_ratio\": acc_ratio,\n",
        "            \"lr\": optimizer.param_groups[0][\"lr\"]\n",
        "        })\n",
        "\n",
        "    run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yyUalI7xA8tK",
        "outputId": "d3226a34-ae1e-4555-ac01-ea665589e857"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_173219-6kcrxpz6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/6kcrxpz6' target=\"_blank\">FER_Augmented_BatchNorm_Dropout_0.3_LR_0.0015_Epochs_30</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/6kcrxpz6' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/6kcrxpz6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.8361 | Train Acc: 0.2418\n",
            "  Val   Loss: 1.8176 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0186 | Acc  Gap:  -0.0095\n",
            "  Loss Ratio: 1.0102 | Acc Ratio: 0.9623\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.8190 | Train Acc: 0.2508\n",
            "  Val   Loss: 1.8257 | Val   Acc: 0.2513\n",
            "  Loss Gap:   -0.0067 | Acc  Gap:  -0.0005\n",
            "  Loss Ratio: 0.9964 | Acc Ratio: 0.9981\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.8165 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8214 | Val   Acc: 0.2513\n",
            "  Loss Gap:   -0.0049 | Acc  Gap:  -0.0000\n",
            "  Loss Ratio: 0.9973 | Acc Ratio: 0.9999\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.8151 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8133 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0018 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0010 | Acc Ratio: 1.0000\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.8138 | Train Acc: 0.2513\n",
            "  Val   Loss: 2.0380 | Val   Acc: 0.2513\n",
            "  Loss Gap:   -0.2242 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 0.8900 | Acc Ratio: 1.0000\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.8129 | Train Acc: 0.2513\n",
            "  Val   Loss: 5.2344 | Val   Acc: 0.2513\n",
            "  Loss Gap:   -3.4216 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 0.3463 | Acc Ratio: 1.0000\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.8111 | Train Acc: 0.2514\n",
            "  Val   Loss: 27.8074 | Val   Acc: 0.1104\n",
            "  Loss Gap:   -25.9963 | Acc  Gap:  0.1409\n",
            "  Loss Ratio: 0.0651 | Acc Ratio: 2.2765\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.8110 | Train Acc: 0.2513\n",
            "  Val   Loss: 7.8794 | Val   Acc: 0.1428\n",
            "  Loss Gap:   -6.0683 | Acc  Gap:  0.1085\n",
            "  Loss Ratio: 0.2298 | Acc Ratio: 1.7595\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.8107 | Train Acc: 0.2513\n",
            "  Val   Loss: 7.5878 | Val   Acc: 0.1390\n",
            "  Loss Gap:   -5.7771 | Acc  Gap:  0.1123\n",
            "  Loss Ratio: 0.2386 | Acc Ratio: 1.8083\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.8107 | Train Acc: 0.2513\n",
            "  Val   Loss: 7.2366 | Val   Acc: 0.1350\n",
            "  Loss Gap:   -5.4258 | Acc  Gap:  0.1163\n",
            "  Loss Ratio: 0.2502 | Acc Ratio: 1.8620\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.8111 | Train Acc: 0.2513\n",
            "  Val   Loss: 6.6469 | Val   Acc: 0.1207\n",
            "  Loss Gap:   -4.8358 | Acc  Gap:  0.1306\n",
            "  Loss Ratio: 0.2725 | Acc Ratio: 2.0823\n",
            "  ⛔ Early stopping at epoch 12\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▁▁▁▁▁█▆▇▇█</td></tr><tr><td>acc_ratio</td><td>▁▁▁▁▁▁█▅▆▆▇</td></tr><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>loss_gap</td><td>█████▇▁▆▆▇▇</td></tr><tr><td>loss_ratio</td><td>████▇▃▁▂▂▂▃</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁██████████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>██████▁▃▂▂▂</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▂█▃▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.13063</td></tr><tr><td>acc_ratio</td><td>2.08234</td></tr><tr><td>epoch</td><td>11</td></tr><tr><td>loss_gap</td><td>-4.83583</td></tr><tr><td>loss_ratio</td><td>0.27247</td></tr><tr><td>lr</td><td>0.0015</td></tr><tr><td>train_accuracy</td><td>0.25132</td></tr><tr><td>train_loss</td><td>1.81106</td></tr><tr><td>val_accuracy</td><td>0.12069</td></tr><tr><td>val_loss</td><td>6.64689</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_Augmented_BatchNorm_Dropout_0.3_LR_0.0015_Epochs_30</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/6kcrxpz6' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/6kcrxpz6</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_173219-6kcrxpz6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_model_with_batchnorm_aug(\n",
        "    x_train, y_train, x_val, y_val,\n",
        "    epochs=30,\n",
        "    lr=0.0015,\n",
        "    dropout_rate=0.3,\n",
        "    weight_decay=1e-5,\n",
        "    patience=8,\n",
        "    batch_size=128\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gWz4ugAPDOM0",
        "outputId": "5580499e-3d7f-4ec5-b8f3-8f598836923d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁████████</td></tr><tr><td>acc_ratio</td><td>▁████████</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>loss_gap</td><td>█▁▂▆█████</td></tr><tr><td>loss_ratio</td><td>█▁▂▆█████</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁████████</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇█▇▃▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>1e-05</td></tr><tr><td>acc_ratio</td><td>1.00004</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>loss_gap</td><td>-0.00055</td></tr><tr><td>loss_ratio</td><td>0.9997</td></tr><tr><td>lr</td><td>0.0015</td></tr><tr><td>train_accuracy</td><td>0.25132</td></tr><tr><td>train_loss</td><td>1.81069</td></tr><tr><td>val_accuracy</td><td>0.25131</td></tr><tr><td>val_loss</td><td>1.81124</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Augmented_1_Dropout_0.3_LR_0.0015_Epochs_30</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/149p8c2g' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/149p8c2g</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_174213-149p8c2g/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_175800-oqbf1cj9</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/oqbf1cj9' target=\"_blank\">Augmented_1_Dropout_0.3_LR_0.0015_Epochs_30</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/oqbf1cj9' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/oqbf1cj9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.8341 | Train Acc: 0.2370\n",
            "  Val   Loss: 1.8291 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0050 | Acc  Gap:  -0.0143\n",
            "  Loss Ratio: 1.0027 | Acc Ratio: 0.9432\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.8192 | Train Acc: 0.2511\n",
            "  Val   Loss: 1.8304 | Val   Acc: 0.2513\n",
            "  Loss Gap:   -0.0112 | Acc  Gap:  -0.0002\n",
            "  Loss Ratio: 0.9939 | Acc Ratio: 0.9994\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.8164 | Train Acc: 0.2511\n",
            "  Val   Loss: 1.8255 | Val   Acc: 0.2513\n",
            "  Loss Gap:   -0.0091 | Acc  Gap:  -0.0003\n",
            "  Loss Ratio: 0.9950 | Acc Ratio: 0.9990\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.8150 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8147 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0003 | Acc  Gap:  -0.0000\n",
            "  Loss Ratio: 1.0002 | Acc Ratio: 0.9999\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.8137 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8103 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0035 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0019 | Acc Ratio: 1.0000\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.8111 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8092 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0019 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0010 | Acc Ratio: 1.0000\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.8110 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8095 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0015 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0008 | Acc Ratio: 1.0000\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.8103 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8094 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0009 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0005 | Acc Ratio: 1.0000\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.8107 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8102 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0005 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0003 | Acc Ratio: 1.0000\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.8107 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8099 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0009 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0005 | Acc Ratio: 1.0000\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.8110 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8094 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0015 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0009 | Acc Ratio: 1.0000\n",
            "Epoch 12/30\n",
            "  Train Loss: 1.8109 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8098 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0012 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0006 | Acc Ratio: 1.0000\n",
            "Epoch 13/30"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-c2f674b70114>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_model_with_batchnorm_aug(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0015\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-454e8ddd2ac6>\u001b[0m in \u001b[0;36mtrain_model_with_batchnorm_aug\u001b[0;34m(x_train, y_train, x_val, y_val, epochs, lr, dropout_rate, weight_decay, patience, batch_size)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0macc_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/console_capture.py\u001b[0m in \u001b[0;36mwrite_with_callbacks\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks_copy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/redirect.py\u001b[0m in \u001b[0;36m_on_write\u001b[0;34m(self, data, written)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m                 \u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwritten_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error in %s callback\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   2492\u001b[0m                 \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"stdout\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2493\u001b[0m                 cbs=[\n\u001b[0;32m-> 2494\u001b[0;31m                     \u001b[0;32mlambda\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_console_raw_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stdout\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2495\u001b[0m                 ],\n\u001b[1;32m   2496\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwb_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_to_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_is_finished\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         message = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_console_raw_callback\u001b[0;34m(self, name, data)\u001b[0m\n\u001b[1;32m   1637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_output_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_log_to_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mpublish_output_raw\u001b[0;34m(self, name, data)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutputRawRecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0motype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetCurrentTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_output_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_publish_output_raw\u001b[0;34m(self, outdata)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_tbdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtbrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTBRecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_sock.py\u001b[0m in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pb.Record\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_record_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36msend_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mserver_req\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmailbox_slot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mserver_req\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_publish\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_packet_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36msend_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServerRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServerResponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36m_send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<BI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sendall_with_error_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServerRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36m_sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0;31m# sent equal to 0 indicates a closed socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_model_with_batchnorm_aug(\n",
        "    x_train, y_train, x_val, y_val,\n",
        "    epochs=30,\n",
        "    lr=0.0015,\n",
        "    dropout_rate=0.3,\n",
        "    weight_decay=1e-5,\n",
        "    patience=8,\n",
        "    batch_size=128\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otOzlQFkIaVL"
      },
      "source": [
        "# Training 2 - fix aug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAjLGhRaIdGm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import wandb\n",
        "\n",
        "class FERDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Custom dataset that applies transforms to FER2013 data\"\"\"\n",
        "    def __init__(self, x_data, y_data, transform=None):\n",
        "        # Convert to tensors if numpy arrays\n",
        "        if isinstance(x_data, np.ndarray):\n",
        "            # x_data shape: (N, 48, 48, 1) -> (N, 48, 48)\n",
        "            self.x_data = torch.tensor(x_data.squeeze(-1), dtype=torch.float32)\n",
        "        else:\n",
        "            self.x_data = x_data.squeeze(-1).float()\n",
        "\n",
        "        if isinstance(y_data, np.ndarray):\n",
        "            self.y_data = torch.tensor(y_data, dtype=torch.long)\n",
        "        else:\n",
        "            self.y_data = y_data.long()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.x_data[idx]  # Shape: (48, 48)\n",
        "        label = self.y_data[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            # Add channel dimension for transforms: (48, 48) -> (1, 48, 48)\n",
        "            image = image.unsqueeze(0)\n",
        "            image = self.transform(image)\n",
        "        else:\n",
        "            # Just add channel dimension\n",
        "            image = image.unsqueeze(0)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phNxcKt4IoaP"
      },
      "outputs": [],
      "source": [
        "# Define augmentation transforms\n",
        "def get_train_transforms():\n",
        "    return transforms.Compose([\n",
        "        # Random horizontal flip (emotions can be mirrored)\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "\n",
        "        # Random rotation (small angles to avoid distorting face too much)\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "\n",
        "        # Random affine transformations (slight shifts and scaling)\n",
        "        transforms.RandomAffine(\n",
        "            degrees=0,  # No additional rotation beyond RandomRotation\n",
        "            translate=(0.1, 0.1),  # 10% translation in each direction\n",
        "            scale=(0.9, 1.1),  # Scale between 90% and 110%\n",
        "            shear=5  # Small shear transformation\n",
        "        ),\n",
        "\n",
        "        # Random erasing (randomly erase rectangular regions)\n",
        "        transforms.RandomErasing(p=0.3, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
        "\n",
        "        # Add some noise by slightly adjusting brightness and contrast\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "\n",
        "        # Normalize to ensure values stay in reasonable range\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])  # Maps [0,1] to [-1,1]\n",
        "    ])\n",
        "\n",
        "def get_val_transforms():\n",
        "    return transforms.Compose([\n",
        "        # Only normalization for validation (no augmentation)\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJxn_aNEIrM-"
      },
      "outputs": [],
      "source": [
        "class FERCNN_With_BatchNorm(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_rate=0.3):\n",
        "        super(FERCNN_With_BatchNorm, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Calculate flattened size dynamically\n",
        "        dummy_input = torch.zeros(1, 1, 48, 48)\n",
        "        with torch.no_grad():\n",
        "            x = self.pool(torch.relu(self.bn1(self.conv1(dummy_input))))\n",
        "            x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "            x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "            flattened_size = x.view(1, -1).shape[1]\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.dropout(torch.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9PHL8sAIw1e"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_with_augmentation(\n",
        "    dataset,\n",
        "    model,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    train_mode=False,\n",
        "    optimizer=None,\n",
        "    batch_size=64\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate model using custom dataset with augmentation support\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=train_mode)\n",
        "\n",
        "    model.to(device)\n",
        "    if train_mode:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for x_batch, y_batch in dataloader:\n",
        "        x_batch = x_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        if train_mode:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(x_batch)\n",
        "        loss = loss_fn(outputs, y_batch)\n",
        "        total_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "        if train_mode:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (preds == y_batch).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2gxjnIQI4Oa"
      },
      "outputs": [],
      "source": [
        "def train_model_with_augmentation(epochs=20, lr=0.001, dropout_rate=0.3):\n",
        "    run = wandb.init(\n",
        "        project=\"ML_4\",\n",
        "        entity=\"nkhar21-student\",\n",
        "        name=f\"FER_Augmented_Dropout_{dropout_rate}_LR_{lr}_Epochs_{epochs}\",\n",
        "        config={\n",
        "            \"dropout_rate\": dropout_rate,\n",
        "            \"pool_kernel\": (2, 2),\n",
        "            \"epochs\": epochs,\n",
        "            \"batch_size\": 64,\n",
        "            \"lr\": lr,\n",
        "            \"augmentation\": True\n",
        "        }\n",
        "    )\n",
        "\n",
        "    batch_size = 64\n",
        "    model = FERCNN_With_BatchNorm(num_classes=7, dropout_rate=dropout_rate).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    wandb.watch(model)\n",
        "\n",
        "    # Create datasets with transforms\n",
        "    train_dataset = FERDataset(x_train, y_train, transform=get_train_transforms())\n",
        "    val_dataset = FERDataset(x_val, y_val, transform=get_val_transforms())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = evaluate_model_with_augmentation(\n",
        "            train_dataset, model, loss_fn, device,\n",
        "            train_mode=True, optimizer=optimizer, batch_size=batch_size\n",
        "        )\n",
        "        val_loss, val_acc = evaluate_model_with_augmentation(\n",
        "            val_dataset, model, loss_fn, device,\n",
        "            train_mode=False, optimizer=None, batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        # Overfitting metrics\n",
        "        loss_gap = train_loss - val_loss\n",
        "        acc_gap = train_acc - val_acc\n",
        "        loss_ratio = train_loss / val_loss if val_loss != 0 else float(\"inf\")\n",
        "        acc_ratio = train_acc / val_acc if val_acc != 0 else float(\"inf\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "        print(f\"  Loss Gap:   {loss_gap:.4f} | Acc  Gap:  {acc_gap:.4f}\")\n",
        "        print(f\"  Loss Ratio: {loss_ratio:.4f} | Acc Ratio: {acc_ratio:.4f}\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"loss_gap\": loss_gap,\n",
        "            \"acc_gap\": acc_gap,\n",
        "            \"loss_ratio\": loss_ratio,\n",
        "            \"acc_ratio\": acc_ratio,\n",
        "        })\n",
        "\n",
        "    run.finish()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LQI7Bn7I40O"
      },
      "outputs": [],
      "source": [
        "# Additional improvements you can try:\n",
        "def train_model_with_advanced_techniques(epochs=30, lr=0.001, dropout_rate=0.5):\n",
        "    \"\"\"\n",
        "    Training with additional techniques for better performance\n",
        "    \"\"\"\n",
        "    run = wandb.init(\n",
        "        project=\"ML_4\",\n",
        "        entity=\"nkhar21-student\",\n",
        "        name=f\"FER_Advanced_Dropout_{dropout_rate}_LR_{lr}_Epochs_{epochs}\",\n",
        "        config={\n",
        "            \"dropout_rate\": dropout_rate,\n",
        "            \"epochs\": epochs,\n",
        "            \"batch_size\": 64,\n",
        "            \"lr\": lr,\n",
        "            \"scheduler\": \"StepLR\",\n",
        "            \"augmentation\": True,\n",
        "            \"label_smoothing\": 0.1\n",
        "        }\n",
        "    )\n",
        "\n",
        "    batch_size = 64\n",
        "    model = FERCNN_With_BatchNorm(num_classes=7, dropout_rate=dropout_rate).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)  # Add weight decay\n",
        "\n",
        "    # Label smoothing for better generalization\n",
        "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "    wandb.watch(model)\n",
        "\n",
        "    # Create datasets with transforms\n",
        "    train_dataset = FERDataset(x_train, y_train, transform=get_train_transforms())\n",
        "    val_dataset = FERDataset(x_val, y_val, transform=get_val_transforms())\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = evaluate_model_with_augmentation(\n",
        "            train_dataset, model, loss_fn, device,\n",
        "            train_mode=True, optimizer=optimizer, batch_size=batch_size\n",
        "        )\n",
        "        val_loss, val_acc = evaluate_model_with_augmentation(\n",
        "            val_dataset, model, loss_fn, device,\n",
        "            train_mode=False, optimizer=None, batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_fer_model.pth')\n",
        "\n",
        "        # Overfitting metrics\n",
        "        loss_gap = train_loss - val_loss\n",
        "        acc_gap = train_acc - val_acc\n",
        "        loss_ratio = train_loss / val_loss if val_loss != 0 else float(\"inf\")\n",
        "        acc_ratio = train_acc / val_acc if val_acc != 0 else float(\"inf\")\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} (LR: {current_lr:.6f})\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "        print(f\"  Best Val Acc: {best_val_acc:.4f}\")\n",
        "        print(f\"  Loss Gap:   {loss_gap:.4f} | Acc  Gap:  {acc_gap:.4f}\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"best_val_accuracy\": best_val_acc,\n",
        "            \"loss_gap\": loss_gap,\n",
        "            \"acc_gap\": acc_gap,\n",
        "            \"learning_rate\": current_lr,\n",
        "        })\n",
        "\n",
        "    run.finish()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v1WXbJnMI-Be",
        "outputId": "861386b7-98cb-4fcb-be0b-57de008bf6b0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_Augmented_Dropout_0.4_LR_0.0015_Epochs_25</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/hte60m0a' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/hte60m0a</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_180838-hte60m0a/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_180900-riitn0t4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/riitn0t4' target=\"_blank\">FER_Augmented_Dropout_0.4_LR_0.0015_Epochs_25</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/riitn0t4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/riitn0t4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "  Train Loss: 1.8352 | Train Acc: 0.2382\n",
            "  Val   Loss: 1.8085 | Val   Acc: 0.2576\n",
            "  Loss Gap:   0.0268 | Acc  Gap:  -0.0194\n",
            "  Loss Ratio: 1.0148 | Acc Ratio: 0.9247\n",
            "Epoch 2/25\n",
            "  Train Loss: 1.8053 | Train Acc: 0.2512\n",
            "  Val   Loss: 1.7880 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0173 | Acc  Gap:  -0.0001\n",
            "  Loss Ratio: 1.0097 | Acc Ratio: 0.9997\n",
            "Epoch 3/25\n",
            "  Train Loss: 1.7600 | Train Acc: 0.2777\n",
            "  Val   Loss: 1.6334 | Val   Acc: 0.3293\n",
            "  Loss Gap:   0.1266 | Acc  Gap:  -0.0516\n",
            "  Loss Ratio: 1.0775 | Acc Ratio: 0.8432\n",
            "Epoch 4/25\n",
            "  Train Loss: 1.6805 | Train Acc: 0.3149\n",
            "  Val   Loss: 1.5518 | Val   Acc: 0.3650\n",
            "  Loss Gap:   0.1287 | Acc  Gap:  -0.0501\n",
            "  Loss Ratio: 1.0829 | Acc Ratio: 0.8628\n",
            "Epoch 5/25\n",
            "  Train Loss: 1.6071 | Train Acc: 0.3506\n",
            "  Val   Loss: 1.4742 | Val   Acc: 0.4241\n",
            "  Loss Gap:   0.1329 | Acc  Gap:  -0.0735\n",
            "  Loss Ratio: 1.0902 | Acc Ratio: 0.8267\n",
            "Epoch 6/25\n",
            "  Train Loss: 1.5559 | Train Acc: 0.3779\n",
            "  Val   Loss: 1.4148 | Val   Acc: 0.4432\n",
            "  Loss Gap:   0.1411 | Acc  Gap:  -0.0653\n",
            "  Loss Ratio: 1.0997 | Acc Ratio: 0.8526\n",
            "Epoch 7/25\n",
            "  Train Loss: 1.5095 | Train Acc: 0.4041\n",
            "  Val   Loss: 1.3815 | Val   Acc: 0.4695\n",
            "  Loss Gap:   0.1280 | Acc  Gap:  -0.0655\n",
            "  Loss Ratio: 1.0926 | Acc Ratio: 0.8606\n",
            "Epoch 8/25\n",
            "  Train Loss: 1.4699 | Train Acc: 0.4194\n",
            "  Val   Loss: 1.3415 | Val   Acc: 0.4674\n",
            "  Loss Gap:   0.1284 | Acc  Gap:  -0.0480\n",
            "  Loss Ratio: 1.0957 | Acc Ratio: 0.8972\n",
            "Epoch 9/25\n",
            "  Train Loss: 1.4429 | Train Acc: 0.4353\n",
            "  Val   Loss: 1.2997 | Val   Acc: 0.4892\n",
            "  Loss Gap:   0.1432 | Acc  Gap:  -0.0539\n",
            "  Loss Ratio: 1.1102 | Acc Ratio: 0.8899\n",
            "Epoch 10/25\n",
            "  Train Loss: 1.4258 | Train Acc: 0.4439\n",
            "  Val   Loss: 1.2870 | Val   Acc: 0.5019\n",
            "  Loss Gap:   0.1388 | Acc  Gap:  -0.0581\n",
            "  Loss Ratio: 1.1078 | Acc Ratio: 0.8843\n",
            "Epoch 11/25\n",
            "  Train Loss: 1.4005 | Train Acc: 0.4524\n",
            "  Val   Loss: 1.2839 | Val   Acc: 0.4960\n",
            "  Loss Gap:   0.1165 | Acc  Gap:  -0.0436\n",
            "  Loss Ratio: 1.0908 | Acc Ratio: 0.9122\n",
            "Epoch 12/25\n",
            "  Train Loss: 1.3849 | Train Acc: 0.4628\n",
            "  Val   Loss: 1.2871 | Val   Acc: 0.5120\n",
            "  Loss Gap:   0.0978 | Acc  Gap:  -0.0492\n",
            "  Loss Ratio: 1.0760 | Acc Ratio: 0.9040\n",
            "Epoch 13/25\n",
            "  Train Loss: 1.3808 | Train Acc: 0.4630\n",
            "  Val   Loss: 1.2471 | Val   Acc: 0.5200\n",
            "  Loss Gap:   0.1336 | Acc  Gap:  -0.0570\n",
            "  Loss Ratio: 1.1072 | Acc Ratio: 0.8904\n",
            "Epoch 14/25\n",
            "  Train Loss: 1.3617 | Train Acc: 0.4695\n",
            "  Val   Loss: 1.2236 | Val   Acc: 0.5293\n",
            "  Loss Gap:   0.1381 | Acc  Gap:  -0.0598\n",
            "  Loss Ratio: 1.1129 | Acc Ratio: 0.8871\n",
            "Epoch 15/25\n",
            "  Train Loss: 1.3518 | Train Acc: 0.4768\n",
            "  Val   Loss: 1.2231 | Val   Acc: 0.5218\n",
            "  Loss Gap:   0.1286 | Acc  Gap:  -0.0450\n",
            "  Loss Ratio: 1.1052 | Acc Ratio: 0.9138\n",
            "Epoch 16/25\n",
            "  Train Loss: 1.3448 | Train Acc: 0.4810\n",
            "  Val   Loss: 1.2279 | Val   Acc: 0.5282\n",
            "  Loss Gap:   0.1169 | Acc  Gap:  -0.0473\n",
            "  Loss Ratio: 1.0952 | Acc Ratio: 0.9105\n",
            "Epoch 17/25\n",
            "  Train Loss: 1.3390 | Train Acc: 0.4840\n",
            "  Val   Loss: 1.2207 | Val   Acc: 0.5385\n",
            "  Loss Gap:   0.1182 | Acc  Gap:  -0.0545\n",
            "  Loss Ratio: 1.0968 | Acc Ratio: 0.8988\n",
            "Epoch 18/25\n",
            "  Train Loss: 1.3230 | Train Acc: 0.4926\n",
            "  Val   Loss: 1.1828 | Val   Acc: 0.5547\n",
            "  Loss Gap:   0.1402 | Acc  Gap:  -0.0621\n",
            "  Loss Ratio: 1.1185 | Acc Ratio: 0.8881\n",
            "Epoch 19/25\n",
            "  Train Loss: 1.3189 | Train Acc: 0.4928\n",
            "  Val   Loss: 1.1807 | Val   Acc: 0.5557\n",
            "  Loss Gap:   0.1382 | Acc  Gap:  -0.0629\n",
            "  Loss Ratio: 1.1170 | Acc Ratio: 0.8868\n",
            "Epoch 20/25\n",
            "  Train Loss: 1.3061 | Train Acc: 0.5030\n",
            "  Val   Loss: 1.1522 | Val   Acc: 0.5617\n",
            "  Loss Gap:   0.1539 | Acc  Gap:  -0.0586\n",
            "  Loss Ratio: 1.1336 | Acc Ratio: 0.8956\n",
            "Epoch 21/25\n",
            "  Train Loss: 1.3017 | Train Acc: 0.5020\n",
            "  Val   Loss: 1.1613 | Val   Acc: 0.5711\n",
            "  Loss Gap:   0.1404 | Acc  Gap:  -0.0690\n",
            "  Loss Ratio: 1.1209 | Acc Ratio: 0.8791\n",
            "Epoch 22/25\n",
            "  Train Loss: 1.2967 | Train Acc: 0.5079\n",
            "  Val   Loss: 1.1520 | Val   Acc: 0.5665\n",
            "  Loss Gap:   0.1448 | Acc  Gap:  -0.0587\n",
            "  Loss Ratio: 1.1257 | Acc Ratio: 0.8964\n",
            "Epoch 23/25\n",
            "  Train Loss: 1.2819 | Train Acc: 0.5126\n",
            "  Val   Loss: 1.1295 | Val   Acc: 0.5700\n",
            "  Loss Gap:   0.1525 | Acc  Gap:  -0.0574\n",
            "  Loss Ratio: 1.1350 | Acc Ratio: 0.8993\n",
            "Epoch 24/25\n",
            "  Train Loss: 1.2749 | Train Acc: 0.5151\n",
            "  Val   Loss: 1.1307 | Val   Acc: 0.5707\n",
            "  Loss Gap:   0.1442 | Acc  Gap:  -0.0556\n",
            "  Loss Ratio: 1.1275 | Acc Ratio: 0.9026\n",
            "Epoch 25/25\n",
            "  Train Loss: 1.2675 | Train Acc: 0.5203\n",
            "  Val   Loss: 1.1192 | Val   Acc: 0.5819\n",
            "  Loss Gap:   0.1483 | Acc  Gap:  -0.0615\n",
            "  Loss Ratio: 1.1325 | Acc Ratio: 0.8942\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▆█▃▃▁▂▂▃▃▂▄▃▃▂▄▄▃▂▂▂▁▂▃▃▂</td></tr><tr><td>acc_ratio</td><td>▅█▂▂▁▂▂▄▄▃▄▄▄▃▅▄▄▃▃▄▃▄▄▄▄</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>loss_gap</td><td>▁▁▇▇▇▇▇▇▇▇▆▅▇▇▇▆▆▇▇█▇████</td></tr><tr><td>loss_ratio</td><td>▁▁▅▅▅▆▆▆▇▆▆▅▆▇▆▆▆▇▇█▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▂▃▄▄▅▅▆▆▆▇▇▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>██▇▆▅▅▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▃▃▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██████</td></tr><tr><td>val_loss</td><td>██▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>-0.06154</td></tr><tr><td>acc_ratio</td><td>0.89423</td></tr><tr><td>epoch</td><td>25</td></tr><tr><td>loss_gap</td><td>0.14828</td></tr><tr><td>loss_ratio</td><td>1.13249</td></tr><tr><td>train_accuracy</td><td>0.52031</td></tr><tr><td>train_loss</td><td>1.26749</td></tr><tr><td>val_accuracy</td><td>0.58185</td></tr><tr><td>val_loss</td><td>1.11921</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_Augmented_Dropout_0.4_LR_0.0015_Epochs_25</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/riitn0t4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/riitn0t4</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_180900-riitn0t4/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = train_model_with_augmentation(epochs=25, lr=0.0015, dropout_rate=0.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zRZTnudXM7ox",
        "outputId": "51ed58fd-a8aa-43d8-d226-b046023d8e99"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_182436-dlzouwt2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/dlzouwt2' target=\"_blank\">FER_Augmented_Dropout_0.3_LR_0.001_Epochs_30</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/dlzouwt2' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/dlzouwt2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.8166 | Train Acc: 0.2429\n",
            "  Val   Loss: 1.7305 | Val   Acc: 0.2851\n",
            "  Loss Gap:   0.0860 | Acc  Gap:  -0.0422\n",
            "  Loss Ratio: 1.0497 | Acc Ratio: 0.8519\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.6894 | Train Acc: 0.3109\n",
            "  Val   Loss: 1.5092 | Val   Acc: 0.4051\n",
            "  Loss Gap:   0.1802 | Acc  Gap:  -0.0942\n",
            "  Loss Ratio: 1.1194 | Acc Ratio: 0.7676\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.5836 | Train Acc: 0.3701\n",
            "  Val   Loss: 1.4601 | Val   Acc: 0.4324\n",
            "  Loss Gap:   0.1234 | Acc  Gap:  -0.0624\n",
            "  Loss Ratio: 1.0845 | Acc Ratio: 0.8558\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.5218 | Train Acc: 0.3994\n",
            "  Val   Loss: 1.3596 | Val   Acc: 0.4690\n",
            "  Loss Gap:   0.1622 | Acc  Gap:  -0.0696\n",
            "  Loss Ratio: 1.1193 | Acc Ratio: 0.8516\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.4781 | Train Acc: 0.4211\n",
            "  Val   Loss: 1.3502 | Val   Acc: 0.4706\n",
            "  Loss Gap:   0.1279 | Acc  Gap:  -0.0494\n",
            "  Loss Ratio: 1.0947 | Acc Ratio: 0.8949\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.4393 | Train Acc: 0.4387\n",
            "  Val   Loss: 1.3115 | Val   Acc: 0.4894\n",
            "  Loss Gap:   0.1278 | Acc  Gap:  -0.0507\n",
            "  Loss Ratio: 1.0974 | Acc Ratio: 0.8964\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.4195 | Train Acc: 0.4507\n",
            "  Val   Loss: 1.2744 | Val   Acc: 0.5092\n",
            "  Loss Gap:   0.1451 | Acc  Gap:  -0.0585\n",
            "  Loss Ratio: 1.1138 | Acc Ratio: 0.8851\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.3985 | Train Acc: 0.4574\n",
            "  Val   Loss: 1.3040 | Val   Acc: 0.4977\n",
            "  Loss Gap:   0.0945 | Acc  Gap:  -0.0403\n",
            "  Loss Ratio: 1.0725 | Acc Ratio: 0.9190\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.3759 | Train Acc: 0.4678\n",
            "  Val   Loss: 1.2880 | Val   Acc: 0.5019\n",
            "  Loss Gap:   0.0879 | Acc  Gap:  -0.0342\n",
            "  Loss Ratio: 1.0682 | Acc Ratio: 0.9319\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.3621 | Train Acc: 0.4749\n",
            "  Val   Loss: 1.2494 | Val   Acc: 0.5322\n",
            "  Loss Gap:   0.1128 | Acc  Gap:  -0.0574\n",
            "  Loss Ratio: 1.0902 | Acc Ratio: 0.8922\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.3388 | Train Acc: 0.4827\n",
            "  Val   Loss: 1.2012 | Val   Acc: 0.5385\n",
            "  Loss Gap:   0.1377 | Acc  Gap:  -0.0558\n",
            "  Loss Ratio: 1.1146 | Acc Ratio: 0.8964\n",
            "Epoch 12/30\n",
            "  Train Loss: 1.3329 | Train Acc: 0.4844\n",
            "  Val   Loss: 1.2079 | Val   Acc: 0.5361\n",
            "  Loss Gap:   0.1251 | Acc  Gap:  -0.0516\n",
            "  Loss Ratio: 1.1035 | Acc Ratio: 0.9037\n",
            "Epoch 13/30\n",
            "  Train Loss: 1.3185 | Train Acc: 0.4926\n",
            "  Val   Loss: 1.1834 | Val   Acc: 0.5521\n",
            "  Loss Gap:   0.1350 | Acc  Gap:  -0.0595\n",
            "  Loss Ratio: 1.1141 | Acc Ratio: 0.8922\n",
            "Epoch 14/30\n",
            "  Train Loss: 1.3059 | Train Acc: 0.4970\n",
            "  Val   Loss: 1.1879 | Val   Acc: 0.5507\n",
            "  Loss Gap:   0.1181 | Acc  Gap:  -0.0537\n",
            "  Loss Ratio: 1.0994 | Acc Ratio: 0.9026\n",
            "Epoch 15/30\n",
            "  Train Loss: 1.2967 | Train Acc: 0.5029\n",
            "  Val   Loss: 1.1747 | Val   Acc: 0.5472\n",
            "  Loss Gap:   0.1219 | Acc  Gap:  -0.0443\n",
            "  Loss Ratio: 1.1038 | Acc Ratio: 0.9191\n",
            "Epoch 16/30\n",
            "  Train Loss: 1.2881 | Train Acc: 0.5051\n",
            "  Val   Loss: 1.1492 | Val   Acc: 0.5578\n",
            "  Loss Gap:   0.1389 | Acc  Gap:  -0.0527\n",
            "  Loss Ratio: 1.1209 | Acc Ratio: 0.9055\n",
            "Epoch 17/30\n",
            "  Train Loss: 1.2728 | Train Acc: 0.5122\n",
            "  Val   Loss: 1.1578 | Val   Acc: 0.5672\n",
            "  Loss Gap:   0.1150 | Acc  Gap:  -0.0550\n",
            "  Loss Ratio: 1.0993 | Acc Ratio: 0.9030\n",
            "Epoch 18/30\n",
            "  Train Loss: 1.2623 | Train Acc: 0.5182\n",
            "  Val   Loss: 1.1896 | Val   Acc: 0.5493\n",
            "  Loss Gap:   0.0727 | Acc  Gap:  -0.0311\n",
            "  Loss Ratio: 1.0611 | Acc Ratio: 0.9434\n",
            "Epoch 19/30\n",
            "  Train Loss: 1.2575 | Train Acc: 0.5179\n",
            "  Val   Loss: 1.1296 | Val   Acc: 0.5747\n",
            "  Loss Gap:   0.1279 | Acc  Gap:  -0.0568\n",
            "  Loss Ratio: 1.1132 | Acc Ratio: 0.9011\n",
            "Epoch 20/30\n",
            "  Train Loss: 1.2566 | Train Acc: 0.5174\n",
            "  Val   Loss: 1.1331 | Val   Acc: 0.5681\n",
            "  Loss Gap:   0.1235 | Acc  Gap:  -0.0507\n",
            "  Loss Ratio: 1.1090 | Acc Ratio: 0.9108\n",
            "Epoch 21/30\n",
            "  Train Loss: 1.2499 | Train Acc: 0.5214\n",
            "  Val   Loss: 1.1036 | Val   Acc: 0.5768\n",
            "  Loss Gap:   0.1463 | Acc  Gap:  -0.0554\n",
            "  Loss Ratio: 1.1326 | Acc Ratio: 0.9040\n",
            "Epoch 22/30\n",
            "  Train Loss: 1.2415 | Train Acc: 0.5277\n",
            "  Val   Loss: 1.1010 | Val   Acc: 0.5845\n",
            "  Loss Gap:   0.1405 | Acc  Gap:  -0.0568\n",
            "  Loss Ratio: 1.1276 | Acc Ratio: 0.9029\n",
            "Epoch 23/30\n",
            "  Train Loss: 1.2320 | Train Acc: 0.5281\n",
            "  Val   Loss: 1.1020 | Val   Acc: 0.5899\n",
            "  Loss Gap:   0.1300 | Acc  Gap:  -0.0618\n",
            "  Loss Ratio: 1.1180 | Acc Ratio: 0.8953\n",
            "Epoch 24/30\n",
            "  Train Loss: 1.2201 | Train Acc: 0.5359\n",
            "  Val   Loss: 1.1007 | Val   Acc: 0.5876\n",
            "  Loss Gap:   0.1193 | Acc  Gap:  -0.0517\n",
            "  Loss Ratio: 1.1084 | Acc Ratio: 0.9120\n",
            "Epoch 25/30\n",
            "  Train Loss: 1.2192 | Train Acc: 0.5332\n",
            "  Val   Loss: 1.1202 | Val   Acc: 0.5758\n",
            "  Loss Gap:   0.0990 | Acc  Gap:  -0.0426\n",
            "  Loss Ratio: 1.0884 | Acc Ratio: 0.9260\n",
            "Epoch 26/30\n",
            "  Train Loss: 1.2076 | Train Acc: 0.5393\n",
            "  Val   Loss: 1.0956 | Val   Acc: 0.5808\n",
            "  Loss Gap:   0.1120 | Acc  Gap:  -0.0415\n",
            "  Loss Ratio: 1.1022 | Acc Ratio: 0.9285\n",
            "Epoch 27/30\n",
            "  Train Loss: 1.2083 | Train Acc: 0.5378\n",
            "  Val   Loss: 1.0802 | Val   Acc: 0.5899\n",
            "  Loss Gap:   0.1281 | Acc  Gap:  -0.0520\n",
            "  Loss Ratio: 1.1186 | Acc Ratio: 0.9118\n",
            "Epoch 28/30\n",
            "  Train Loss: 1.1965 | Train Acc: 0.5475\n",
            "  Val   Loss: 1.0852 | Val   Acc: 0.5947\n",
            "  Loss Gap:   0.1114 | Acc  Gap:  -0.0473\n",
            "  Loss Ratio: 1.1026 | Acc Ratio: 0.9205\n",
            "Epoch 29/30\n",
            "  Train Loss: 1.1957 | Train Acc: 0.5451\n",
            "  Val   Loss: 1.0637 | Val   Acc: 0.5994\n",
            "  Loss Gap:   0.1320 | Acc  Gap:  -0.0544\n",
            "  Loss Ratio: 1.1241 | Acc Ratio: 0.9093\n",
            "Epoch 30/30\n",
            "  Train Loss: 1.1921 | Train Acc: 0.5497\n",
            "  Val   Loss: 1.0533 | Val   Acc: 0.6022\n",
            "  Loss Gap:   0.1388 | Acc  Gap:  -0.0526\n",
            "  Loss Ratio: 1.1318 | Acc Ratio: 0.9127\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▇▁▅▄▆▆▅▇█▅▅▆▅▅▇▆▅█▅▆▅▅▅▆▇▇▆▆▅▆</td></tr><tr><td>acc_ratio</td><td>▄▁▅▄▆▆▆▇█▆▆▆▆▆▇▆▆█▆▇▆▆▆▇▇▇▇▇▇▇</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>loss_gap</td><td>▂█▄▇▅▅▆▂▂▄▅▄▅▄▄▅▄▁▅▄▆▅▅▄▃▄▅▄▅▅</td></tr><tr><td>loss_ratio</td><td>▁▇▄▇▅▅▆▃▃▄▆▆▆▅▆▇▅▂▆▆██▇▆▄▅▇▅▇█</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>█▇▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███▇█████</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▄▃▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>-0.05257</td></tr><tr><td>acc_ratio</td><td>0.91271</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>loss_gap</td><td>0.13885</td></tr><tr><td>loss_ratio</td><td>1.13183</td></tr><tr><td>train_accuracy</td><td>0.54966</td></tr><tr><td>train_loss</td><td>1.19212</td></tr><tr><td>val_accuracy</td><td>0.60223</td></tr><tr><td>val_loss</td><td>1.05327</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_Augmented_Dropout_0.3_LR_0.001_Epochs_30</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/dlzouwt2' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/dlzouwt2</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_182436-dlzouwt2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = train_model_with_augmentation(epochs=30, lr=0.001, dropout_rate=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mrw3MlJhRt56",
        "outputId": "e518c506-b27c-4113-ee19-12cc061ef21c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_185425-fnltagtr</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/fnltagtr' target=\"_blank\">FER_Augmented_Dropout_0.25_LR_0.0016_Epochs_50</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/fnltagtr' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/fnltagtr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "  Train Loss: 1.8279 | Train Acc: 0.2440\n",
            "  Val   Loss: 1.7730 | Val   Acc: 0.2520\n",
            "  Loss Gap:   0.0550 | Acc  Gap:  -0.0080\n",
            "  Loss Ratio: 1.0310 | Acc Ratio: 0.9683\n",
            "Epoch 2/50\n",
            "  Train Loss: 1.7579 | Train Acc: 0.2776\n",
            "  Val   Loss: 1.6247 | Val   Acc: 0.3427\n",
            "  Loss Gap:   0.1332 | Acc  Gap:  -0.0652\n",
            "  Loss Ratio: 1.0820 | Acc Ratio: 0.8099\n",
            "Epoch 3/50\n",
            "  Train Loss: 1.6311 | Train Acc: 0.3479\n",
            "  Val   Loss: 1.4836 | Val   Acc: 0.4310\n",
            "  Loss Gap:   0.1475 | Acc  Gap:  -0.0831\n",
            "  Loss Ratio: 1.0994 | Acc Ratio: 0.8071\n",
            "Epoch 4/50\n",
            "  Train Loss: 1.5513 | Train Acc: 0.3882\n",
            "  Val   Loss: 1.3875 | Val   Acc: 0.4730\n",
            "  Loss Gap:   0.1639 | Acc  Gap:  -0.0848\n",
            "  Loss Ratio: 1.1181 | Acc Ratio: 0.8206\n",
            "Epoch 5/50\n",
            "  Train Loss: 1.4912 | Train Acc: 0.4182\n",
            "  Val   Loss: 1.3426 | Val   Acc: 0.4822\n",
            "  Loss Gap:   0.1486 | Acc  Gap:  -0.0641\n",
            "  Loss Ratio: 1.1106 | Acc Ratio: 0.8671\n",
            "Epoch 6/50\n",
            "  Train Loss: 1.4506 | Train Acc: 0.4333\n",
            "  Val   Loss: 1.3211 | Val   Acc: 0.4901\n",
            "  Loss Gap:   0.1295 | Acc  Gap:  -0.0568\n",
            "  Loss Ratio: 1.0980 | Acc Ratio: 0.8841\n",
            "Epoch 7/50\n",
            "  Train Loss: 1.4163 | Train Acc: 0.4499\n",
            "  Val   Loss: 1.2830 | Val   Acc: 0.4951\n",
            "  Loss Gap:   0.1333 | Acc  Gap:  -0.0452\n",
            "  Loss Ratio: 1.1039 | Acc Ratio: 0.9088\n",
            "Epoch 8/50\n",
            "  Train Loss: 1.3948 | Train Acc: 0.4574\n",
            "  Val   Loss: 1.2527 | Val   Acc: 0.5143\n",
            "  Loss Gap:   0.1422 | Acc  Gap:  -0.0569\n",
            "  Loss Ratio: 1.1135 | Acc Ratio: 0.8894\n",
            "Epoch 9/50\n",
            "  Train Loss: 1.3803 | Train Acc: 0.4603\n",
            "  Val   Loss: 1.2475 | Val   Acc: 0.5158\n",
            "  Loss Gap:   0.1328 | Acc  Gap:  -0.0555\n",
            "  Loss Ratio: 1.1065 | Acc Ratio: 0.8923\n",
            "Epoch 10/50\n",
            "  Train Loss: 1.3619 | Train Acc: 0.4737\n",
            "  Val   Loss: 1.2235 | Val   Acc: 0.5228\n",
            "  Loss Gap:   0.1384 | Acc  Gap:  -0.0491\n",
            "  Loss Ratio: 1.1132 | Acc Ratio: 0.9060\n",
            "Epoch 11/50\n",
            "  Train Loss: 1.3485 | Train Acc: 0.4793\n",
            "  Val   Loss: 1.2119 | Val   Acc: 0.5300\n",
            "  Loss Gap:   0.1366 | Acc  Gap:  -0.0506\n",
            "  Loss Ratio: 1.1128 | Acc Ratio: 0.9045\n",
            "Epoch 12/50\n",
            "  Train Loss: 1.3316 | Train Acc: 0.4870\n",
            "  Val   Loss: 1.1896 | Val   Acc: 0.5413\n",
            "  Loss Gap:   0.1420 | Acc  Gap:  -0.0542\n",
            "  Loss Ratio: 1.1194 | Acc Ratio: 0.8998\n",
            "Epoch 13/50\n",
            "  Train Loss: 1.3249 | Train Acc: 0.4890\n",
            "  Val   Loss: 1.1699 | Val   Acc: 0.5409\n",
            "  Loss Gap:   0.1550 | Acc  Gap:  -0.0520\n",
            "  Loss Ratio: 1.1325 | Acc Ratio: 0.9039\n",
            "Epoch 14/50\n",
            "  Train Loss: 1.3126 | Train Acc: 0.4965\n",
            "  Val   Loss: 1.1663 | Val   Acc: 0.5521\n",
            "  Loss Gap:   0.1463 | Acc  Gap:  -0.0556\n",
            "  Loss Ratio: 1.1254 | Acc Ratio: 0.8993\n",
            "Epoch 15/50\n",
            "  Train Loss: 1.2998 | Train Acc: 0.5029\n",
            "  Val   Loss: 1.1722 | Val   Acc: 0.5397\n",
            "  Loss Gap:   0.1276 | Acc  Gap:  -0.0368\n",
            "  Loss Ratio: 1.1088 | Acc Ratio: 0.9319\n",
            "Epoch 16/50\n",
            "  Train Loss: 1.2843 | Train Acc: 0.5053\n",
            "  Val   Loss: 1.1596 | Val   Acc: 0.5481\n",
            "  Loss Gap:   0.1247 | Acc  Gap:  -0.0427\n",
            "  Loss Ratio: 1.1076 | Acc Ratio: 0.9220\n",
            "Epoch 17/50\n",
            "  Train Loss: 1.2813 | Train Acc: 0.5100\n",
            "  Val   Loss: 1.1505 | Val   Acc: 0.5613\n",
            "  Loss Gap:   0.1308 | Acc  Gap:  -0.0513\n",
            "  Loss Ratio: 1.1137 | Acc Ratio: 0.9087\n",
            "Epoch 18/50\n",
            "  Train Loss: 1.2749 | Train Acc: 0.5131\n",
            "  Val   Loss: 1.1311 | Val   Acc: 0.5683\n",
            "  Loss Gap:   0.1438 | Acc  Gap:  -0.0552\n",
            "  Loss Ratio: 1.1271 | Acc Ratio: 0.9029\n",
            "Epoch 19/50\n",
            "  Train Loss: 1.2597 | Train Acc: 0.5195\n",
            "  Val   Loss: 1.1433 | Val   Acc: 0.5726\n",
            "  Loss Gap:   0.1163 | Acc  Gap:  -0.0531\n",
            "  Loss Ratio: 1.1018 | Acc Ratio: 0.9073\n",
            "Epoch 20/50\n",
            "  Train Loss: 1.2504 | Train Acc: 0.5228\n",
            "  Val   Loss: 1.1497 | Val   Acc: 0.5726\n",
            "  Loss Gap:   0.1008 | Acc  Gap:  -0.0499\n",
            "  Loss Ratio: 1.0876 | Acc Ratio: 0.9129\n",
            "Epoch 21/50\n",
            "  Train Loss: 1.2484 | Train Acc: 0.5248\n",
            "  Val   Loss: 1.1163 | Val   Acc: 0.5730\n",
            "  Loss Gap:   0.1321 | Acc  Gap:  -0.0482\n",
            "  Loss Ratio: 1.1183 | Acc Ratio: 0.9159\n",
            "Epoch 22/50\n",
            "  Train Loss: 1.2468 | Train Acc: 0.5233\n",
            "  Val   Loss: 1.1199 | Val   Acc: 0.5744\n",
            "  Loss Gap:   0.1269 | Acc  Gap:  -0.0510\n",
            "  Loss Ratio: 1.1133 | Acc Ratio: 0.9111\n",
            "Epoch 23/50\n",
            "  Train Loss: 1.2363 | Train Acc: 0.5300\n",
            "  Val   Loss: 1.1227 | Val   Acc: 0.5806\n",
            "  Loss Gap:   0.1136 | Acc  Gap:  -0.0507\n",
            "  Loss Ratio: 1.1012 | Acc Ratio: 0.9128\n",
            "Epoch 24/50\n",
            "  Train Loss: 1.2358 | Train Acc: 0.5335\n",
            "  Val   Loss: 1.1180 | Val   Acc: 0.5714\n",
            "  Loss Gap:   0.1178 | Acc  Gap:  -0.0379\n",
            "  Loss Ratio: 1.1054 | Acc Ratio: 0.9337\n",
            "Epoch 25/50\n",
            "  Train Loss: 1.2202 | Train Acc: 0.5392\n",
            "  Val   Loss: 1.1103 | Val   Acc: 0.5852\n",
            "  Loss Gap:   0.1098 | Acc  Gap:  -0.0460\n",
            "  Loss Ratio: 1.0989 | Acc Ratio: 0.9215\n",
            "Epoch 26/50\n",
            "  Train Loss: 1.2234 | Train Acc: 0.5384\n",
            "  Val   Loss: 1.1079 | Val   Acc: 0.5892\n",
            "  Loss Gap:   0.1155 | Acc  Gap:  -0.0508\n",
            "  Loss Ratio: 1.1042 | Acc Ratio: 0.9138\n",
            "Epoch 27/50\n",
            "  Train Loss: 1.2062 | Train Acc: 0.5461\n",
            "  Val   Loss: 1.0884 | Val   Acc: 0.5888\n",
            "  Loss Gap:   0.1177 | Acc  Gap:  -0.0427\n",
            "  Loss Ratio: 1.1082 | Acc Ratio: 0.9275\n",
            "Epoch 28/50\n",
            "  Train Loss: 1.2053 | Train Acc: 0.5463\n",
            "  Val   Loss: 1.0806 | Val   Acc: 0.5974\n",
            "  Loss Gap:   0.1248 | Acc  Gap:  -0.0510\n",
            "  Loss Ratio: 1.1154 | Acc Ratio: 0.9145\n",
            "Epoch 29/50\n",
            "  Train Loss: 1.2035 | Train Acc: 0.5504\n",
            "  Val   Loss: 1.0966 | Val   Acc: 0.5911\n",
            "  Loss Gap:   0.1069 | Acc  Gap:  -0.0407\n",
            "  Loss Ratio: 1.0975 | Acc Ratio: 0.9312\n",
            "Epoch 30/50\n",
            "  Train Loss: 1.2016 | Train Acc: 0.5484\n",
            "  Val   Loss: 1.0815 | Val   Acc: 0.5953\n",
            "  Loss Gap:   0.1201 | Acc  Gap:  -0.0469\n",
            "  Loss Ratio: 1.1110 | Acc Ratio: 0.9212\n",
            "Epoch 31/50\n",
            "  Train Loss: 1.1880 | Train Acc: 0.5521\n",
            "  Val   Loss: 1.0965 | Val   Acc: 0.5878\n",
            "  Loss Gap:   0.0915 | Acc  Gap:  -0.0356\n",
            "  Loss Ratio: 1.0834 | Acc Ratio: 0.9394\n",
            "Epoch 32/50\n",
            "  Train Loss: 1.1866 | Train Acc: 0.5538\n",
            "  Val   Loss: 1.0782 | Val   Acc: 0.5968\n",
            "  Loss Gap:   0.1083 | Acc  Gap:  -0.0430\n",
            "  Loss Ratio: 1.1005 | Acc Ratio: 0.9279\n",
            "Epoch 33/50\n",
            "  Train Loss: 1.1837 | Train Acc: 0.5573\n",
            "  Val   Loss: 1.0782 | Val   Acc: 0.5991\n",
            "  Loss Gap:   0.1055 | Acc  Gap:  -0.0418\n",
            "  Loss Ratio: 1.0978 | Acc Ratio: 0.9303\n",
            "Epoch 34/50\n",
            "  Train Loss: 1.1736 | Train Acc: 0.5612\n",
            "  Val   Loss: 1.0657 | Val   Acc: 0.5960\n",
            "  Loss Gap:   0.1078 | Acc  Gap:  -0.0348\n",
            "  Loss Ratio: 1.1012 | Acc Ratio: 0.9416\n",
            "Epoch 35/50\n",
            "  Train Loss: 1.1739 | Train Acc: 0.5612\n",
            "  Val   Loss: 1.0516 | Val   Acc: 0.6104\n",
            "  Loss Gap:   0.1223 | Acc  Gap:  -0.0492\n",
            "  Loss Ratio: 1.1163 | Acc Ratio: 0.9194\n",
            "Epoch 36/50\n",
            "  Train Loss: 1.1698 | Train Acc: 0.5606\n",
            "  Val   Loss: 1.0493 | Val   Acc: 0.6052\n",
            "  Loss Gap:   0.1205 | Acc  Gap:  -0.0446\n",
            "  Loss Ratio: 1.1149 | Acc Ratio: 0.9264\n",
            "Epoch 37/50\n",
            "  Train Loss: 1.1583 | Train Acc: 0.5658\n",
            "  Val   Loss: 1.0738 | Val   Acc: 0.5984\n",
            "  Loss Gap:   0.0845 | Acc  Gap:  -0.0326\n",
            "  Loss Ratio: 1.0787 | Acc Ratio: 0.9455\n",
            "Epoch 38/50\n",
            "  Train Loss: 1.1513 | Train Acc: 0.5704\n",
            "  Val   Loss: 1.0555 | Val   Acc: 0.6047\n",
            "  Loss Gap:   0.0958 | Acc  Gap:  -0.0342\n",
            "  Loss Ratio: 1.0908 | Acc Ratio: 0.9434\n",
            "Epoch 39/50\n",
            "  Train Loss: 1.1561 | Train Acc: 0.5694\n",
            "  Val   Loss: 1.0423 | Val   Acc: 0.6101\n",
            "  Loss Gap:   0.1137 | Acc  Gap:  -0.0407\n",
            "  Loss Ratio: 1.1091 | Acc Ratio: 0.9333\n",
            "Epoch 40/50\n",
            "  Train Loss: 1.1416 | Train Acc: 0.5755\n",
            "  Val   Loss: 1.0508 | Val   Acc: 0.6031\n",
            "  Loss Gap:   0.0908 | Acc  Gap:  -0.0276\n",
            "  Loss Ratio: 1.0864 | Acc Ratio: 0.9542\n",
            "Epoch 41/50\n",
            "  Train Loss: 1.1413 | Train Acc: 0.5726\n",
            "  Val   Loss: 1.0755 | Val   Acc: 0.5927\n",
            "  Loss Gap:   0.0658 | Acc  Gap:  -0.0200\n",
            "  Loss Ratio: 1.0612 | Acc Ratio: 0.9662\n",
            "Epoch 42/50\n",
            "  Train Loss: 1.1459 | Train Acc: 0.5726\n",
            "  Val   Loss: 1.0553 | Val   Acc: 0.6111\n",
            "  Loss Gap:   0.0906 | Acc  Gap:  -0.0386\n",
            "  Loss Ratio: 1.0858 | Acc Ratio: 0.9369\n",
            "Epoch 43/50\n",
            "  Train Loss: 1.1353 | Train Acc: 0.5820\n",
            "  Val   Loss: 1.0545 | Val   Acc: 0.6014\n",
            "  Loss Gap:   0.0807 | Acc  Gap:  -0.0193\n",
            "  Loss Ratio: 1.0766 | Acc Ratio: 0.9678\n",
            "Epoch 44/50\n",
            "  Train Loss: 1.1323 | Train Acc: 0.5766\n",
            "  Val   Loss: 1.0404 | Val   Acc: 0.6129\n",
            "  Loss Gap:   0.0919 | Acc  Gap:  -0.0362\n",
            "  Loss Ratio: 1.0883 | Acc Ratio: 0.9409\n",
            "Epoch 45/50\n",
            "  Train Loss: 1.1286 | Train Acc: 0.5800\n",
            "  Val   Loss: 1.0356 | Val   Acc: 0.6115\n",
            "  Loss Gap:   0.0930 | Acc  Gap:  -0.0314\n",
            "  Loss Ratio: 1.0898 | Acc Ratio: 0.9486\n",
            "Epoch 46/50\n",
            "  Train Loss: 1.1290 | Train Acc: 0.5797\n",
            "  Val   Loss: 1.0306 | Val   Acc: 0.6099\n",
            "  Loss Gap:   0.0984 | Acc  Gap:  -0.0301\n",
            "  Loss Ratio: 1.0955 | Acc Ratio: 0.9506\n",
            "Epoch 47/50\n",
            "  Train Loss: 1.1118 | Train Acc: 0.5875\n",
            "  Val   Loss: 1.0291 | Val   Acc: 0.6111\n",
            "  Loss Gap:   0.0826 | Acc  Gap:  -0.0236\n",
            "  Loss Ratio: 1.0803 | Acc Ratio: 0.9614\n",
            "Epoch 48/50\n",
            "  Train Loss: 1.1166 | Train Acc: 0.5859\n",
            "  Val   Loss: 1.0324 | Val   Acc: 0.6125\n",
            "  Loss Gap:   0.0842 | Acc  Gap:  -0.0266\n",
            "  Loss Ratio: 1.0816 | Acc Ratio: 0.9565\n",
            "Epoch 49/50\n",
            "  Train Loss: 1.1100 | Train Acc: 0.5845\n",
            "  Val   Loss: 1.0427 | Val   Acc: 0.6097\n",
            "  Loss Gap:   0.0673 | Acc  Gap:  -0.0252\n",
            "  Loss Ratio: 1.0645 | Acc Ratio: 0.9586\n",
            "Epoch 50/50\n",
            "  Train Loss: 1.1122 | Train Acc: 0.5840\n",
            "  Val   Loss: 1.0239 | Val   Acc: 0.6130\n",
            "  Loss Gap:   0.0884 | Acc  Gap:  -0.0291\n",
            "  Loss Ratio: 1.0863 | Acc Ratio: 0.9526\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>█▃▁▁▃▅▄▄▄▄▄▄▅▅▄▄▄▄▄▄▅▄▅▄▅▅▅▅▆▄▆▅▆▇▅▅▆▆▇▆</td></tr><tr><td>acc_ratio</td><td>█▁▁▂▄▅▅▅▅▅▅▆▆▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▆▆▆▇█▇█▇▇█▇▇</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>loss_gap</td><td>▁▆█▇▆▇▆▆▆▇▇▆▅▆▇▄▆▅▅▅▅▅▄▅▃▄▄▅▅▃▅▃▂▃▃▃▄▃▃▃</td></tr><tr><td>loss_ratio</td><td>▁▅▆▇▆▆▇▆▇▇██▆▆▇▆▅▇▇▆▆▆▆▇▆▅▆▇▇▄▆▅▃▅▄▅▅▄▄▅</td></tr><tr><td>train_accuracy</td><td>▁▂▃▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>val_loss</td><td>█▇▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>-0.02906</td></tr><tr><td>acc_ratio</td><td>0.9526</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>loss_gap</td><td>0.08838</td></tr><tr><td>loss_ratio</td><td>1.08632</td></tr><tr><td>train_accuracy</td><td>0.58397</td></tr><tr><td>train_loss</td><td>1.11225</td></tr><tr><td>val_accuracy</td><td>0.61303</td></tr><tr><td>val_loss</td><td>1.02387</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_Augmented_Dropout_0.25_LR_0.0016_Epochs_50</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/fnltagtr' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/fnltagtr</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_185425-fnltagtr/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = train_model_with_augmentation(epochs=50, lr=0.0016, dropout_rate=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KbVnR3U4JB7e",
        "outputId": "1ca2c045-a403-4013-a681-998f822f8b5c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_214817-1g6wtpfo</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/1g6wtpfo' target=\"_blank\">FER_Advanced_Dropout_0.5_LR_0.0015_Epochs_60</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/1g6wtpfo' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/1g6wtpfo</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/60 (LR: 0.001500)\n",
            "  Train Loss: 1.8689 | Train Acc: 0.2363\n",
            "  Val   Loss: 1.8400 | Val   Acc: 0.2513\n",
            "  Best Val Acc: 0.2513\n",
            "  Loss Gap:   0.0289 | Acc  Gap:  -0.0151\n",
            "Epoch 2/60 (LR: 0.001500)\n",
            "  Train Loss: 1.8255 | Train Acc: 0.2621\n",
            "  Val   Loss: 1.8169 | Val   Acc: 0.2717\n",
            "  Best Val Acc: 0.2717\n",
            "  Loss Gap:   0.0087 | Acc  Gap:  -0.0096\n",
            "Epoch 3/60 (LR: 0.001500)\n",
            "  Train Loss: 1.8016 | Train Acc: 0.2843\n",
            "  Val   Loss: 1.7478 | Val   Acc: 0.3224\n",
            "  Best Val Acc: 0.3224\n",
            "  Loss Gap:   0.0538 | Acc  Gap:  -0.0381\n",
            "Epoch 4/60 (LR: 0.001500)\n",
            "  Train Loss: 1.7864 | Train Acc: 0.2931\n",
            "  Val   Loss: 1.7367 | Val   Acc: 0.3213\n",
            "  Best Val Acc: 0.3224\n",
            "  Loss Gap:   0.0498 | Acc  Gap:  -0.0282\n",
            "Epoch 5/60 (LR: 0.001500)\n",
            "  Train Loss: 1.7751 | Train Acc: 0.2993\n",
            "  Val   Loss: 1.7184 | Val   Acc: 0.3319\n",
            "  Best Val Acc: 0.3319\n",
            "  Loss Gap:   0.0567 | Acc  Gap:  -0.0326\n",
            "Epoch 6/60 (LR: 0.001500)\n",
            "  Train Loss: 1.7439 | Train Acc: 0.3115\n",
            "  Val   Loss: 1.6230 | Val   Acc: 0.3727\n",
            "  Best Val Acc: 0.3727\n",
            "  Loss Gap:   0.1209 | Acc  Gap:  -0.0612\n",
            "Epoch 7/60 (LR: 0.001500)\n",
            "  Train Loss: 1.7130 | Train Acc: 0.3327\n",
            "  Val   Loss: 1.6096 | Val   Acc: 0.3856\n",
            "  Best Val Acc: 0.3856\n",
            "  Loss Gap:   0.1034 | Acc  Gap:  -0.0529\n",
            "Epoch 8/60 (LR: 0.001500)\n",
            "  Train Loss: 1.6935 | Train Acc: 0.3460\n",
            "  Val   Loss: 1.5860 | Val   Acc: 0.4089\n",
            "  Best Val Acc: 0.4089\n",
            "  Loss Gap:   0.1074 | Acc  Gap:  -0.0629\n",
            "Epoch 9/60 (LR: 0.001500)\n",
            "  Train Loss: 1.6591 | Train Acc: 0.3675\n",
            "  Val   Loss: 1.5491 | Val   Acc: 0.4523\n",
            "  Best Val Acc: 0.4523\n",
            "  Loss Gap:   0.1100 | Acc  Gap:  -0.0848\n",
            "Epoch 10/60 (LR: 0.000750)\n",
            "  Train Loss: 1.6220 | Train Acc: 0.3866\n",
            "  Val   Loss: 1.5091 | Val   Acc: 0.4579\n",
            "  Best Val Acc: 0.4579\n",
            "  Loss Gap:   0.1129 | Acc  Gap:  -0.0712\n",
            "Epoch 11/60 (LR: 0.000750)\n",
            "  Train Loss: 1.5804 | Train Acc: 0.4127\n",
            "  Val   Loss: 1.4806 | Val   Acc: 0.4810\n",
            "  Best Val Acc: 0.4810\n",
            "  Loss Gap:   0.0998 | Acc  Gap:  -0.0683\n",
            "Epoch 12/60 (LR: 0.000750)\n",
            "  Train Loss: 1.5542 | Train Acc: 0.4370\n",
            "  Val   Loss: 1.4688 | Val   Acc: 0.4842\n",
            "  Best Val Acc: 0.4842\n",
            "  Loss Gap:   0.0854 | Acc  Gap:  -0.0471\n",
            "Epoch 13/60 (LR: 0.000750)\n",
            "  Train Loss: 1.5301 | Train Acc: 0.4507\n",
            "  Val   Loss: 1.4410 | Val   Acc: 0.5111\n",
            "  Best Val Acc: 0.5111\n",
            "  Loss Gap:   0.0891 | Acc  Gap:  -0.0605\n",
            "Epoch 14/60 (LR: 0.000750)\n",
            "  Train Loss: 1.5140 | Train Acc: 0.4614\n",
            "  Val   Loss: 1.4213 | Val   Acc: 0.5214\n",
            "  Best Val Acc: 0.5214\n",
            "  Loss Gap:   0.0927 | Acc  Gap:  -0.0601\n",
            "Epoch 15/60 (LR: 0.000750)\n",
            "  Train Loss: 1.5010 | Train Acc: 0.4653\n",
            "  Val   Loss: 1.3979 | Val   Acc: 0.5225\n",
            "  Best Val Acc: 0.5225\n",
            "  Loss Gap:   0.1031 | Acc  Gap:  -0.0571\n",
            "Epoch 16/60 (LR: 0.000750)\n",
            "  Train Loss: 1.4862 | Train Acc: 0.4787\n",
            "  Val   Loss: 1.3928 | Val   Acc: 0.5291\n",
            "  Best Val Acc: 0.5291\n",
            "  Loss Gap:   0.0934 | Acc  Gap:  -0.0504\n",
            "Epoch 17/60 (LR: 0.000750)\n",
            "  Train Loss: 1.4793 | Train Acc: 0.4855\n",
            "  Val   Loss: 1.3830 | Val   Acc: 0.5273\n",
            "  Best Val Acc: 0.5291\n",
            "  Loss Gap:   0.0963 | Acc  Gap:  -0.0419\n",
            "Epoch 18/60 (LR: 0.000750)\n",
            "  Train Loss: 1.4639 | Train Acc: 0.4910\n",
            "  Val   Loss: 1.3685 | Val   Acc: 0.5468\n",
            "  Best Val Acc: 0.5468\n",
            "  Loss Gap:   0.0954 | Acc  Gap:  -0.0558\n",
            "Epoch 19/60 (LR: 0.000750)\n",
            "  Train Loss: 1.4518 | Train Acc: 0.4982\n",
            "  Val   Loss: 1.3437 | Val   Acc: 0.5493\n",
            "  Best Val Acc: 0.5493\n",
            "  Loss Gap:   0.1081 | Acc  Gap:  -0.0510\n",
            "Epoch 20/60 (LR: 0.000375)\n",
            "  Train Loss: 1.4446 | Train Acc: 0.5066\n",
            "  Val   Loss: 1.3447 | Val   Acc: 0.5618\n",
            "  Best Val Acc: 0.5618\n",
            "  Loss Gap:   0.0999 | Acc  Gap:  -0.0553\n",
            "Epoch 21/60 (LR: 0.000375)\n",
            "  Train Loss: 1.4265 | Train Acc: 0.5173\n",
            "  Val   Loss: 1.3137 | Val   Acc: 0.5683\n",
            "  Best Val Acc: 0.5683\n",
            "  Loss Gap:   0.1128 | Acc  Gap:  -0.0510\n",
            "Epoch 22/60 (LR: 0.000375)\n",
            "  Train Loss: 1.4145 | Train Acc: 0.5242\n",
            "  Val   Loss: 1.3133 | Val   Acc: 0.5657\n",
            "  Best Val Acc: 0.5683\n",
            "  Loss Gap:   0.1012 | Acc  Gap:  -0.0414\n",
            "Epoch 23/60 (LR: 0.000375)\n",
            "  Train Loss: 1.4023 | Train Acc: 0.5308\n",
            "  Val   Loss: 1.3008 | Val   Acc: 0.5839\n",
            "  Best Val Acc: 0.5839\n",
            "  Loss Gap:   0.1015 | Acc  Gap:  -0.0532\n",
            "Epoch 24/60 (LR: 0.000375)\n",
            "  Train Loss: 1.3921 | Train Acc: 0.5393\n",
            "  Val   Loss: 1.2946 | Val   Acc: 0.5798\n",
            "  Best Val Acc: 0.5839\n",
            "  Loss Gap:   0.0975 | Acc  Gap:  -0.0405\n",
            "Epoch 25/60 (LR: 0.000375)\n",
            "  Train Loss: 1.3871 | Train Acc: 0.5364\n",
            "  Val   Loss: 1.2977 | Val   Acc: 0.5834\n",
            "  Best Val Acc: 0.5839\n",
            "  Loss Gap:   0.0894 | Acc  Gap:  -0.0470\n",
            "Epoch 26/60 (LR: 0.000375)\n",
            "  Train Loss: 1.3763 | Train Acc: 0.5482\n",
            "  Val   Loss: 1.2712 | Val   Acc: 0.5914\n",
            "  Best Val Acc: 0.5914\n",
            "  Loss Gap:   0.1051 | Acc  Gap:  -0.0433\n",
            "Epoch 27/60 (LR: 0.000375)\n",
            "  Train Loss: 1.3750 | Train Acc: 0.5475\n",
            "  Val   Loss: 1.2769 | Val   Acc: 0.5848\n",
            "  Best Val Acc: 0.5914\n",
            "  Loss Gap:   0.0980 | Acc  Gap:  -0.0373\n",
            "Epoch 28/60 (LR: 0.000375)\n",
            "  Train Loss: 1.3674 | Train Acc: 0.5514\n",
            "  Val   Loss: 1.2686 | Val   Acc: 0.5989\n",
            "  Best Val Acc: 0.5989\n",
            "  Loss Gap:   0.0988 | Acc  Gap:  -0.0475\n",
            "Epoch 29/60 (LR: 0.000375)\n",
            "  Train Loss: 1.3590 | Train Acc: 0.5538\n",
            "  Val   Loss: 1.2588 | Val   Acc: 0.5965\n",
            "  Best Val Acc: 0.5989\n",
            "  Loss Gap:   0.1002 | Acc  Gap:  -0.0427\n",
            "Epoch 30/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3557 | Train Acc: 0.5595\n",
            "  Val   Loss: 1.2555 | Val   Acc: 0.6029\n",
            "  Best Val Acc: 0.6029\n",
            "  Loss Gap:   0.1003 | Acc  Gap:  -0.0434\n",
            "Epoch 31/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3396 | Train Acc: 0.5692\n",
            "  Val   Loss: 1.2466 | Val   Acc: 0.6087\n",
            "  Best Val Acc: 0.6087\n",
            "  Loss Gap:   0.0930 | Acc  Gap:  -0.0395\n",
            "Epoch 32/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3325 | Train Acc: 0.5707\n",
            "  Val   Loss: 1.2436 | Val   Acc: 0.6113\n",
            "  Best Val Acc: 0.6113\n",
            "  Loss Gap:   0.0888 | Acc  Gap:  -0.0406\n",
            "Epoch 33/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3331 | Train Acc: 0.5710\n",
            "  Val   Loss: 1.2349 | Val   Acc: 0.6120\n",
            "  Best Val Acc: 0.6120\n",
            "  Loss Gap:   0.0982 | Acc  Gap:  -0.0409\n",
            "Epoch 34/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3281 | Train Acc: 0.5707\n",
            "  Val   Loss: 1.2395 | Val   Acc: 0.6094\n",
            "  Best Val Acc: 0.6120\n",
            "  Loss Gap:   0.0886 | Acc  Gap:  -0.0387\n",
            "Epoch 35/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3242 | Train Acc: 0.5775\n",
            "  Val   Loss: 1.2340 | Val   Acc: 0.6120\n",
            "  Best Val Acc: 0.6120\n",
            "  Loss Gap:   0.0902 | Acc  Gap:  -0.0345\n",
            "Epoch 36/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3202 | Train Acc: 0.5798\n",
            "  Val   Loss: 1.2330 | Val   Acc: 0.6087\n",
            "  Best Val Acc: 0.6120\n",
            "  Loss Gap:   0.0872 | Acc  Gap:  -0.0288\n",
            "Epoch 37/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3200 | Train Acc: 0.5788\n",
            "  Val   Loss: 1.2299 | Val   Acc: 0.6120\n",
            "  Best Val Acc: 0.6120\n",
            "  Loss Gap:   0.0901 | Acc  Gap:  -0.0332\n",
            "Epoch 38/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3138 | Train Acc: 0.5780\n",
            "  Val   Loss: 1.2285 | Val   Acc: 0.6113\n",
            "  Best Val Acc: 0.6120\n",
            "  Loss Gap:   0.0854 | Acc  Gap:  -0.0333\n",
            "Epoch 39/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3098 | Train Acc: 0.5859\n",
            "  Val   Loss: 1.2212 | Val   Acc: 0.6210\n",
            "  Best Val Acc: 0.6210\n",
            "  Loss Gap:   0.0886 | Acc  Gap:  -0.0351\n",
            "Epoch 40/60 (LR: 0.000094)\n",
            "  Train Loss: 1.3051 | Train Acc: 0.5830\n",
            "  Val   Loss: 1.2266 | Val   Acc: 0.6181\n",
            "  Best Val Acc: 0.6210\n",
            "  Loss Gap:   0.0785 | Acc  Gap:  -0.0351\n",
            "Epoch 41/60 (LR: 0.000094)\n",
            "  Train Loss: 1.3044 | Train Acc: 0.5853\n",
            "  Val   Loss: 1.2188 | Val   Acc: 0.6196\n",
            "  Best Val Acc: 0.6210\n",
            "  Loss Gap:   0.0856 | Acc  Gap:  -0.0343\n",
            "Epoch 42/60 (LR: 0.000094)\n",
            "  Train Loss: 1.3002 | Train Acc: 0.5901\n",
            "  Val   Loss: 1.2212 | Val   Acc: 0.6170\n",
            "  Best Val Acc: 0.6210\n",
            "  Loss Gap:   0.0790 | Acc  Gap:  -0.0269\n",
            "Epoch 43/60 (LR: 0.000094)\n",
            "  Train Loss: 1.2930 | Train Acc: 0.5905\n",
            "  Val   Loss: 1.2179 | Val   Acc: 0.6219\n",
            "  Best Val Acc: 0.6219\n",
            "  Loss Gap:   0.0751 | Acc  Gap:  -0.0315\n"
          ]
        }
      ],
      "source": [
        "model = train_model_with_advanced_techniques(epochs=60, lr=0.0015, dropout_rate=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BOUcEHG6qXw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "933bc9ef-7727-47a9-8b86-d07c156bd183"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250605_040748-sinszn4w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/sinszn4w' target=\"_blank\">FER_Advanced_Dropout_0.5_LR_0.0015_Epochs_60</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/sinszn4w' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/sinszn4w</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60 (LR: 0.001500)\n",
            "  Train Loss: 1.8653 | Train Acc: 0.2352\n",
            "  Val   Loss: 1.8333 | Val   Acc: 0.2513\n",
            "  Best Val Acc: 0.2513\n",
            "  Loss Gap:   0.0320 | Acc  Gap:  -0.0161\n",
            "Epoch 2/60 (LR: 0.001500)\n",
            "  Train Loss: 1.8396 | Train Acc: 0.2500\n",
            "  Val   Loss: 1.8231 | Val   Acc: 0.2515\n",
            "  Best Val Acc: 0.2515\n",
            "  Loss Gap:   0.0165 | Acc  Gap:  -0.0015\n",
            "Epoch 3/60 (LR: 0.001500)\n",
            "  Train Loss: 1.8317 | Train Acc: 0.2565\n",
            "  Val   Loss: 1.8071 | Val   Acc: 0.2530\n",
            "  Best Val Acc: 0.2530\n",
            "  Loss Gap:   0.0246 | Acc  Gap:  0.0035\n",
            "Epoch 4/60 (LR: 0.001500)\n",
            "  Train Loss: 1.8109 | Train Acc: 0.2676\n",
            "  Val   Loss: 1.7393 | Val   Acc: 0.3015\n",
            "  Best Val Acc: 0.3015\n",
            "  Loss Gap:   0.0717 | Acc  Gap:  -0.0339\n",
            "Epoch 5/60 (LR: 0.001500)\n",
            "  Train Loss: 1.7642 | Train Acc: 0.2907\n",
            "  Val   Loss: 1.6626 | Val   Acc: 0.3814\n",
            "  Best Val Acc: 0.3814\n",
            "  Loss Gap:   0.1016 | Acc  Gap:  -0.0907\n",
            "Epoch 6/60 (LR: 0.001500)\n",
            "  Train Loss: 1.7221 | Train Acc: 0.3224\n",
            "  Val   Loss: 1.6159 | Val   Acc: 0.3962\n",
            "  Best Val Acc: 0.3962\n",
            "  Loss Gap:   0.1062 | Acc  Gap:  -0.0738\n",
            "Epoch 7/60 (LR: 0.001500)\n",
            "  Train Loss: 1.6943 | Train Acc: 0.3382\n",
            "  Val   Loss: 1.5826 | Val   Acc: 0.4220\n",
            "  Best Val Acc: 0.4220\n",
            "  Loss Gap:   0.1117 | Acc  Gap:  -0.0838\n",
            "Epoch 8/60 (LR: 0.001500)\n",
            "  Train Loss: 1.6636 | Train Acc: 0.3610\n",
            "  Val   Loss: 1.5419 | Val   Acc: 0.4199\n",
            "  Best Val Acc: 0.4220\n",
            "  Loss Gap:   0.1217 | Acc  Gap:  -0.0588\n",
            "Epoch 9/60 (LR: 0.001500)\n",
            "  Train Loss: 1.6361 | Train Acc: 0.3726\n",
            "  Val   Loss: 1.5522 | Val   Acc: 0.4309\n",
            "  Best Val Acc: 0.4309\n",
            "  Loss Gap:   0.0839 | Acc  Gap:  -0.0582\n",
            "Epoch 10/60 (LR: 0.000750)\n",
            "  Train Loss: 1.6094 | Train Acc: 0.3878\n",
            "  Val   Loss: 1.5092 | Val   Acc: 0.4505\n",
            "  Best Val Acc: 0.4505\n",
            "  Loss Gap:   0.1002 | Acc  Gap:  -0.0628\n",
            "Epoch 11/60 (LR: 0.000750)\n",
            "  Train Loss: 1.5703 | Train Acc: 0.4085\n",
            "  Val   Loss: 1.4783 | Val   Acc: 0.4512\n",
            "  Best Val Acc: 0.4512\n",
            "  Loss Gap:   0.0920 | Acc  Gap:  -0.0427\n",
            "Epoch 12/60 (LR: 0.000750)\n",
            "  Train Loss: 1.5469 | Train Acc: 0.4228\n",
            "  Val   Loss: 1.4687 | Val   Acc: 0.4763\n",
            "  Best Val Acc: 0.4763\n",
            "  Loss Gap:   0.0782 | Acc  Gap:  -0.0535\n",
            "Epoch 13/60 (LR: 0.000750)\n",
            "  Train Loss: 1.5329 | Train Acc: 0.4381\n",
            "  Val   Loss: 1.4456 | Val   Acc: 0.4934\n",
            "  Best Val Acc: 0.4934\n",
            "  Loss Gap:   0.0872 | Acc  Gap:  -0.0553\n",
            "Epoch 14/60 (LR: 0.000750)\n",
            "  Train Loss: 1.5218 | Train Acc: 0.4485\n",
            "  Val   Loss: 1.4335 | Val   Acc: 0.5066\n",
            "  Best Val Acc: 0.5066\n",
            "  Loss Gap:   0.0883 | Acc  Gap:  -0.0581\n",
            "Epoch 15/60 (LR: 0.000750)\n",
            "  Train Loss: 1.5137 | Train Acc: 0.4508\n",
            "  Val   Loss: 1.4215 | Val   Acc: 0.5162\n",
            "  Best Val Acc: 0.5162\n",
            "  Loss Gap:   0.0922 | Acc  Gap:  -0.0654\n",
            "Epoch 16/60 (LR: 0.000750)\n",
            "  Train Loss: 1.5015 | Train Acc: 0.4606\n",
            "  Val   Loss: 1.4144 | Val   Acc: 0.5310\n",
            "  Best Val Acc: 0.5310\n",
            "  Loss Gap:   0.0871 | Acc  Gap:  -0.0704\n",
            "Epoch 17/60 (LR: 0.000750)\n",
            "  Train Loss: 1.4858 | Train Acc: 0.4729\n",
            "  Val   Loss: 1.3817 | Val   Acc: 0.5347\n",
            "  Best Val Acc: 0.5347\n",
            "  Loss Gap:   0.1041 | Acc  Gap:  -0.0618\n",
            "Epoch 18/60 (LR: 0.000750)\n",
            "  Train Loss: 1.4751 | Train Acc: 0.4765\n",
            "  Val   Loss: 1.3718 | Val   Acc: 0.5326\n",
            "  Best Val Acc: 0.5347\n",
            "  Loss Gap:   0.1033 | Acc  Gap:  -0.0561\n",
            "Epoch 19/60 (LR: 0.000750)\n",
            "  Train Loss: 1.4633 | Train Acc: 0.4923\n",
            "  Val   Loss: 1.3649 | Val   Acc: 0.5425\n",
            "  Best Val Acc: 0.5425\n",
            "  Loss Gap:   0.0984 | Acc  Gap:  -0.0502\n",
            "Epoch 20/60 (LR: 0.000375)\n",
            "  Train Loss: 1.4562 | Train Acc: 0.4921\n",
            "  Val   Loss: 1.3496 | Val   Acc: 0.5528\n",
            "  Best Val Acc: 0.5528\n",
            "  Loss Gap:   0.1066 | Acc  Gap:  -0.0606\n",
            "Epoch 21/60 (LR: 0.000375)\n",
            "  Train Loss: 1.4392 | Train Acc: 0.5069\n",
            "  Val   Loss: 1.3363 | Val   Acc: 0.5550\n",
            "  Best Val Acc: 0.5550\n",
            "  Loss Gap:   0.1029 | Acc  Gap:  -0.0481\n",
            "Epoch 22/60 (LR: 0.000375)\n",
            "  Train Loss: 1.4239 | Train Acc: 0.5159\n",
            "  Val   Loss: 1.3271 | Val   Acc: 0.5700\n",
            "  Best Val Acc: 0.5700\n",
            "  Loss Gap:   0.0968 | Acc  Gap:  -0.0541\n",
            "Epoch 23/60 (LR: 0.000375)\n",
            "  Train Loss: 1.4248 | Train Acc: 0.5119\n",
            "  Val   Loss: 1.3185 | Val   Acc: 0.5724\n",
            "  Best Val Acc: 0.5724\n",
            "  Loss Gap:   0.1063 | Acc  Gap:  -0.0605\n",
            "Epoch 24/60 (LR: 0.000375)\n",
            "  Train Loss: 1.4106 | Train Acc: 0.5265\n",
            "  Val   Loss: 1.3062 | Val   Acc: 0.5799\n",
            "  Best Val Acc: 0.5799\n",
            "  Loss Gap:   0.1044 | Acc  Gap:  -0.0534\n",
            "Epoch 25/60 (LR: 0.000375)\n",
            "  Train Loss: 1.4029 | Train Acc: 0.5266\n",
            "  Val   Loss: 1.2957 | Val   Acc: 0.5874\n",
            "  Best Val Acc: 0.5874\n",
            "  Loss Gap:   0.1073 | Acc  Gap:  -0.0608\n",
            "Epoch 26/60 (LR: 0.000375)\n",
            "  Train Loss: 1.3931 | Train Acc: 0.5345\n",
            "  Val   Loss: 1.2945 | Val   Acc: 0.5806\n",
            "  Best Val Acc: 0.5874\n",
            "  Loss Gap:   0.0985 | Acc  Gap:  -0.0461\n",
            "Epoch 27/60 (LR: 0.000375)\n",
            "  Train Loss: 1.3841 | Train Acc: 0.5379\n",
            "  Val   Loss: 1.2860 | Val   Acc: 0.5907\n",
            "  Best Val Acc: 0.5907\n",
            "  Loss Gap:   0.0981 | Acc  Gap:  -0.0529\n",
            "Epoch 28/60 (LR: 0.000375)\n",
            "  Train Loss: 1.3796 | Train Acc: 0.5433\n",
            "  Val   Loss: 1.2883 | Val   Acc: 0.5942\n",
            "  Best Val Acc: 0.5942\n",
            "  Loss Gap:   0.0914 | Acc  Gap:  -0.0510\n",
            "Epoch 29/60 (LR: 0.000375)\n",
            "  Train Loss: 1.3734 | Train Acc: 0.5470\n",
            "  Val   Loss: 1.2754 | Val   Acc: 0.5991\n",
            "  Best Val Acc: 0.5991\n",
            "  Loss Gap:   0.0980 | Acc  Gap:  -0.0521\n",
            "Epoch 30/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3678 | Train Acc: 0.5524\n",
            "  Val   Loss: 1.2667 | Val   Acc: 0.5951\n",
            "  Best Val Acc: 0.5991\n",
            "  Loss Gap:   0.1011 | Acc  Gap:  -0.0426\n",
            "Epoch 31/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3573 | Train Acc: 0.5540\n",
            "  Val   Loss: 1.2538 | Val   Acc: 0.6062\n",
            "  Best Val Acc: 0.6062\n",
            "  Loss Gap:   0.1035 | Acc  Gap:  -0.0523\n",
            "Epoch 32/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3469 | Train Acc: 0.5595\n",
            "  Val   Loss: 1.2612 | Val   Acc: 0.6059\n",
            "  Best Val Acc: 0.6062\n",
            "  Loss Gap:   0.0857 | Acc  Gap:  -0.0464\n",
            "Epoch 33/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3425 | Train Acc: 0.5649\n",
            "  Val   Loss: 1.2494 | Val   Acc: 0.6088\n",
            "  Best Val Acc: 0.6088\n",
            "  Loss Gap:   0.0931 | Acc  Gap:  -0.0440\n",
            "Epoch 34/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3437 | Train Acc: 0.5637\n",
            "  Val   Loss: 1.2425 | Val   Acc: 0.6139\n",
            "  Best Val Acc: 0.6139\n",
            "  Loss Gap:   0.1012 | Acc  Gap:  -0.0502\n",
            "Epoch 35/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3344 | Train Acc: 0.5714\n",
            "  Val   Loss: 1.2446 | Val   Acc: 0.6129\n",
            "  Best Val Acc: 0.6139\n",
            "  Loss Gap:   0.0898 | Acc  Gap:  -0.0415\n",
            "Epoch 36/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3342 | Train Acc: 0.5668\n",
            "  Val   Loss: 1.2403 | Val   Acc: 0.6137\n",
            "  Best Val Acc: 0.6139\n",
            "  Loss Gap:   0.0940 | Acc  Gap:  -0.0470\n",
            "Epoch 37/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3320 | Train Acc: 0.5689\n",
            "  Val   Loss: 1.2343 | Val   Acc: 0.6129\n",
            "  Best Val Acc: 0.6139\n",
            "  Loss Gap:   0.0977 | Acc  Gap:  -0.0439\n",
            "Epoch 38/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3273 | Train Acc: 0.5727\n",
            "  Val   Loss: 1.2343 | Val   Acc: 0.6184\n",
            "  Best Val Acc: 0.6184\n",
            "  Loss Gap:   0.0929 | Acc  Gap:  -0.0457\n",
            "Epoch 39/60 (LR: 0.000188)\n",
            "  Train Loss: 1.3222 | Train Acc: 0.5744\n",
            "  Val   Loss: 1.2329 | Val   Acc: 0.6177\n",
            "  Best Val Acc: 0.6184\n",
            "  Loss Gap:   0.0893 | Acc  Gap:  -0.0433\n",
            "Epoch 40/60 (LR: 0.000094)\n",
            "  Train Loss: 1.3188 | Train Acc: 0.5767\n",
            "  Val   Loss: 1.2249 | Val   Acc: 0.6165\n",
            "  Best Val Acc: 0.6184\n",
            "  Loss Gap:   0.0939 | Acc  Gap:  -0.0399\n",
            "Epoch 41/60 (LR: 0.000094)\n",
            "  Train Loss: 1.3103 | Train Acc: 0.5834\n",
            "  Val   Loss: 1.2271 | Val   Acc: 0.6203\n",
            "  Best Val Acc: 0.6203\n",
            "  Loss Gap:   0.0831 | Acc  Gap:  -0.0369\n",
            "Epoch 42/60 (LR: 0.000094)\n",
            "  Train Loss: 1.3097 | Train Acc: 0.5823\n",
            "  Val   Loss: 1.2263 | Val   Acc: 0.6165\n",
            "  Best Val Acc: 0.6203\n",
            "  Loss Gap:   0.0834 | Acc  Gap:  -0.0342\n",
            "Epoch 43/60 (LR: 0.000094)\n",
            "  Train Loss: 1.3077 | Train Acc: 0.5885\n",
            "  Val   Loss: 1.2199 | Val   Acc: 0.6223\n",
            "  Best Val Acc: 0.6223\n",
            "  Loss Gap:   0.0877 | Acc  Gap:  -0.0338\n",
            "Epoch 44/60 (LR: 0.000094)\n",
            "  Train Loss: 1.3009 | Train Acc: 0.5871\n",
            "  Val   Loss: 1.2207 | Val   Acc: 0.6243\n",
            "  Best Val Acc: 0.6243\n",
            "  Loss Gap:   0.0803 | Acc  Gap:  -0.0372\n",
            "Epoch 45/60 (LR: 0.000094)\n",
            "  Train Loss: 1.3024 | Train Acc: 0.5838\n",
            "  Val   Loss: 1.2186 | Val   Acc: 0.6224\n",
            "  Best Val Acc: 0.6243\n",
            "  Loss Gap:   0.0837 | Acc  Gap:  -0.0386\n",
            "Epoch 46/60 (LR: 0.000094)\n",
            "  Train Loss: 1.3040 | Train Acc: 0.5885\n",
            "  Val   Loss: 1.2156 | Val   Acc: 0.6231\n",
            "  Best Val Acc: 0.6243\n",
            "  Loss Gap:   0.0884 | Acc  Gap:  -0.0346\n",
            "Epoch 47/60 (LR: 0.000094)\n",
            "  Train Loss: 1.2988 | Train Acc: 0.5883\n",
            "  Val   Loss: 1.2147 | Val   Acc: 0.6284\n",
            "  Best Val Acc: 0.6284\n",
            "  Loss Gap:   0.0842 | Acc  Gap:  -0.0401\n",
            "Epoch 48/60 (LR: 0.000094)\n",
            "  Train Loss: 1.2961 | Train Acc: 0.5903\n",
            "  Val   Loss: 1.2149 | Val   Acc: 0.6254\n",
            "  Best Val Acc: 0.6284\n",
            "  Loss Gap:   0.0812 | Acc  Gap:  -0.0351\n",
            "Epoch 49/60 (LR: 0.000094)\n",
            "  Train Loss: 1.2968 | Train Acc: 0.5879\n",
            "  Val   Loss: 1.2111 | Val   Acc: 0.6282\n",
            "  Best Val Acc: 0.6284\n",
            "  Loss Gap:   0.0857 | Acc  Gap:  -0.0402\n",
            "Epoch 50/60 (LR: 0.000047)\n",
            "  Train Loss: 1.2971 | Train Acc: 0.5935\n",
            "  Val   Loss: 1.2098 | Val   Acc: 0.6266\n",
            "  Best Val Acc: 0.6284\n",
            "  Loss Gap:   0.0874 | Acc  Gap:  -0.0331\n",
            "Epoch 51/60 (LR: 0.000047)\n",
            "  Train Loss: 1.2927 | Train Acc: 0.5933\n",
            "  Val   Loss: 1.2090 | Val   Acc: 0.6275\n",
            "  Best Val Acc: 0.6284\n",
            "  Loss Gap:   0.0838 | Acc  Gap:  -0.0342\n",
            "Epoch 52/60 (LR: 0.000047)\n",
            "  Train Loss: 1.2881 | Train Acc: 0.5950\n",
            "  Val   Loss: 1.2073 | Val   Acc: 0.6270\n",
            "  Best Val Acc: 0.6284\n",
            "  Loss Gap:   0.0808 | Acc  Gap:  -0.0319\n",
            "Epoch 53/60 (LR: 0.000047)\n",
            "  Train Loss: 1.2915 | Train Acc: 0.5925\n",
            "  Val   Loss: 1.2080 | Val   Acc: 0.6287\n",
            "  Best Val Acc: 0.6287\n",
            "  Loss Gap:   0.0835 | Acc  Gap:  -0.0362\n",
            "Epoch 54/60 (LR: 0.000047)\n",
            "  Train Loss: 1.2864 | Train Acc: 0.5956\n",
            "  Val   Loss: 1.2086 | Val   Acc: 0.6303\n",
            "  Best Val Acc: 0.6303\n",
            "  Loss Gap:   0.0778 | Acc  Gap:  -0.0347\n",
            "Epoch 55/60 (LR: 0.000047)\n",
            "  Train Loss: 1.2878 | Train Acc: 0.5926\n",
            "  Val   Loss: 1.2060 | Val   Acc: 0.6306\n",
            "  Best Val Acc: 0.6306\n",
            "  Loss Gap:   0.0818 | Acc  Gap:  -0.0380\n",
            "Epoch 56/60 (LR: 0.000047)\n",
            "  Train Loss: 1.2835 | Train Acc: 0.5952\n",
            "  Val   Loss: 1.2052 | Val   Acc: 0.6292\n",
            "  Best Val Acc: 0.6306\n",
            "  Loss Gap:   0.0783 | Acc  Gap:  -0.0340\n",
            "Epoch 57/60 (LR: 0.000047)\n",
            "  Train Loss: 1.2869 | Train Acc: 0.5948\n",
            "  Val   Loss: 1.2046 | Val   Acc: 0.6308\n",
            "  Best Val Acc: 0.6308\n",
            "  Loss Gap:   0.0822 | Acc  Gap:  -0.0360\n",
            "Epoch 58/60 (LR: 0.000047)\n",
            "  Train Loss: 1.2834 | Train Acc: 0.5981\n",
            "  Val   Loss: 1.2044 | Val   Acc: 0.6322\n",
            "  Best Val Acc: 0.6322\n",
            "  Loss Gap:   0.0791 | Acc  Gap:  -0.0341\n",
            "Epoch 59/60 (LR: 0.000047)\n",
            "  Train Loss: 1.2817 | Train Acc: 0.5946\n",
            "  Val   Loss: 1.2042 | Val   Acc: 0.6304\n",
            "  Best Val Acc: 0.6322\n",
            "  Loss Gap:   0.0775 | Acc  Gap:  -0.0359\n",
            "Epoch 60/60 (LR: 0.000023)\n",
            "  Train Loss: 1.2770 | Train Acc: 0.6023\n",
            "  Val   Loss: 1.2049 | Val   Acc: 0.6310\n",
            "  Best Val Acc: 0.6322\n",
            "  Loss Gap:   0.0721 | Acc  Gap:  -0.0286\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▇█▅▁▂▃▅▄▄▃▃▃▄▃▄▃▄▃▄▄▅▄▄▄▅▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>best_val_accuracy</td><td>▁▁▂▃▄▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>learning_rate</td><td>██████▄▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_gap</td><td>▁▂▅██▆▇▇▆▆▇▇█▇▇▇█▇▇▇▆▇▆▇▇▆▇▆▆▆▆▆▆▆▆▆▆▆▆▅</td></tr><tr><td>train_accuracy</td><td>▁▁▂▃▃▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>train_loss</td><td>███▇▇▆▅▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▄▄▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇█████████████████████</td></tr><tr><td>val_loss</td><td>██▇▆▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>-0.02862</td></tr><tr><td>best_val_accuracy</td><td>0.63218</td></tr><tr><td>epoch</td><td>60</td></tr><tr><td>learning_rate</td><td>2e-05</td></tr><tr><td>loss_gap</td><td>0.07208</td></tr><tr><td>train_accuracy</td><td>0.60234</td></tr><tr><td>train_loss</td><td>1.27702</td></tr><tr><td>val_accuracy</td><td>0.63096</td></tr><tr><td>val_loss</td><td>1.20494</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_Advanced_Dropout_0.5_LR_0.0015_Epochs_60</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/sinszn4w' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/sinszn4w</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250605_040748-sinszn4w/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_advanced_1 = train_model_with_advanced_techniques(epochs=60, lr=0.0015, dropout_rate=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save best model"
      ],
      "metadata": {
        "id": "1rksvgLjW23R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "# Start a run just for uploading the model (can be short)\n",
        "run = wandb.init(project=\"ML_4\", entity=\"nkhar21-student\", name=\"upload_best_fer_model\")\n",
        "\n",
        "# Create an artifact\n",
        "artifact = wandb.Artifact(name=\"best_fer_model\", type=\"model\")\n",
        "artifact.add_file(\"best_fer_model.pth\")\n",
        "\n",
        "# Log the artifact\n",
        "run.log_artifact(artifact)\n",
        "\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "LpGroOp7UTKe",
        "outputId": "937fa019-7e4f-4d34-c0e2-014f0d8ebb48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250605_135032-usoupy2x</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/usoupy2x' target=\"_blank\">upload_best_fer_model</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/usoupy2x' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/usoupy2x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">upload_best_fer_model</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/usoupy2x' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/usoupy2x</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250605_135032-usoupy2x/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trianing 3 - ResNet"
      ],
      "metadata": {
        "id": "23-QTcPtYlQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0 - Models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "# ResNet Building Blocks\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, dropout_rate=0.0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else None\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        if self.dropout:\n",
        "            out = self.dropout(out)\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, dropout_rate=0.0):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
        "        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else None\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        if self.dropout:\n",
        "            out = self.dropout(out)\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        if self.dropout:\n",
        "            out = self.dropout(out)\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "# ResNet Model\n",
        "class FERResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=7, dropout_rate=0.5):\n",
        "        super(FERResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Initial convolution layer\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # ResNet layers\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "\n",
        "        # Global average pooling and classifier\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride, self.dropout_rate * 0.3))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.maxpool(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Factory functions for different ResNet variants\n",
        "def FERResNet18(num_classes=7, dropout_rate=0.5):\n",
        "    return FERResNet(BasicBlock, [2, 2, 2, 2], num_classes, dropout_rate)\n",
        "\n",
        "def FERResNet34(num_classes=7, dropout_rate=0.5):\n",
        "    return FERResNet(BasicBlock, [3, 4, 6, 3], num_classes, dropout_rate)\n",
        "\n",
        "def FERResNet50(num_classes=7, dropout_rate=0.5):\n",
        "    return FERResNet(Bottleneck, [3, 4, 6, 3], num_classes, dropout_rate)\n"
      ],
      "metadata": {
        "id": "p5V719TgYoFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 - Data Augementation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "class FERDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, images, labels, transform=None, use_mixup=False, mixup_alpha=0.2):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        self.use_mixup = use_mixup\n",
        "        self.mixup_alpha = mixup_alpha\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Handle different image shapes\n",
        "        image = self.images[idx]\n",
        "\n",
        "        # Handle different input formats\n",
        "        if len(image.shape) == 1:\n",
        "            # Flattened image (2304,) -> (48, 48)\n",
        "            image = image.reshape(48, 48)\n",
        "        elif len(image.shape) == 3:\n",
        "            # Handle (48, 48, 1) or (1, 48, 48) formats\n",
        "            if image.shape[-1] == 1:\n",
        "                # (48, 48, 1) -> (48, 48)\n",
        "                image = image.squeeze(-1)\n",
        "            elif image.shape[0] == 1:\n",
        "                # (1, 48, 48) -> (48, 48)\n",
        "                image = image.squeeze(0)\n",
        "            else:\n",
        "                # If it's RGB or other format, take first channel\n",
        "                image = image[:, :, 0] if image.shape[-1] > 1 else image[0, :, :]\n",
        "        elif len(image.shape) == 2:\n",
        "            # Already (48, 48), good to go\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected image shape: {image.shape}\")\n",
        "\n",
        "        # Ensure we have the right data type and range\n",
        "        if image.dtype != np.uint8:\n",
        "            # If image is normalized (0-1), convert back to 0-255\n",
        "            if image.max() <= 1.0:\n",
        "                image = (image * 255).astype(np.uint8)\n",
        "            else:\n",
        "                image = image.astype(np.uint8)\n",
        "\n",
        "        # Convert to PIL Image\n",
        "        image = Image.fromarray(image, mode='L')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.use_mixup and hasattr(self, 'training') and self.training:\n",
        "            # Simple mixup implementation\n",
        "            if random.random() < 0.5:\n",
        "                mix_idx = random.randint(0, len(self.images) - 1)\n",
        "                mix_image = self.images[mix_idx]\n",
        "\n",
        "                # Handle different shapes for mix_image too\n",
        "                if len(mix_image.shape) == 1:\n",
        "                    mix_image = mix_image.reshape(48, 48)\n",
        "                elif len(mix_image.shape) == 3:\n",
        "                    if mix_image.shape[-1] == 1:\n",
        "                        mix_image = mix_image.squeeze(-1)\n",
        "                    elif mix_image.shape[0] == 1:\n",
        "                        mix_image = mix_image.squeeze(0)\n",
        "                    else:\n",
        "                        mix_image = mix_image[:, :, 0] if mix_image.shape[-1] > 1 else mix_image[0, :, :]\n",
        "\n",
        "                if mix_image.dtype != np.uint8:\n",
        "                    if mix_image.max() <= 1.0:\n",
        "                        mix_image = (mix_image * 255).astype(np.uint8)\n",
        "                    else:\n",
        "                        mix_image = mix_image.astype(np.uint8)\n",
        "\n",
        "                mix_image = Image.fromarray(mix_image, mode='L')\n",
        "                mix_label = self.labels[mix_idx]\n",
        "\n",
        "                if self.transform:\n",
        "                    mix_image = self.transform(mix_image)\n",
        "\n",
        "                lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n",
        "                image = lam * image + (1 - lam) * mix_image\n",
        "                label = (label, mix_label, lam)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Fixed Enhanced Data Augmentation\n",
        "def get_train_transforms():\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((56, 56)),  # Slightly larger for random crop\n",
        "        transforms.RandomCrop(48),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "        transforms.RandomApply([\n",
        "            transforms.ColorJitter(brightness=0.3, contrast=0.3)\n",
        "        ], p=0.3),\n",
        "        transforms.RandomApply([\n",
        "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0))\n",
        "        ], p=0.2),\n",
        "        transforms.ToTensor(),  # Convert to tensor first\n",
        "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.15)),  # Apply RandomErasing after ToTensor\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
        "    ])\n",
        "\n",
        "def get_val_transforms():\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((48, 48)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])\n",
        "\n",
        "# Alternative training transforms with more robust augmentation\n",
        "def get_train_transforms_alternative():\n",
        "    \"\"\"Alternative transform pipeline that's more robust\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((52, 52)),  # Slightly larger for random crop\n",
        "        transforms.RandomCrop(48, padding=4),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=10),\n",
        "        transforms.RandomApply([\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
        "        ], p=0.3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomErasing(p=0.15, scale=(0.02, 0.12)),\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])"
      ],
      "metadata": {
        "id": "Lg3IVpQDY0v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 - Evaluation\n",
        "def evaluate_model_with_augmentation(model, test_loader, device, num_augmentations=5):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Test Time Augmentation (TTA)\n",
        "            batch_predictions = []\n",
        "\n",
        "            # Original image\n",
        "            outputs = model(images)\n",
        "            batch_predictions.append(F.softmax(outputs, dim=1))\n",
        "\n",
        "            # Augmented versions\n",
        "            for _ in range(num_augmentations - 1):\n",
        "                # Random horizontal flip\n",
        "                if random.random() > 0.5:\n",
        "                    aug_images = torch.flip(images, dims=[3])\n",
        "                else:\n",
        "                    aug_images = images\n",
        "\n",
        "                outputs = model(aug_images)\n",
        "                batch_predictions.append(F.softmax(outputs, dim=1))\n",
        "\n",
        "            # Average predictions\n",
        "            avg_predictions = torch.mean(torch.stack(batch_predictions), dim=0)\n",
        "            _, predicted = torch.max(avg_predictions, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy, all_predictions, all_labels\n"
      ],
      "metadata": {
        "id": "amwNUEoIZD8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 - Training\n",
        "def train_model_with_advanced_techniques(model_type='resnet18', epochs=60, lr=0.0015,\n",
        "                                       dropout_rate=0.5, batch_size=128,\n",
        "                                       weight_decay=1e-4, save_path='fer_resnet_model.pth',\n",
        "                                       x_train=None, y_train=None, x_val=None, y_val=None):\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize model based on type - assuming these are imported from your models file\n",
        "    if model_type == 'resnet18':\n",
        "        model = FERResNet18(num_classes=7, dropout_rate=dropout_rate)\n",
        "    elif model_type == 'resnet34':\n",
        "        model = FERResNet34(num_classes=7, dropout_rate=dropout_rate)\n",
        "    elif model_type == 'resnet50':\n",
        "        model = FERResNet50(num_classes=7, dropout_rate=dropout_rate)\n",
        "    else:\n",
        "        raise ValueError(\"model_type must be 'resnet18', 'resnet34', or 'resnet50'\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Handle data - if not provided as parameters, try to use global variables\n",
        "    if x_train is None:\n",
        "        try:\n",
        "            # Try to access global variables (modify variable names as needed)\n",
        "            import sys\n",
        "            frame = sys._getframe(1)\n",
        "            x_train = frame.f_globals.get('x_train')\n",
        "            y_train = frame.f_globals.get('y_train')\n",
        "            x_val = frame.f_globals.get('x_val')\n",
        "            y_val = frame.f_globals.get('y_val')\n",
        "\n",
        "            if x_train is None:\n",
        "                raise ValueError(\"Please provide training data as parameters or ensure x_train, y_train, x_val, y_val are available\")\n",
        "        except:\n",
        "            raise ValueError(\"Please provide x_train, y_train, x_val, y_val as function parameters\")\n",
        "\n",
        "    print(f\"Training data shape: {x_train.shape}, Validation data shape: {x_val.shape}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = FERDataset(x_train, y_train, transform=get_train_transforms(), use_mixup=False)\n",
        "    val_dataset = FERDataset(x_val, y_val, transform=get_val_transforms())\n",
        "\n",
        "    # Create data loaders with num_workers=0 to avoid multiprocessing issues\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,\n",
        "                                           shuffle=False, num_workers=0)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5,\n",
        "                                verbose=True, min_lr=1e-6)\n",
        "\n",
        "    # Training variables\n",
        "    best_val_acc = 0.0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    print(f\"Training {model_type.upper()} for {epochs} epochs...\")\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_acc = correct_train / total_train\n",
        "        val_acc = correct_val / total_val\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        # Store metrics\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_acc)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_acc': best_val_acc,\n",
        "                'model_type': model_type,\n",
        "                'dropout_rate': dropout_rate\n",
        "            }, save_path)\n",
        "\n",
        "        # Print progress\n",
        "        print(f'Epoch {epoch+1}/{epochs} (LR: {current_lr:.6f})')\n",
        "        print(f'  Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f}')\n",
        "        print(f'  Val   Loss: {avg_val_loss:.4f} | Val   Acc: {val_acc:.4f}')\n",
        "        print(f'  Best Val Acc: {best_val_acc:.4f}')\n",
        "        print(f'  Loss Gap:   {avg_train_loss - avg_val_loss:.4f} | Acc  Gap:  {train_acc - val_acc:.4f}')\n",
        "        print()\n",
        "\n",
        "    print(f\"Training completed! Best validation accuracy: {best_val_acc:.4f}\")\n",
        "    print(f\"Model saved as: {save_path}\")\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load(save_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "GDaABzTmZQqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 - Loading model\n",
        "# Function to load saved model\n",
        "def load_fer_resnet_model(model_path, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model_type = checkpoint['model_type']\n",
        "    dropout_rate = checkpoint['dropout_rate']\n",
        "\n",
        "    if model_type == 'resnet18':\n",
        "        model = FERResNet18(num_classes=7, dropout_rate=dropout_rate)\n",
        "    elif model_type == 'resnet34':\n",
        "        model = FERResNet34(num_classes=7, dropout_rate=dropout_rate)\n",
        "    elif model_type == 'resnet50':\n",
        "        model = FERResNet50(num_classes=7, dropout_rate=dropout_rate)\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.to(device)\n",
        "\n",
        "    print(f\"Loaded {model_type.upper()} model with validation accuracy: {checkpoint['best_val_acc']:.4f}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZQGXX3CoZXQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_resnet18 = train_model_with_advanced_techniques(\n",
        "    model_type='resnet18',\n",
        "    epochs=60,\n",
        "    lr=0.0015,\n",
        "    dropout_rate=0.5,\n",
        "    save_path='fer_resnet18_model.pth',\n",
        "    x_train=x_train,  # your training data\n",
        "    y_train=y_train,  # your training labels\n",
        "    x_val=x_val,      # your validation data\n",
        "    y_val=y_val       # your validation labels\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MiIEU3dWZf2Q",
        "outputId": "064996d4-d9a0-4164-b08b-0745100fc579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Training data shape: (22967, 48, 48, 1), Validation data shape: (5742, 48, 48, 1)\n",
            "Training RESNET18 for 60 epochs...\n",
            "Model parameters: 11,173,831\n",
            "Epoch 1/60 (LR: 0.001500)\n",
            "  Train Loss: 1.8297 | Train Acc: 0.2611\n",
            "  Val   Loss: 1.7137 | Val   Acc: 0.3457\n",
            "  Best Val Acc: 0.3457\n",
            "  Loss Gap:   0.1160 | Acc  Gap:  -0.0846\n",
            "\n",
            "Epoch 2/60 (LR: 0.001500)\n",
            "  Train Loss: 1.7165 | Train Acc: 0.3424\n",
            "  Val   Loss: 1.7362 | Val   Acc: 0.3718\n",
            "  Best Val Acc: 0.3718\n",
            "  Loss Gap:   -0.0196 | Acc  Gap:  -0.0294\n",
            "\n",
            "Epoch 3/60 (LR: 0.001500)\n",
            "  Train Loss: 1.6585 | Train Acc: 0.3779\n",
            "  Val   Loss: 1.5656 | Val   Acc: 0.4270\n",
            "  Best Val Acc: 0.4270\n",
            "  Loss Gap:   0.0929 | Acc  Gap:  -0.0491\n",
            "\n",
            "Epoch 4/60 (LR: 0.001500)\n",
            "  Train Loss: 1.6117 | Train Acc: 0.4101\n",
            "  Val   Loss: 1.5088 | Val   Acc: 0.4592\n",
            "  Best Val Acc: 0.4592\n",
            "  Loss Gap:   0.1029 | Acc  Gap:  -0.0492\n",
            "\n",
            "Epoch 5/60 (LR: 0.001500)\n",
            "  Train Loss: 1.5669 | Train Acc: 0.4336\n",
            "  Val   Loss: 1.5158 | Val   Acc: 0.4619\n",
            "  Best Val Acc: 0.4619\n",
            "  Loss Gap:   0.0510 | Acc  Gap:  -0.0283\n",
            "\n",
            "Epoch 6/60 (LR: 0.001500)\n",
            "  Train Loss: 1.5405 | Train Acc: 0.4499\n",
            "  Val   Loss: 1.4425 | Val   Acc: 0.5026\n",
            "  Best Val Acc: 0.5026\n",
            "  Loss Gap:   0.0980 | Acc  Gap:  -0.0527\n",
            "\n",
            "Epoch 7/60 (LR: 0.001500)\n",
            "  Train Loss: 1.5042 | Train Acc: 0.4703\n",
            "  Val   Loss: 1.4291 | Val   Acc: 0.5099\n",
            "  Best Val Acc: 0.5099\n",
            "  Loss Gap:   0.0751 | Acc  Gap:  -0.0396\n",
            "\n",
            "Epoch 8/60 (LR: 0.001500)\n",
            "  Train Loss: 1.4868 | Train Acc: 0.4796\n",
            "  Val   Loss: 1.4175 | Val   Acc: 0.5226\n",
            "  Best Val Acc: 0.5226\n",
            "  Loss Gap:   0.0692 | Acc  Gap:  -0.0430\n",
            "\n",
            "Epoch 9/60 (LR: 0.001500)\n",
            "  Train Loss: 1.4644 | Train Acc: 0.4946\n",
            "  Val   Loss: 1.4216 | Val   Acc: 0.5087\n",
            "  Best Val Acc: 0.5226\n",
            "  Loss Gap:   0.0429 | Acc  Gap:  -0.0141\n",
            "\n",
            "Epoch 10/60 (LR: 0.001500)\n",
            "  Train Loss: 1.4549 | Train Acc: 0.4985\n",
            "  Val   Loss: 1.3873 | Val   Acc: 0.5401\n",
            "  Best Val Acc: 0.5401\n",
            "  Loss Gap:   0.0676 | Acc  Gap:  -0.0416\n",
            "\n",
            "Epoch 11/60 (LR: 0.001500)\n",
            "  Train Loss: 1.4395 | Train Acc: 0.5103\n",
            "  Val   Loss: 1.3547 | Val   Acc: 0.5428\n",
            "  Best Val Acc: 0.5428\n",
            "  Loss Gap:   0.0848 | Acc  Gap:  -0.0325\n",
            "\n",
            "Epoch 12/60 (LR: 0.001500)\n",
            "  Train Loss: 1.4250 | Train Acc: 0.5126\n",
            "  Val   Loss: 1.3565 | Val   Acc: 0.5481\n",
            "  Best Val Acc: 0.5481\n",
            "  Loss Gap:   0.0685 | Acc  Gap:  -0.0355\n",
            "\n",
            "Epoch 13/60 (LR: 0.001500)\n",
            "  Train Loss: 1.4123 | Train Acc: 0.5229\n",
            "  Val   Loss: 1.3463 | Val   Acc: 0.5458\n",
            "  Best Val Acc: 0.5481\n",
            "  Loss Gap:   0.0660 | Acc  Gap:  -0.0229\n",
            "\n",
            "Epoch 14/60 (LR: 0.001500)\n",
            "  Train Loss: 1.4037 | Train Acc: 0.5245\n",
            "  Val   Loss: 1.3748 | Val   Acc: 0.5329\n",
            "  Best Val Acc: 0.5481\n",
            "  Loss Gap:   0.0290 | Acc  Gap:  -0.0084\n",
            "\n",
            "Epoch 15/60 (LR: 0.001500)\n",
            "  Train Loss: 1.3909 | Train Acc: 0.5295\n",
            "  Val   Loss: 1.3520 | Val   Acc: 0.5583\n",
            "  Best Val Acc: 0.5583\n",
            "  Loss Gap:   0.0389 | Acc  Gap:  -0.0288\n",
            "\n",
            "Epoch 16/60 (LR: 0.001500)\n",
            "  Train Loss: 1.3815 | Train Acc: 0.5359\n",
            "  Val   Loss: 1.3218 | Val   Acc: 0.5641\n",
            "  Best Val Acc: 0.5641\n",
            "  Loss Gap:   0.0596 | Acc  Gap:  -0.0281\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-a45d7e53efb5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model_resnet18 = train_model_with_advanced_techniques(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'resnet18'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0015\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-d9c8e8a859a7>\u001b[0m in \u001b[0;36mtrain_model_with_advanced_techniques\u001b[0;34m(model_type, epochs, lr, dropout_rate, batch_size, weight_decay, save_path, x_train, y_train, x_val, y_val)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-f94ebab69790>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-f94ebab69790>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ResNet34 (deeper, might perform better)\n",
        "# model_resnet34 = train_model_with_advanced_techniques(\n",
        "#     model_type='resnet34',\n",
        "#     epochs=60,\n",
        "#     lr=0.001,  # Slightly lower LR for deeper model\n",
        "#     dropout_rate=0.4,  # Less dropout for deeper model\n",
        "#     save_path='fer_resnet34_model.pth'\n",
        "# )"
      ],
      "metadata": {
        "id": "vKqFEA9GZmQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train ResNet50 (deeper, might perform better)\n",
        "# model_resnet50 = train_model_with_advanced_techniques(\n",
        "#     model_type='resnet50',\n",
        "#     epochs=60,\n",
        "#     lr=0.0007,  # Slightly lower LR for deeper model\n",
        "#     dropout_rate=0.25,  # Less dropout for deeper model\n",
        "#     save_path='fer_resnet50_model.pth'\n",
        "# )"
      ],
      "metadata": {
        "id": "BsuoZKcGZpLF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}