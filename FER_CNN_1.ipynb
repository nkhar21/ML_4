{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKW9k/xXChGcTpupKi58Or",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkhar21/ML_4/blob/main/FER_CNN_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.1 Imports"
      ],
      "metadata": {
        "id": "jAaXkgrK6tJT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "h0UE78366AFN",
        "outputId": "26107954-8f22-4f07-f43e-eba541d85237"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-102a6d26-7ee3-46e6-9270-31f744dfffd9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-102a6d26-7ee3-46e6-9270-31f744dfffd9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"nkhar21\",\"key\":\"5be6dca82ee543f5a1e380f0fccfe96d\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle/\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "KtHF0xEa6EAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I6PcuBc6UG9",
        "outputId": "febc327f-72a0-4b6c-f76a-d074de8c3528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading challenges-in-representation-learning-facial-expression-recognition-challenge.zip to /content\n",
            " 83% 238M/285M [00:00<00:00, 483MB/s]\n",
            "100% 285M/285M [00:00<00:00, 510MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip challenges-in-representation-learning-facial-expression-recognition-challenge.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofW0y3DT6VeL",
        "outputId": "3da92b09-fa19-474d-b043-b8c4c05ab6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
            "  inflating: example_submission.csv  \n",
            "  inflating: fer2013.tar.gz          \n",
            "  inflating: icml_face_data.csv      \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "example_submission_df = pd.read_csv('example_submission.csv')\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "face_data_df = pd.read_csv('icml_face_data.csv')"
      ],
      "metadata": {
        "id": "jwtDtCKz6Zy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.2 Preprocess data"
      ],
      "metadata": {
        "id": "GRpBJRtI65nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert pixel strings to numpy arrays\n",
        "def process_pixels(pixels_str):\n",
        "    return np.array(pixels_str.split(), dtype='float32')\n",
        "\n",
        "# For training\n",
        "train_df['pixels'] = train_df['pixels'].apply(process_pixels)\n",
        "x_train = np.stack(train_df['pixels'].values)\n",
        "y_train = train_df['emotion'].values\n",
        "\n",
        "# For testing\n",
        "test_df['pixels'] = test_df['pixels'].apply(process_pixels)\n",
        "x_test = np.stack(test_df['pixels'].values)\n",
        "\n",
        "# Normalize and reshape\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "x_train = x_train.reshape(-1, 48, 48, 1)\n",
        "x_test = x_test.reshape(-1, 48, 48, 1)\n",
        "\n",
        "x_test.shape, x_train.shape, y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz2qyD7n6ih9",
        "outputId": "6607802e-9ffd-496a-95a5-4bfc9e420805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((7178, 48, 48, 1), (28709, 48, 48, 1), (28709,))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into train and validation sets (e.g. 80% train, 20% val)\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    x_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "x_train.shape, x_test.shape, y_train.shape, x_val.shape, y_val.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSwpremw_ZSm",
        "outputId": "3606e3c1-ccfe-464b-f175-ca76f298dd86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((22967, 48, 48, 1), (7178, 48, 48, 1), (22967,), (5742, 48, 48, 1), (5742,))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.3 wandb"
      ],
      "metadata": {
        "id": "zbMSY0cs7jne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "idx = 10\n",
        "\n",
        "\n",
        "emotion_dict = {\n",
        "    0: \"Angry\",\n",
        "    1: \"Disgust\",\n",
        "    2: \"Fear\",\n",
        "    3: \"Happy\",\n",
        "    4: \"Sad\",\n",
        "    5: \"Surprise\",\n",
        "    6: \"Neutral\"\n",
        "}\n",
        "\n",
        "plt.imshow(x_train[idx].squeeze(), cmap='gray')\n",
        "plt.title(f'Emotion: {emotion_dict[y_train[idx]]}')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "J-jIl1q57E18",
        "outputId": "5725185d-db3c-4785-c6de-bcc06ae77566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOKSYzbWE4bt",
        "outputId": "8eb02d1a-7612-4ff1-8b99-3fd75b1cede2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.3 Wandb"
      ],
      "metadata": {
        "id": "VCaOA4bR7m8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WANDB_API_KEY  = 'f8a227b42dc881e037b25911fa86b8a491fc0581'\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "_so3QlG-7oaq",
        "outputId": "2f4ebd9d-5b88-4ea5-d3f2-4434be9c5078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.11)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.29.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnkhar21\u001b[0m (\u001b[33mnkhar21-student\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.4 Helper functions"
      ],
      "metadata": {
        "id": "H1HKo7O5-z5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "\n",
        "def evaluate_model(\n",
        "    x_data,         # NumPy array or torch.Tensor of shape (N, 48, 48, 1)\n",
        "    y_data,         # NumPy array or torch.Tensor of shape (N,)\n",
        "    model,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    train_mode=False,\n",
        "    optimizer=None,\n",
        "    batch_size=64\n",
        "):\n",
        "    \"\"\"\n",
        "    Assumes:\n",
        "      - x_data always has shape (N, 48, 48, 1)\n",
        "      - y_data always has shape (N,)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Convert x_data → FloatTensor on CPU, then permute to (N, 1, 48, 48)\n",
        "    if isinstance(x_data, torch.Tensor):\n",
        "        x_tensor = x_data.float()\n",
        "    else:\n",
        "        x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
        "\n",
        "    # x_tensor is (N, 48, 48, 1) → permute to (N, 1, 48, 48)\n",
        "    x_tensor = x_tensor.permute(0, 3, 1, 2)\n",
        "\n",
        "    # 2. Convert y_data → LongTensor of shape (N,)\n",
        "    if isinstance(y_data, torch.Tensor):\n",
        "        y_tensor = y_data.long()\n",
        "    else:\n",
        "        y_tensor = torch.tensor(y_data, dtype=torch.long)\n",
        "\n",
        "    # 3. Build DataLoader\n",
        "    dataset = TensorDataset(x_tensor, y_tensor)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=train_mode)\n",
        "\n",
        "    # 4. Set model mode\n",
        "    model.to(device)\n",
        "    if train_mode:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # 5. Loop over batches\n",
        "    for x_batch, y_batch in dataloader:\n",
        "        x_batch = x_batch.to(device)    # shape: (B, 1, 48, 48)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        if train_mode:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(x_batch)        # (B, 7)\n",
        "        loss = loss_fn(outputs, y_batch)\n",
        "        total_loss += loss.item() * x_batch.size(0)\n",
        "\n",
        "        if train_mode:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        preds = outputs.argmax(dim=1)   # (B,)\n",
        "        total += y_batch.size(0)\n",
        "        correct += (preds == y_batch).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "tmJNJM4A-zQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 1 - Simple Conv2D"
      ],
      "metadata": {
        "id": "ZT4j_QBU8eMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "\n",
        "# === 0. MODEL DEFINITION ===\n",
        "class SimpleFERCNN(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(SimpleFERCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(32 * 48 * 48, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))      # shape → (B, 8, 48, 48)\n",
        "        x = torch.relu(self.conv2(x))      # shape → (B, 16, 48, 48)\n",
        "        x = torch.relu(self.conv3(x))      # shape → (B, 32, 48, 48)\n",
        "        x = self.flatten(x)                # shape → (B, 32*48*48)\n",
        "        x = torch.relu(self.fc1(x))        # shape → (B, 256)\n",
        "        x = torch.relu(self.fc2(x))        # shape → (B, 128)\n",
        "        x = self.fc3(x)                    # shape → (B, num_classes)\n",
        "        return x\n",
        "\n",
        "# === 1. WANDB INIT ===\n",
        "wandb.init(project=\"ML_4\", entity=\"nkhar21-student\", name=\"CNN_1.0_Basic\")\n",
        "\n",
        "# === 2. CONFIG ===\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "learning_rate = 0.01\n",
        "\n",
        "# === 4. MODEL + OPTIMIZER ===\n",
        "model = SimpleFERCNN(num_classes=7).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "wandb.watch(model)\n",
        "\n",
        "# === 5. TRAINING LOOP ===\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = evaluate_model(\n",
        "        x_train, y_train, model, loss_fn, device,\n",
        "        train_mode=True, optimizer=optimizer, batch_size=batch_size\n",
        "    )\n",
        "    val_loss, val_acc = evaluate_model(\n",
        "        x_val, y_val, model, loss_fn, device,\n",
        "        train_mode=False, optimizer=None, batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Overfitting metrics\n",
        "    loss_gap = train_loss - val_loss\n",
        "    acc_gap = train_acc - val_acc\n",
        "    loss_ratio = train_loss / val_loss if val_loss != 0 else float(\"inf\")\n",
        "    acc_ratio = train_acc / val_acc if val_acc != 0 else float(\"inf\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "    print(f\"  Loss Gap:   {loss_gap:.4f} | Acc  Gap:  {acc_gap:.4f}\")\n",
        "    print(f\"  Loss Ratio: {loss_ratio:.4f} | Acc Ratio: {acc_ratio:.4f}\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_accuracy\": train_acc,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_accuracy\": val_acc,\n",
        "        \"loss_gap\": loss_gap,\n",
        "        \"acc_gap\": acc_gap,\n",
        "        \"loss_ratio\": loss_ratio,\n",
        "        \"acc_ratio\": acc_ratio,\n",
        "    })\n",
        "\n",
        "# === 6. SAVE MODEL ===\n",
        "torch.save(model.state_dict(), \"cnn_eval_model.pth\")\n",
        "wandb.save(\"cnn_eval_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5SIDhvuy7o_r",
        "outputId": "3d10f00d-68eb-4ed0-915c-4958dd2f0f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁████████████</td></tr><tr><td>acc_ratio</td><td>▁████████████</td></tr><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>loss_gap</td><td>█▁▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>loss_ratio</td><td>█▁▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁████████████</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃█▇▃▄▄▃▄▄▁▂▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>1e-05</td></tr><tr><td>acc_ratio</td><td>1.00004</td></tr><tr><td>epoch</td><td>13</td></tr><tr><td>loss_gap</td><td>0.00043</td></tr><tr><td>loss_ratio</td><td>1.00023</td></tr><tr><td>train_accuracy</td><td>0.25132</td></tr><tr><td>train_loss</td><td>1.81063</td></tr><tr><td>val_accuracy</td><td>0.25131</td></tr><tr><td>val_loss</td><td>1.8102</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">CNN_1.0_Basic</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/yqjuoing' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/yqjuoing</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_182852-yqjuoing/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_183117-wphbih92</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/wphbih92' target=\"_blank\">CNN_1.0_Basic</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/wphbih92' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/wphbih92</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "  Train Loss: 1.9891 | Train Acc: 0.2485\n",
            "  Val   Loss: 1.8123 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.1767 | Acc  Gap:  -0.0028\n",
            "  Loss Ratio: 1.0975 | Acc Ratio: 0.9890\n",
            "Epoch 2/20\n",
            "  Train Loss: 1.8121 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8119 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0001 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0001 | Acc Ratio: 1.0000\n",
            "Epoch 3/20\n",
            "  Train Loss: 1.8118 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8103 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0014 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0008 | Acc Ratio: 1.0000\n",
            "Epoch 4/20\n",
            "  Train Loss: 1.8114 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8111 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0002 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0001 | Acc Ratio: 1.0000\n",
            "Epoch 5/20\n",
            "  Train Loss: 1.8113 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8103 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0009 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0005 | Acc Ratio: 1.0000\n",
            "Epoch 6/20\n",
            "  Train Loss: 1.8108 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8111 | Val   Acc: 0.2513\n",
            "  Loss Gap:   -0.0003 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 0.9998 | Acc Ratio: 1.0000\n",
            "Epoch 7/20\n",
            "  Train Loss: 1.8110 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8099 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0011 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0006 | Acc Ratio: 1.0000\n",
            "Epoch 8/20\n",
            "  Train Loss: 1.8108 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8117 | Val   Acc: 0.2513\n",
            "  Loss Gap:   -0.0009 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 0.9995 | Acc Ratio: 1.0000\n",
            "Epoch 9/20\n",
            "  Train Loss: 1.8111 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8097 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0014 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0008 | Acc Ratio: 1.0000\n",
            "Epoch 10/20\n",
            "  Train Loss: 1.8108 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8097 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0011 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0006 | Acc Ratio: 1.0000\n",
            "Epoch 11/20\n",
            "  Train Loss: 1.8108 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8104 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0005 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0003 | Acc Ratio: 1.0000\n",
            "Epoch 12/20\n",
            "  Train Loss: 1.8110 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8101 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0009 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0005 | Acc Ratio: 1.0000\n",
            "Epoch 13/20\n",
            "  Train Loss: 1.8107 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8105 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0001 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0001 | Acc Ratio: 1.0000\n",
            "Epoch 14/20\n",
            "  Train Loss: 1.8109 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8097 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0012 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0006 | Acc Ratio: 1.0000\n",
            "Epoch 15/20\n",
            "  Train Loss: 1.8103 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8108 | Val   Acc: 0.2513\n",
            "  Loss Gap:   -0.0004 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 0.9998 | Acc Ratio: 1.0000\n",
            "Epoch 16/20\n",
            "  Train Loss: 1.8106 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8102 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0004 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0002 | Acc Ratio: 1.0000\n",
            "Epoch 17/20\n",
            "  Train Loss: 1.8104 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8103 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0001 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0000 | Acc Ratio: 1.0000\n",
            "Epoch 18/20\n",
            "  Train Loss: 1.8106 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8099 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0007 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0004 | Acc Ratio: 1.0000\n",
            "Epoch 19/20\n",
            "  Train Loss: 1.8106 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8100 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0006 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0003 | Acc Ratio: 1.0000\n",
            "Epoch 20/20\n",
            "  Train Loss: 1.8105 | Train Acc: 0.2513\n",
            "  Val   Loss: 1.8097 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0008 | Acc  Gap:  0.0000\n",
            "  Loss Ratio: 1.0004 | Acc Ratio: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/wandb/run-20250603_183117-wphbih92/files/cnn_eval_model.pth']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 2 - same with different lr"
      ],
      "metadata": {
        "id": "x-fbFACtFrDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "\n",
        "# === 0. MODEL DEFINITION ===\n",
        "class SimpleFERCNN(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(SimpleFERCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(32 * 48 * 48, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))      # shape → (B, 8, 48, 48)\n",
        "        x = torch.relu(self.conv2(x))      # shape → (B, 16, 48, 48)\n",
        "        x = torch.relu(self.conv3(x))      # shape → (B, 32, 48, 48)\n",
        "        x = self.flatten(x)                # shape → (B, 32*48*48)\n",
        "        x = torch.relu(self.fc1(x))        # shape → (B, 256)\n",
        "        x = torch.relu(self.fc2(x))        # shape → (B, 128)\n",
        "        x = self.fc3(x)                    # shape → (B, num_classes)\n",
        "        return x\n",
        "\n",
        "# === 1. WANDB INIT ===\n",
        "wandb.init(project=\"ML_4\", entity=\"nkhar21-student\", name=\"CNN_2.0_reduce_rl\")\n",
        "\n",
        "# === 2. CONFIG ===\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "# === 4. MODEL + OPTIMIZER ===\n",
        "model = SimpleFERCNN(num_classes=7).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "wandb.watch(model)\n",
        "\n",
        "# === 5. TRAINING LOOP ===\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = evaluate_model(\n",
        "        x_train, y_train, model, loss_fn, device,\n",
        "        train_mode=True, optimizer=optimizer, batch_size=batch_size\n",
        "    )\n",
        "    val_loss, val_acc = evaluate_model(\n",
        "        x_val, y_val, model, loss_fn, device,\n",
        "        train_mode=False, optimizer=None, batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Overfitting metrics\n",
        "    loss_gap = train_loss - val_loss\n",
        "    acc_gap = train_acc - val_acc\n",
        "    loss_ratio = train_loss / val_loss if val_loss != 0 else float(\"inf\")\n",
        "    acc_ratio = train_acc / val_acc if val_acc != 0 else float(\"inf\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "    print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "    print(f\"  Loss Gap:   {loss_gap:.4f} | Acc  Gap:  {acc_gap:.4f}\")\n",
        "    print(f\"  Loss Ratio: {loss_ratio:.4f} | Acc Ratio: {acc_ratio:.4f}\")\n",
        "\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_accuracy\": train_acc,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"val_accuracy\": val_acc,\n",
        "        \"loss_gap\": loss_gap,\n",
        "        \"acc_gap\": acc_gap,\n",
        "        \"loss_ratio\": loss_ratio,\n",
        "        \"acc_ratio\": acc_ratio,\n",
        "    })\n",
        "\n",
        "# === 6. SAVE MODEL ===\n",
        "torch.save(model.state_dict(), \"cnn_eval_model.pth\")\n",
        "wandb.save(\"cnn_eval_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aVL1QuPyFqUl",
        "outputId": "fc5487ab-bd22-4a13-fba4-78e1fadddd18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▄█</td></tr><tr><td>acc_ratio</td><td>▁▅█</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>loss_gap</td><td>█▅▁</td></tr><tr><td>loss_ratio</td><td>█▅▁</td></tr><tr><td>train_accuracy</td><td>▁▅█</td></tr><tr><td>train_loss</td><td>█▄▁</td></tr><tr><td>val_accuracy</td><td>▁▅█</td></tr><tr><td>val_loss</td><td>█▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.04374</td></tr><tr><td>acc_ratio</td><td>1.09374</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>loss_gap</td><td>-0.09298</td></tr><tr><td>loss_ratio</td><td>0.93218</td></tr><tr><td>train_accuracy</td><td>0.5103</td></tr><tr><td>train_loss</td><td>1.27802</td></tr><tr><td>val_accuracy</td><td>0.46656</td></tr><tr><td>val_loss</td><td>1.371</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">CNN_1.0_Basic</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/yckqx3jh' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/yckqx3jh</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_183511-yckqx3jh/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_183939-8mtu46cp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/8mtu46cp' target=\"_blank\">CNN_2.0_reduce_rl</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/8mtu46cp' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/8mtu46cp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "  Train Loss: 1.6664 | Train Acc: 0.3362\n",
            "  Val   Loss: 1.5453 | Val   Acc: 0.3922\n",
            "  Loss Gap:   0.1212 | Acc  Gap:  -0.0560\n",
            "  Loss Ratio: 1.0784 | Acc Ratio: 0.8572\n",
            "Epoch 2/20\n",
            "  Train Loss: 1.4409 | Train Acc: 0.4427\n",
            "  Val   Loss: 1.4029 | Val   Acc: 0.4516\n",
            "  Loss Gap:   0.0380 | Acc  Gap:  -0.0089\n",
            "  Loss Ratio: 1.0271 | Acc Ratio: 0.9803\n",
            "Epoch 3/20\n",
            "  Train Loss: 1.2322 | Train Acc: 0.5322\n",
            "  Val   Loss: 1.4118 | Val   Acc: 0.4586\n",
            "  Loss Gap:   -0.1796 | Acc  Gap:  0.0736\n",
            "  Loss Ratio: 0.8728 | Acc Ratio: 1.1605\n",
            "Epoch 4/20\n",
            "  Train Loss: 0.9509 | Train Acc: 0.6520\n",
            "  Val   Loss: 1.3900 | Val   Acc: 0.4892\n",
            "  Loss Gap:   -0.4391 | Acc  Gap:  0.1628\n",
            "  Loss Ratio: 0.6841 | Acc Ratio: 1.3328\n",
            "Epoch 5/20\n",
            "  Train Loss: 0.5482 | Train Acc: 0.8044\n",
            "  Val   Loss: 1.8086 | Val   Acc: 0.4786\n",
            "  Loss Gap:   -1.2604 | Acc  Gap:  0.3258\n",
            "  Loss Ratio: 0.3031 | Acc Ratio: 1.6808\n",
            "Epoch 6/20\n",
            "  Train Loss: 0.2184 | Train Acc: 0.9306\n",
            "  Val   Loss: 2.4753 | Val   Acc: 0.4835\n",
            "  Loss Gap:   -2.2569 | Acc  Gap:  0.4471\n",
            "  Loss Ratio: 0.0882 | Acc Ratio: 1.9248\n",
            "Epoch 7/20\n",
            "  Train Loss: 0.0847 | Train Acc: 0.9778\n",
            "  Val   Loss: 3.1212 | Val   Acc: 0.4852\n",
            "  Loss Gap:   -3.0365 | Acc  Gap:  0.4926\n",
            "  Loss Ratio: 0.0271 | Acc Ratio: 2.0153\n",
            "Epoch 8/20\n",
            "  Train Loss: 0.0587 | Train Acc: 0.9865\n",
            "  Val   Loss: 3.3411 | Val   Acc: 0.4542\n",
            "  Loss Gap:   -3.2824 | Acc  Gap:  0.5323\n",
            "  Loss Ratio: 0.0176 | Acc Ratio: 2.1720\n",
            "Epoch 9/20\n",
            "  Train Loss: 0.0492 | Train Acc: 0.9877\n",
            "  Val   Loss: 3.7018 | Val   Acc: 0.4693\n",
            "  Loss Gap:   -3.6526 | Acc  Gap:  0.5183\n",
            "  Loss Ratio: 0.0133 | Acc Ratio: 2.1044\n",
            "Epoch 10/20\n",
            "  Train Loss: 0.0386 | Train Acc: 0.9916\n",
            "  Val   Loss: 3.6917 | Val   Acc: 0.4831\n",
            "  Loss Gap:   -3.6531 | Acc  Gap:  0.5085\n",
            "  Loss Ratio: 0.0105 | Acc Ratio: 2.0525\n",
            "Epoch 11/20\n",
            "  Train Loss: 0.0301 | Train Acc: 0.9929\n",
            "  Val   Loss: 4.2861 | Val   Acc: 0.4741\n",
            "  Loss Gap:   -4.2560 | Acc  Gap:  0.5188\n",
            "  Loss Ratio: 0.0070 | Acc Ratio: 2.0944\n",
            "Epoch 12/20\n",
            "  Train Loss: 0.0368 | Train Acc: 0.9903\n",
            "  Val   Loss: 4.0239 | Val   Acc: 0.4657\n",
            "  Loss Gap:   -3.9870 | Acc  Gap:  0.5246\n",
            "  Loss Ratio: 0.0092 | Acc Ratio: 2.1265\n",
            "Epoch 13/20\n",
            "  Train Loss: 0.0355 | Train Acc: 0.9897\n",
            "  Val   Loss: 4.3993 | Val   Acc: 0.4730\n",
            "  Loss Gap:   -4.3637 | Acc  Gap:  0.5167\n",
            "  Loss Ratio: 0.0081 | Acc Ratio: 2.0924\n",
            "Epoch 14/20\n",
            "  Train Loss: 0.0338 | Train Acc: 0.9894\n",
            "  Val   Loss: 4.2795 | Val   Acc: 0.4659\n",
            "  Loss Gap:   -4.2457 | Acc  Gap:  0.5235\n",
            "  Loss Ratio: 0.0079 | Acc Ratio: 2.1237\n",
            "Epoch 15/20\n",
            "  Train Loss: 0.0347 | Train Acc: 0.9887\n",
            "  Val   Loss: 4.7151 | Val   Acc: 0.4709\n",
            "  Loss Gap:   -4.6805 | Acc  Gap:  0.5178\n",
            "  Loss Ratio: 0.0074 | Acc Ratio: 2.0995\n",
            "Epoch 16/20\n",
            "  Train Loss: 0.0392 | Train Acc: 0.9875\n",
            "  Val   Loss: 4.6933 | Val   Acc: 0.4713\n",
            "  Loss Gap:   -4.6542 | Acc  Gap:  0.5162\n",
            "  Loss Ratio: 0.0083 | Acc Ratio: 2.0954\n",
            "Epoch 17/20\n",
            "  Train Loss: 0.0287 | Train Acc: 0.9907\n",
            "  Val   Loss: 4.4260 | Val   Acc: 0.4749\n",
            "  Loss Gap:   -4.3973 | Acc  Gap:  0.5158\n",
            "  Loss Ratio: 0.0065 | Acc Ratio: 2.0861\n",
            "Epoch 18/20\n",
            "  Train Loss: 0.0203 | Train Acc: 0.9934\n",
            "  Val   Loss: 4.7325 | Val   Acc: 0.4742\n",
            "  Loss Gap:   -4.7122 | Acc  Gap:  0.5192\n",
            "  Loss Ratio: 0.0043 | Acc Ratio: 2.0948\n",
            "Epoch 19/20\n",
            "  Train Loss: 0.0131 | Train Acc: 0.9963\n",
            "  Val   Loss: 4.8184 | Val   Acc: 0.4633\n",
            "  Loss Gap:   -4.8053 | Acc  Gap:  0.5330\n",
            "  Loss Ratio: 0.0027 | Acc Ratio: 2.1507\n",
            "Epoch 20/20\n",
            "  Train Loss: 0.0090 | Train Acc: 0.9965\n",
            "  Val   Loss: 4.8795 | Val   Acc: 0.4734\n",
            "  Loss Gap:   -4.8705 | Acc  Gap:  0.5231\n",
            "  Loss Ratio: 0.0018 | Acc Ratio: 2.1051\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/wandb/run-20250603_183939-8mtu46cp/files/cnn_eval_model.pth']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 3 - Add dropout to aid overfitt"
      ],
      "metadata": {
        "id": "kAy2epI2Jlhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleFERCNN_With_Dropout(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_rate=0.5):\n",
        "        super(SimpleFERCNN_With_Dropout, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        self.fc1 = nn.Linear(32 * 48 * 48, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.dropout(torch.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# === 1. TRAINING FUNCTION ===\n",
        "epochs = 10\n",
        "def train_model(dropout_rate):\n",
        "    run = wandb.init(\n",
        "        project=\"ML_4\",\n",
        "        entity=\"nkhar21-student\",\n",
        "        name=f\"Dropout_{dropout_rate}\",\n",
        "        config={\"dropout_rate\": dropout_rate, \"epochs\": 10, \"batch_size\": 64, \"lr\": 0.001}\n",
        "    )\n",
        "\n",
        "    config = wandb.config\n",
        "    model = SimpleFERCNN_With_Dropout(num_classes=7, dropout_rate=dropout_rate).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    wandb.watch(model)\n",
        "\n",
        "    best_val_acc = 0\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        train_loss, train_acc = evaluate_model(\n",
        "            x_train, y_train, model, loss_fn, device,\n",
        "            train_mode=True, optimizer=optimizer, batch_size=config.batch_size\n",
        "        )\n",
        "        val_loss, val_acc = evaluate_model(\n",
        "            x_val, y_val, model, loss_fn, device,\n",
        "            train_mode=False, optimizer=None, batch_size=config.batch_size\n",
        "        )\n",
        "\n",
        "        loss_gap = train_loss - val_loss\n",
        "        acc_gap = train_acc - val_acc\n",
        "        loss_ratio = train_loss / val_loss if val_loss != 0 else float(\"inf\")\n",
        "        acc_ratio = train_acc / val_acc if val_acc != 0 else float(\"inf\")\n",
        "\n",
        "        print(f\"[Dropout {dropout_rate}] Epoch {epoch+1}/{config.epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"loss_gap\": loss_gap,\n",
        "            \"acc_gap\": acc_gap,\n",
        "            \"loss_ratio\": loss_ratio,\n",
        "            \"acc_ratio\": acc_ratio,\n",
        "        })\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), f\"best_model_dropout_{dropout_rate}.pth\")\n",
        "            wandb.save(f\"best_model_dropout_{dropout_rate}.pth\")\n",
        "\n",
        "    run.finish()\n",
        "    return best_val_acc\n",
        "\n",
        "\n",
        "# === 2. RUN TRAINING FOR DIFFERENT DROPOUTS ===\n",
        "dropout_rates = [0.0, 0.2, 0.4, 0.5]\n",
        "results = {}\n",
        "\n",
        "for rate in dropout_rates:\n",
        "    acc = train_model(dropout_rate=rate)\n",
        "    results[rate] = acc\n",
        "\n",
        "# === 3. SELECT BEST MODEL ===\n",
        "best_dropout = max(results, key=results.get)\n",
        "print(f\"\\n✅ Best Dropout Rate: {best_dropout} with Val Accuracy: {results[best_dropout]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wXeFiq2E94-b",
        "outputId": "cdc7597a-714d-497c-e0e2-b3b4f1d126cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▂▂▃▄▅▇▇█</td></tr><tr><td>acc_ratio</td><td>▁▂▂▃▄▅▇▇█</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>loss_gap</td><td>███▇▇▅▄▂▁</td></tr><tr><td>loss_ratio</td><td>██▇▇▆▄▃▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▄▅▆▇██</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▄▂▂▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆██▇▇▇▇</td></tr><tr><td>val_loss</td><td>▃▂▁▁▁▂▄▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.4036</td></tr><tr><td>acc_ratio</td><td>1.84951</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>loss_gap</td><td>-1.90812</td></tr><tr><td>loss_ratio</td><td>0.15031</td></tr><tr><td>train_accuracy</td><td>0.8787</td></tr><tr><td>train_loss</td><td>0.33755</td></tr><tr><td>val_accuracy</td><td>0.4751</td></tr><tr><td>val_loss</td><td>2.24567</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Dropout_0.2</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/m8gb6dru' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/m8gb6dru</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_185630-m8gb6dru/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_185744-kwdbkc72</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/kwdbkc72' target=\"_blank\">Dropout_0.0</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/kwdbkc72' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/kwdbkc72</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dropout 0.0] Epoch 1/10\n",
            "  Train Loss: 1.7027 | Train Acc: 0.3185\n",
            "  Val   Loss: 1.6021 | Val   Acc: 0.3687\n",
            "[Dropout 0.0] Epoch 2/10\n",
            "  Train Loss: 1.5067 | Train Acc: 0.4149\n",
            "  Val   Loss: 1.4648 | Val   Acc: 0.4291\n",
            "[Dropout 0.0] Epoch 3/10\n",
            "  Train Loss: 1.3322 | Train Acc: 0.4892\n",
            "  Val   Loss: 1.4004 | Val   Acc: 0.4561\n",
            "[Dropout 0.0] Epoch 4/10\n",
            "  Train Loss: 1.1044 | Train Acc: 0.5868\n",
            "  Val   Loss: 1.4184 | Val   Acc: 0.4709\n",
            "[Dropout 0.0] Epoch 5/10\n",
            "  Train Loss: 0.7737 | Train Acc: 0.7222\n",
            "  Val   Loss: 1.6187 | Val   Acc: 0.4734\n",
            "[Dropout 0.0] Epoch 6/10\n",
            "  Train Loss: 0.4012 | Train Acc: 0.8606\n",
            "  Val   Loss: 2.0371 | Val   Acc: 0.4650\n",
            "[Dropout 0.0] Epoch 7/10\n",
            "  Train Loss: 0.1564 | Train Acc: 0.9527\n",
            "  Val   Loss: 2.7709 | Val   Acc: 0.4552\n",
            "[Dropout 0.0] Epoch 8/10\n",
            "  Train Loss: 0.0733 | Train Acc: 0.9821\n",
            "  Val   Loss: 3.4763 | Val   Acc: 0.4643\n",
            "[Dropout 0.0] Epoch 9/10\n",
            "  Train Loss: 0.0574 | Train Acc: 0.9860\n",
            "  Val   Loss: 3.5768 | Val   Acc: 0.4619\n",
            "[Dropout 0.0] Epoch 10/10\n",
            "  Train Loss: 0.0486 | Train Acc: 0.9884\n",
            "  Val   Loss: 3.7569 | Val   Acc: 0.4535\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▁▂▃▅▆████</td></tr><tr><td>acc_ratio</td><td>▁▂▂▃▅▆████</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss_gap</td><td>███▇▆▅▃▂▁▁</td></tr><tr><td>loss_ratio</td><td>██▇▆▄▂▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▄▅▇████</td></tr><tr><td>train_loss</td><td>█▇▆▅▄▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▇██▇▇▇▇▇</td></tr><tr><td>val_loss</td><td>▂▁▁▁▂▃▅▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.53487</td></tr><tr><td>acc_ratio</td><td>2.17943</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss_gap</td><td>-3.7083</td></tr><tr><td>loss_ratio</td><td>0.01292</td></tr><tr><td>train_accuracy</td><td>0.98837</td></tr><tr><td>train_loss</td><td>0.04855</td></tr><tr><td>val_accuracy</td><td>0.4535</td></tr><tr><td>val_loss</td><td>3.75686</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Dropout_0.0</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/kwdbkc72' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/kwdbkc72</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_185744-kwdbkc72/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_185855-ljgcajes</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/ljgcajes' target=\"_blank\">Dropout_0.2</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/ljgcajes' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/ljgcajes</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dropout 0.2] Epoch 1/10\n",
            "  Train Loss: 1.7071 | Train Acc: 0.3122\n",
            "  Val   Loss: 1.5829 | Val   Acc: 0.3734\n",
            "[Dropout 0.2] Epoch 2/10\n",
            "  Train Loss: 1.5178 | Train Acc: 0.4054\n",
            "  Val   Loss: 1.4791 | Val   Acc: 0.4260\n",
            "[Dropout 0.2] Epoch 3/10\n",
            "  Train Loss: 1.3756 | Train Acc: 0.4694\n",
            "  Val   Loss: 1.4013 | Val   Acc: 0.4594\n",
            "[Dropout 0.2] Epoch 4/10\n",
            "  Train Loss: 1.1934 | Train Acc: 0.5487\n",
            "  Val   Loss: 1.3786 | Val   Acc: 0.4791\n",
            "[Dropout 0.2] Epoch 5/10\n",
            "  Train Loss: 0.9400 | Train Acc: 0.6511\n",
            "  Val   Loss: 1.4457 | Val   Acc: 0.4770\n",
            "[Dropout 0.2] Epoch 6/10\n",
            "  Train Loss: 0.6371 | Train Acc: 0.7727\n",
            "  Val   Loss: 1.7141 | Val   Acc: 0.4702\n",
            "[Dropout 0.2] Epoch 7/10\n",
            "  Train Loss: 0.4016 | Train Acc: 0.8583\n",
            "  Val   Loss: 1.9656 | Val   Acc: 0.4681\n",
            "[Dropout 0.2] Epoch 8/10\n",
            "  Train Loss: 0.2669 | Train Acc: 0.9076\n",
            "  Val   Loss: 2.4377 | Val   Acc: 0.4690\n",
            "[Dropout 0.2] Epoch 9/10\n",
            "  Train Loss: 0.1936 | Train Acc: 0.9355\n",
            "  Val   Loss: 2.6067 | Val   Acc: 0.4664\n",
            "[Dropout 0.2] Epoch 10/10\n",
            "  Train Loss: 0.1541 | Train Acc: 0.9484\n",
            "  Val   Loss: 2.8467 | Val   Acc: 0.4472\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▂▂▃▄▆▇▇██</td></tr><tr><td>acc_ratio</td><td>▁▂▂▃▄▅▆▇▇█</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss_gap</td><td>███▇▆▅▄▂▂▁</td></tr><tr><td>loss_ratio</td><td>██▇▇▅▃▂▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▄▅▆▇███</td></tr><tr><td>train_loss</td><td>█▇▇▆▅▃▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▇██▇▇▇▇▆</td></tr><tr><td>val_loss</td><td>▂▁▁▁▁▃▄▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.50117</td></tr><tr><td>acc_ratio</td><td>2.12061</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss_gap</td><td>-2.69259</td></tr><tr><td>loss_ratio</td><td>0.05414</td></tr><tr><td>train_accuracy</td><td>0.9484</td></tr><tr><td>train_loss</td><td>0.15413</td></tr><tr><td>val_accuracy</td><td>0.44723</td></tr><tr><td>val_loss</td><td>2.84672</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Dropout_0.2</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/ljgcajes' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/ljgcajes</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_185855-ljgcajes/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_185958-ob15f7zk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/ob15f7zk' target=\"_blank\">Dropout_0.4</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/ob15f7zk' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/ob15f7zk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dropout 0.4] Epoch 1/10\n",
            "  Train Loss: 1.7690 | Train Acc: 0.2710\n",
            "  Val   Loss: 1.6741 | Val   Acc: 0.3292\n",
            "[Dropout 0.4] Epoch 2/10\n",
            "  Train Loss: 1.6282 | Train Acc: 0.3581\n",
            "  Val   Loss: 1.5159 | Val   Acc: 0.4068\n",
            "[Dropout 0.4] Epoch 3/10\n",
            "  Train Loss: 1.4792 | Train Acc: 0.4271\n",
            "  Val   Loss: 1.4371 | Val   Acc: 0.4443\n",
            "[Dropout 0.4] Epoch 4/10\n",
            "  Train Loss: 1.3283 | Train Acc: 0.4947\n",
            "  Val   Loss: 1.3842 | Val   Acc: 0.4627\n",
            "[Dropout 0.4] Epoch 5/10\n",
            "  Train Loss: 1.1760 | Train Acc: 0.5532\n",
            "  Val   Loss: 1.3616 | Val   Acc: 0.4852\n",
            "[Dropout 0.4] Epoch 6/10\n",
            "  Train Loss: 1.0044 | Train Acc: 0.6254\n",
            "  Val   Loss: 1.4065 | Val   Acc: 0.4854\n",
            "[Dropout 0.4] Epoch 7/10\n",
            "  Train Loss: 0.8226 | Train Acc: 0.6960\n",
            "  Val   Loss: 1.4667 | Val   Acc: 0.4876\n",
            "[Dropout 0.4] Epoch 8/10\n",
            "  Train Loss: 0.6666 | Train Acc: 0.7522\n",
            "  Val   Loss: 1.6440 | Val   Acc: 0.4902\n",
            "[Dropout 0.4] Epoch 9/10\n",
            "  Train Loss: 0.5497 | Train Acc: 0.7967\n",
            "  Val   Loss: 1.7862 | Val   Acc: 0.4897\n",
            "[Dropout 0.4] Epoch 10/10\n",
            "  Train Loss: 0.4648 | Train Acc: 0.8268\n",
            "  Val   Loss: 1.9687 | Val   Acc: 0.4774\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▁▂▃▃▄▆▆▇█</td></tr><tr><td>acc_ratio</td><td>▁▁▂▃▃▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss_gap</td><td>███▇▇▆▅▃▂▁</td></tr><tr><td>loss_ratio</td><td>███▇▆▅▄▂▂▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▄▅▅▆▇██</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▄▃▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇█████▇</td></tr><tr><td>val_loss</td><td>▅▃▂▁▁▂▂▄▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.34948</td></tr><tr><td>acc_ratio</td><td>1.73211</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss_gap</td><td>-1.50392</td></tr><tr><td>loss_ratio</td><td>0.23608</td></tr><tr><td>train_accuracy</td><td>0.82684</td></tr><tr><td>train_loss</td><td>0.46476</td></tr><tr><td>val_accuracy</td><td>0.47736</td></tr><tr><td>val_loss</td><td>1.96869</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Dropout_0.4</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/ob15f7zk' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/ob15f7zk</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_185958-ob15f7zk/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_190102-1mahjs1x</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/1mahjs1x' target=\"_blank\">Dropout_0.5</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/1mahjs1x' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/1mahjs1x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dropout 0.5] Epoch 1/10\n",
            "  Train Loss: 1.7663 | Train Acc: 0.2737\n",
            "  Val   Loss: 1.6323 | Val   Acc: 0.3502\n",
            "[Dropout 0.5] Epoch 2/10\n",
            "  Train Loss: 1.6262 | Train Acc: 0.3582\n",
            "  Val   Loss: 1.5246 | Val   Acc: 0.4042\n",
            "[Dropout 0.5] Epoch 3/10\n",
            "  Train Loss: 1.5103 | Train Acc: 0.4113\n",
            "  Val   Loss: 1.4264 | Val   Acc: 0.4519\n",
            "[Dropout 0.5] Epoch 4/10\n",
            "  Train Loss: 1.3844 | Train Acc: 0.4614\n",
            "  Val   Loss: 1.3799 | Val   Acc: 0.4749\n",
            "[Dropout 0.5] Epoch 5/10\n",
            "  Train Loss: 1.2464 | Train Acc: 0.5238\n",
            "  Val   Loss: 1.3376 | Val   Acc: 0.4946\n",
            "[Dropout 0.5] Epoch 6/10\n",
            "  Train Loss: 1.1044 | Train Acc: 0.5831\n",
            "  Val   Loss: 1.3790 | Val   Acc: 0.4868\n",
            "[Dropout 0.5] Epoch 7/10\n",
            "  Train Loss: 0.9433 | Train Acc: 0.6483\n",
            "  Val   Loss: 1.4063 | Val   Acc: 0.4944\n",
            "[Dropout 0.5] Epoch 8/10\n",
            "  Train Loss: 0.8068 | Train Acc: 0.7000\n",
            "  Val   Loss: 1.4899 | Val   Acc: 0.4951\n",
            "[Dropout 0.5] Epoch 9/10\n",
            "  Train Loss: 0.6791 | Train Acc: 0.7447\n",
            "  Val   Loss: 1.5375 | Val   Acc: 0.4878\n",
            "[Dropout 0.5] Epoch 10/10\n",
            "  Train Loss: 0.5834 | Train Acc: 0.7833\n",
            "  Val   Loss: 1.6951 | Val   Acc: 0.4934\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▂▂▂▃▄▅▆▇█</td></tr><tr><td>acc_ratio</td><td>▁▂▂▃▃▅▆▆▇█</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss_gap</td><td>███▇▇▆▅▃▂▁</td></tr><tr><td>loss_ratio</td><td>███▇▇▅▄▃▂▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▄▄▅▆▇▇█</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▄▃▂▂▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▇██████</td></tr><tr><td>val_loss</td><td>▇▅▃▂▁▂▂▄▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.28987</td></tr><tr><td>acc_ratio</td><td>1.58752</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss_gap</td><td>-1.11175</td></tr><tr><td>loss_ratio</td><td>0.34415</td></tr><tr><td>train_accuracy</td><td>0.78325</td></tr><tr><td>train_loss</td><td>0.58337</td></tr><tr><td>val_accuracy</td><td>0.49338</td></tr><tr><td>val_loss</td><td>1.69512</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Dropout_0.5</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/1mahjs1x' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/1mahjs1x</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_190102-1mahjs1x/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Best Dropout Rate: 0.5 with Val Accuracy: 0.4951\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 4 - Add maxpooling"
      ],
      "metadata": {
        "id": "MEe28T85OME3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleFERCNN_With_Dropout_And_MaxPool(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_rate=0.5, pool_kernel=(2, 2)):\n",
        "        super(SimpleFERCNN_With_Dropout_And_MaxPool, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=pool_kernel)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Dynamically calculate the flattened size\n",
        "        dummy_input = torch.zeros(1, 1, 48, 48)\n",
        "        with torch.no_grad():\n",
        "            x = self.pool(torch.relu(self.conv1(dummy_input)))\n",
        "            x = self.pool(torch.relu(self.conv2(x)))\n",
        "            x = self.pool(torch.relu(self.conv3(x)))\n",
        "            flattened_size = x.view(1, -1).shape[1]\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.dropout(torch.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "h2qISvT0OPp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_maxpool(dropout_rate, pool_kernel):\n",
        "    run = wandb.init(\n",
        "        project=\"ML_4\",\n",
        "        entity=\"nkhar21-student\",\n",
        "        name=f\"Dropout_{dropout_rate}_Pool_{pool_kernel}\",\n",
        "        config={\"dropout_rate\": dropout_rate, \"pool_kernel\": pool_kernel, \"epochs\": 10, \"batch_size\": 64, \"lr\": 0.001}\n",
        "    )\n",
        "\n",
        "    config = wandb.config\n",
        "    model = SimpleFERCNN_With_Dropout_And_MaxPool(\n",
        "        num_classes=7,\n",
        "        dropout_rate=dropout_rate,\n",
        "        pool_kernel=pool_kernel\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    wandb.watch(model)\n",
        "\n",
        "    best_val_acc = 0\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        train_loss, train_acc = evaluate_model(\n",
        "            x_train, y_train, model, loss_fn, device,\n",
        "            train_mode=True, optimizer=optimizer, batch_size=config.batch_size\n",
        "        )\n",
        "        val_loss, val_acc = evaluate_model(\n",
        "            x_val, y_val, model, loss_fn, device,\n",
        "            train_mode=False, optimizer=None, batch_size=config.batch_size\n",
        "        )\n",
        "\n",
        "        print(f\"[Dropout {dropout_rate} | Pool {pool_kernel}] Epoch {epoch+1}/{config.epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "        })\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), f\"best_model_dropout_{dropout_rate}_pool_{pool_kernel}.pth\")\n",
        "            wandb.save(f\"best_model_dropout_{dropout_rate}_pool_{pool_kernel}.pth\")\n",
        "\n",
        "    run.finish()\n",
        "    return best_val_acc\n"
      ],
      "metadata": {
        "id": "mtDU13pFOU8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.5\n",
        "pool_kernels = [(2, 2), (3, 3)]\n",
        "results = {}\n",
        "\n",
        "for pool in pool_kernels:\n",
        "    acc = train_model_with_maxpool(dropout_rate=dropout_rate, pool_kernel=pool)\n",
        "    results[pool] = acc\n",
        "\n",
        "best_pool = max(results, key=results.get)\n",
        "print(f\"\\n✅ Best Pooling Kernel: {best_pool} with Val Accuracy: {results[best_pool]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EQmgsCFGOYwn",
        "outputId": "b6b391b0-8b03-4eaa-9515-2adc8ca3edd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Dropout_0.5_Pool_(3, 3)</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/m195qdil' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/m195qdil</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_191500-m195qdil/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_191643-u6zqx9w2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/u6zqx9w2' target=\"_blank\">Dropout_0.5_Pool_(2, 2)</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/u6zqx9w2' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/u6zqx9w2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dropout 0.5 | Pool (2, 2)] Epoch 1/10\n",
            "  Train Loss: 1.8165 | Train Acc: 0.2428\n",
            "  Val   Loss: 1.7790 | Val   Acc: 0.2513\n",
            "[Dropout 0.5 | Pool (2, 2)] Epoch 2/10\n",
            "  Train Loss: 1.7401 | Train Acc: 0.2875\n",
            "  Val   Loss: 1.6438 | Val   Acc: 0.3440\n",
            "[Dropout 0.5 | Pool (2, 2)] Epoch 3/10\n",
            "  Train Loss: 1.6211 | Train Acc: 0.3640\n",
            "  Val   Loss: 1.5813 | Val   Acc: 0.3788\n",
            "[Dropout 0.5 | Pool (2, 2)] Epoch 4/10\n",
            "  Train Loss: 1.5513 | Train Acc: 0.3951\n",
            "  Val   Loss: 1.4875 | Val   Acc: 0.4150\n",
            "[Dropout 0.5 | Pool (2, 2)] Epoch 5/10\n",
            "  Train Loss: 1.4959 | Train Acc: 0.4194\n",
            "  Val   Loss: 1.4425 | Val   Acc: 0.4382\n",
            "[Dropout 0.5 | Pool (2, 2)] Epoch 6/10\n",
            "  Train Loss: 1.4464 | Train Acc: 0.4419\n",
            "  Val   Loss: 1.3929 | Val   Acc: 0.4634\n",
            "[Dropout 0.5 | Pool (2, 2)] Epoch 7/10\n",
            "  Train Loss: 1.4088 | Train Acc: 0.4600\n",
            "  Val   Loss: 1.3904 | Val   Acc: 0.4603\n",
            "[Dropout 0.5 | Pool (2, 2)] Epoch 8/10\n",
            "  Train Loss: 1.3768 | Train Acc: 0.4719\n",
            "  Val   Loss: 1.3603 | Val   Acc: 0.4777\n",
            "[Dropout 0.5 | Pool (2, 2)] Epoch 9/10\n",
            "  Train Loss: 1.3443 | Train Acc: 0.4857\n",
            "  Val   Loss: 1.3297 | Val   Acc: 0.4885\n",
            "[Dropout 0.5 | Pool (2, 2)] Epoch 10/10\n",
            "  Train Loss: 1.3132 | Train Acc: 0.5003\n",
            "  Val   Loss: 1.3236 | Val   Acc: 0.4956\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▂▄▅▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▇▅▄▄▃▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_accuracy</td><td>0.50033</td></tr><tr><td>train_loss</td><td>1.31318</td></tr><tr><td>val_accuracy</td><td>0.49565</td></tr><tr><td>val_loss</td><td>1.3236</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Dropout_0.5_Pool_(2, 2)</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/u6zqx9w2' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/u6zqx9w2</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_191643-u6zqx9w2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_191709-kekjau08</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/kekjau08' target=\"_blank\">Dropout_0.5_Pool_(3, 3)</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/kekjau08' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/kekjau08</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dropout 0.5 | Pool (3, 3)] Epoch 1/10\n",
            "  Train Loss: 1.8163 | Train Acc: 0.2498\n",
            "  Val   Loss: 1.7832 | Val   Acc: 0.2513\n",
            "[Dropout 0.5 | Pool (3, 3)] Epoch 2/10\n",
            "  Train Loss: 1.7723 | Train Acc: 0.2596\n",
            "  Val   Loss: 1.7181 | Val   Acc: 0.2863\n",
            "[Dropout 0.5 | Pool (3, 3)] Epoch 3/10\n",
            "  Train Loss: 1.7072 | Train Acc: 0.2999\n",
            "  Val   Loss: 1.6485 | Val   Acc: 0.3342\n",
            "[Dropout 0.5 | Pool (3, 3)] Epoch 4/10\n",
            "  Train Loss: 1.6415 | Train Acc: 0.3472\n",
            "  Val   Loss: 1.5892 | Val   Acc: 0.3657\n",
            "[Dropout 0.5 | Pool (3, 3)] Epoch 5/10\n",
            "  Train Loss: 1.5943 | Train Acc: 0.3733\n",
            "  Val   Loss: 1.5475 | Val   Acc: 0.3932\n",
            "[Dropout 0.5 | Pool (3, 3)] Epoch 6/10\n",
            "  Train Loss: 1.5688 | Train Acc: 0.3876\n",
            "  Val   Loss: 1.5298 | Val   Acc: 0.3953\n",
            "[Dropout 0.5 | Pool (3, 3)] Epoch 7/10\n",
            "  Train Loss: 1.5464 | Train Acc: 0.4002\n",
            "  Val   Loss: 1.5244 | Val   Acc: 0.4026\n",
            "[Dropout 0.5 | Pool (3, 3)] Epoch 8/10\n",
            "  Train Loss: 1.5339 | Train Acc: 0.4044\n",
            "  Val   Loss: 1.5015 | Val   Acc: 0.4131\n",
            "[Dropout 0.5 | Pool (3, 3)] Epoch 9/10\n",
            "  Train Loss: 1.5169 | Train Acc: 0.4117\n",
            "  Val   Loss: 1.4833 | Val   Acc: 0.4192\n",
            "[Dropout 0.5 | Pool (3, 3)] Epoch 10/10\n",
            "  Train Loss: 1.5054 | Train Acc: 0.4175\n",
            "  Val   Loss: 1.4754 | Val   Acc: 0.4230\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▁▃▅▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▇▆▄▃▂▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▄▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▇▅▄▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_accuracy</td><td>0.41747</td></tr><tr><td>train_loss</td><td>1.50542</td></tr><tr><td>val_accuracy</td><td>0.42302</td></tr><tr><td>val_loss</td><td>1.47544</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Dropout_0.5_Pool_(3, 3)</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/kekjau08' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/kekjau08</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_191709-kekjau08/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Best Pooling Kernel: (2, 2) with Val Accuracy: 0.4956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 5 - increase channels -> larger model, decrease dropout - reduce underfit, more epochs."
      ],
      "metadata": {
        "id": "lwGPLOeKDZdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "\n",
        "class ImprovedFERCNN(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_rate=0.3):\n",
        "        super(ImprovedFERCNN, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "        # Increased number of channels\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Dynamically calculate flattened size\n",
        "        dummy_input = torch.zeros(1, 1, 48, 48)\n",
        "        with torch.no_grad():\n",
        "            x = self.pool(torch.relu(self.conv1(dummy_input)))\n",
        "            x = self.pool(torch.relu(self.conv2(x)))\n",
        "            x = self.pool(torch.relu(self.conv3(x)))\n",
        "            flattened_size = x.view(1, -1).shape[1]\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = self.pool(torch.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.dropout(torch.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "ylp9OYDbDj9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_improved_model():\n",
        "    run = wandb.init(\n",
        "        project=\"ML_4\",\n",
        "        entity=\"nkhar21-student\",\n",
        "        name=\"Model_Dropout_0.4_Pool_2x2_20Epochs\",\n",
        "        config={\n",
        "            \"dropout_rate\": 0.4,\n",
        "            \"pool_kernel\": (2, 2),\n",
        "            \"epochs\": 20,\n",
        "            \"batch_size\": 64,\n",
        "            \"lr\": 0.001\n",
        "        }\n",
        "    )\n",
        "\n",
        "    config = wandb.config\n",
        "    model = ImprovedFERCNN(num_classes=7, dropout_rate=config.dropout_rate).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    wandb.watch(model)\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        train_loss, train_acc = evaluate_model(\n",
        "            x_train, y_train, model, loss_fn, device,\n",
        "            train_mode=True, optimizer=optimizer, batch_size=config.batch_size\n",
        "        )\n",
        "        val_loss, val_acc = evaluate_model(\n",
        "            x_val, y_val, model, loss_fn, device,\n",
        "            train_mode=False, optimizer=None, batch_size=config.batch_size\n",
        "        )\n",
        "\n",
        "        # Overfitting metrics\n",
        "        loss_gap = train_loss - val_loss\n",
        "        acc_gap = train_acc - val_acc\n",
        "        loss_ratio = train_loss / val_loss if val_loss != 0 else float(\"inf\")\n",
        "        acc_ratio = train_acc / val_acc if val_acc != 0 else float(\"inf\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config.epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "        print(f\"  Loss Gap:   {loss_gap:.4f} | Acc  Gap:  {acc_gap:.4f}\")\n",
        "        print(f\"  Loss Ratio: {loss_ratio:.4f} | Acc Ratio: {acc_ratio:.4f}\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"loss_gap\": loss_gap,\n",
        "            \"acc_gap\": acc_gap,\n",
        "            \"loss_ratio\": loss_ratio,\n",
        "            \"acc_ratio\": acc_ratio,\n",
        "        })\n",
        "\n",
        "    run.finish()\n"
      ],
      "metadata": {
        "id": "XO9vac2LF6aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_improved_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OGk_9VPLF_ZU",
        "outputId": "619373e6-6146-4908-90ea-542306f37be9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_231705-4fv7r1wt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/4fv7r1wt' target=\"_blank\">Model_Dropout_0.3_Pool_2x2_20Epochs</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/4fv7r1wt' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/4fv7r1wt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "  Train Loss: 1.7751 | Train Acc: 0.2690\n",
            "  Val   Loss: 1.6324 | Val   Acc: 0.3553\n",
            "  Loss Gap:   0.1426 | Acc  Gap:  -0.0863\n",
            "  Loss Ratio: 1.0874 | Acc Ratio: 0.7570\n",
            "Epoch 2/20\n",
            "  Train Loss: 1.5602 | Train Acc: 0.3911\n",
            "  Val   Loss: 1.4638 | Val   Acc: 0.4298\n",
            "  Loss Gap:   0.0964 | Acc  Gap:  -0.0387\n",
            "  Loss Ratio: 1.0659 | Acc Ratio: 0.9100\n",
            "Epoch 3/20\n",
            "  Train Loss: 1.4246 | Train Acc: 0.4492\n",
            "  Val   Loss: 1.3426 | Val   Acc: 0.4812\n",
            "  Loss Gap:   0.0821 | Acc  Gap:  -0.0320\n",
            "  Loss Ratio: 1.0611 | Acc Ratio: 0.9334\n",
            "Epoch 4/20\n",
            "  Train Loss: 1.3264 | Train Acc: 0.4924\n",
            "  Val   Loss: 1.2845 | Val   Acc: 0.5108\n",
            "  Loss Gap:   0.0419 | Acc  Gap:  -0.0184\n",
            "  Loss Ratio: 1.0327 | Acc Ratio: 0.9639\n",
            "Epoch 5/20\n",
            "  Train Loss: 1.2434 | Train Acc: 0.5262\n",
            "  Val   Loss: 1.2368 | Val   Acc: 0.5341\n",
            "  Loss Gap:   0.0066 | Acc  Gap:  -0.0079\n",
            "  Loss Ratio: 1.0053 | Acc Ratio: 0.9852\n",
            "Epoch 6/20\n",
            "  Train Loss: 1.1787 | Train Acc: 0.5531\n",
            "  Val   Loss: 1.2468 | Val   Acc: 0.5265\n",
            "  Loss Gap:   -0.0682 | Acc  Gap:  0.0267\n",
            "  Loss Ratio: 0.9453 | Acc Ratio: 1.0507\n",
            "Epoch 7/20\n",
            "  Train Loss: 1.1164 | Train Acc: 0.5769\n",
            "  Val   Loss: 1.1983 | Val   Acc: 0.5526\n",
            "  Loss Gap:   -0.0818 | Acc  Gap:  0.0243\n",
            "  Loss Ratio: 0.9317 | Acc Ratio: 1.0440\n",
            "Epoch 8/20\n",
            "  Train Loss: 1.0507 | Train Acc: 0.6021\n",
            "  Val   Loss: 1.2042 | Val   Acc: 0.5488\n",
            "  Loss Gap:   -0.1535 | Acc  Gap:  0.0533\n",
            "  Loss Ratio: 0.8725 | Acc Ratio: 1.0972\n",
            "Epoch 9/20\n",
            "  Train Loss: 0.9794 | Train Acc: 0.6346\n",
            "  Val   Loss: 1.2064 | Val   Acc: 0.5601\n",
            "  Loss Gap:   -0.2270 | Acc  Gap:  0.0745\n",
            "  Loss Ratio: 0.8118 | Acc Ratio: 1.1331\n",
            "Epoch 10/20\n",
            "  Train Loss: 0.9182 | Train Acc: 0.6592\n",
            "  Val   Loss: 1.2382 | Val   Acc: 0.5566\n",
            "  Loss Gap:   -0.3200 | Acc  Gap:  0.1026\n",
            "  Loss Ratio: 0.7415 | Acc Ratio: 1.1843\n",
            "Epoch 11/20\n",
            "  Train Loss: 0.8479 | Train Acc: 0.6829\n",
            "  Val   Loss: 1.2543 | Val   Acc: 0.5629\n",
            "  Loss Gap:   -0.4064 | Acc  Gap:  0.1201\n",
            "  Loss Ratio: 0.6760 | Acc Ratio: 1.2133\n",
            "Epoch 12/20\n",
            "  Train Loss: 0.7897 | Train Acc: 0.7052\n",
            "  Val   Loss: 1.3107 | Val   Acc: 0.5583\n",
            "  Loss Gap:   -0.5209 | Acc  Gap:  0.1468\n",
            "  Loss Ratio: 0.6025 | Acc Ratio: 1.2630\n",
            "Epoch 13/20\n",
            "  Train Loss: 0.7333 | Train Acc: 0.7256\n",
            "  Val   Loss: 1.3482 | Val   Acc: 0.5622\n",
            "  Loss Gap:   -0.6148 | Acc  Gap:  0.1635\n",
            "  Loss Ratio: 0.5439 | Acc Ratio: 1.2908\n",
            "Epoch 14/20\n",
            "  Train Loss: 0.6723 | Train Acc: 0.7477\n",
            "  Val   Loss: 1.4289 | Val   Acc: 0.5667\n",
            "  Loss Gap:   -0.7566 | Acc  Gap:  0.1810\n",
            "  Loss Ratio: 0.4705 | Acc Ratio: 1.3194\n",
            "Epoch 15/20\n",
            "  Train Loss: 0.6324 | Train Acc: 0.7631\n",
            "  Val   Loss: 1.4406 | Val   Acc: 0.5580\n",
            "  Loss Gap:   -0.8082 | Acc  Gap:  0.2051\n",
            "  Loss Ratio: 0.4390 | Acc Ratio: 1.3675\n",
            "Epoch 16/20\n",
            "  Train Loss: 0.5902 | Train Acc: 0.7770\n",
            "  Val   Loss: 1.5343 | Val   Acc: 0.5676\n",
            "  Loss Gap:   -0.9440 | Acc  Gap:  0.2095\n",
            "  Loss Ratio: 0.3847 | Acc Ratio: 1.3690\n",
            "Epoch 17/20\n",
            "  Train Loss: 0.5550 | Train Acc: 0.7925\n",
            "  Val   Loss: 1.5847 | Val   Acc: 0.5646\n",
            "  Loss Gap:   -1.0297 | Acc  Gap:  0.2279\n",
            "  Loss Ratio: 0.3502 | Acc Ratio: 1.4037\n",
            "Epoch 18/20\n",
            "  Train Loss: 0.5147 | Train Acc: 0.8096\n",
            "  Val   Loss: 1.6331 | Val   Acc: 0.5629\n",
            "  Loss Gap:   -1.1184 | Acc  Gap:  0.2468\n",
            "  Loss Ratio: 0.3152 | Acc Ratio: 1.4384\n",
            "Epoch 19/20\n",
            "  Train Loss: 0.4783 | Train Acc: 0.8219\n",
            "  Val   Loss: 1.6851 | Val   Acc: 0.5587\n",
            "  Loss Gap:   -1.2068 | Acc  Gap:  0.2632\n",
            "  Loss Ratio: 0.2838 | Acc Ratio: 1.4711\n",
            "Epoch 20/20\n",
            "  Train Loss: 0.4555 | Train Acc: 0.8281\n",
            "  Val   Loss: 1.7574 | Val   Acc: 0.5583\n",
            "  Loss Gap:   -1.3019 | Acc  Gap:  0.2698\n",
            "  Loss Ratio: 0.2592 | Acc Ratio: 1.4832\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▂▂▂▃▃▃▄▄▅▅▆▆▆▇▇▇███</td></tr><tr><td>acc_ratio</td><td>▁▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>loss_gap</td><td>████▇▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>loss_ratio</td><td>████▇▇▇▆▆▅▅▄▃▃▃▂▂▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▃▄▄▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▅▅▄▄▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▆▇▇█▇████████████</td></tr><tr><td>val_loss</td><td>▆▄▃▂▁▂▁▁▁▂▂▂▃▄▄▅▆▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.2698</td></tr><tr><td>acc_ratio</td><td>1.48322</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>loss_gap</td><td>-1.30192</td></tr><tr><td>loss_ratio</td><td>0.25917</td></tr><tr><td>train_accuracy</td><td>0.82814</td></tr><tr><td>train_loss</td><td>0.45547</td></tr><tr><td>val_accuracy</td><td>0.55834</td></tr><tr><td>val_loss</td><td>1.75739</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Model_Dropout_0.3_Pool_2x2_20Epochs</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/4fv7r1wt' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/4fv7r1wt</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_231705-4fv7r1wt/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_improved_model() # increased dropout rate 0.3 -> 0.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EuraSpbdG8SP",
        "outputId": "30258310-4f3f-43cb-cd57-1c8e7adb0f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_231958-f1m5ht6m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/f1m5ht6m' target=\"_blank\">Model_Dropout_0.4_Pool_2x2_20Epochs</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/f1m5ht6m' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/f1m5ht6m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "  Train Loss: 1.8006 | Train Acc: 0.2505\n",
            "  Val   Loss: 1.6849 | Val   Acc: 0.3222\n",
            "  Loss Gap:   0.1157 | Acc  Gap:  -0.0717\n",
            "  Loss Ratio: 1.0687 | Acc Ratio: 0.7776\n",
            "Epoch 2/20\n",
            "  Train Loss: 1.6238 | Train Acc: 0.3605\n",
            "  Val   Loss: 1.5371 | Val   Acc: 0.4046\n",
            "  Loss Gap:   0.0867 | Acc  Gap:  -0.0440\n",
            "  Loss Ratio: 1.0564 | Acc Ratio: 0.8911\n",
            "Epoch 3/20\n",
            "  Train Loss: 1.5089 | Train Acc: 0.4157\n",
            "  Val   Loss: 1.4607 | Val   Acc: 0.4321\n",
            "  Loss Gap:   0.0482 | Acc  Gap:  -0.0164\n",
            "  Loss Ratio: 1.0330 | Acc Ratio: 0.9622\n",
            "Epoch 4/20\n",
            "  Train Loss: 1.4312 | Train Acc: 0.4507\n",
            "  Val   Loss: 1.3730 | Val   Acc: 0.4742\n",
            "  Loss Gap:   0.0581 | Acc  Gap:  -0.0235\n",
            "  Loss Ratio: 1.0423 | Acc Ratio: 0.9504\n",
            "Epoch 5/20\n",
            "  Train Loss: 1.3639 | Train Acc: 0.4766\n",
            "  Val   Loss: 1.3324 | Val   Acc: 0.4969\n",
            "  Loss Gap:   0.0314 | Acc  Gap:  -0.0203\n",
            "  Loss Ratio: 1.0236 | Acc Ratio: 0.9591\n",
            "Epoch 6/20\n",
            "  Train Loss: 1.2974 | Train Acc: 0.5017\n",
            "  Val   Loss: 1.2808 | Val   Acc: 0.5124\n",
            "  Loss Gap:   0.0166 | Acc  Gap:  -0.0106\n",
            "  Loss Ratio: 1.0130 | Acc Ratio: 0.9792\n",
            "Epoch 7/20\n",
            "  Train Loss: 1.2379 | Train Acc: 0.5282\n",
            "  Val   Loss: 1.2502 | Val   Acc: 0.5287\n",
            "  Loss Gap:   -0.0123 | Acc  Gap:  -0.0005\n",
            "  Loss Ratio: 0.9902 | Acc Ratio: 0.9990\n",
            "Epoch 8/20\n",
            "  Train Loss: 1.1854 | Train Acc: 0.5531\n",
            "  Val   Loss: 1.2266 | Val   Acc: 0.5392\n",
            "  Loss Gap:   -0.0412 | Acc  Gap:  0.0139\n",
            "  Loss Ratio: 0.9664 | Acc Ratio: 1.0257\n",
            "Epoch 9/20\n",
            "  Train Loss: 1.1313 | Train Acc: 0.5719\n",
            "  Val   Loss: 1.2212 | Val   Acc: 0.5402\n",
            "  Loss Gap:   -0.0900 | Acc  Gap:  0.0317\n",
            "  Loss Ratio: 0.9263 | Acc Ratio: 1.0586\n",
            "Epoch 10/20\n",
            "  Train Loss: 1.0774 | Train Acc: 0.5930\n",
            "  Val   Loss: 1.2299 | Val   Acc: 0.5423\n",
            "  Loss Gap:   -0.1525 | Acc  Gap:  0.0507\n",
            "  Loss Ratio: 0.8760 | Acc Ratio: 1.0934\n",
            "Epoch 11/20\n",
            "  Train Loss: 1.0293 | Train Acc: 0.6086\n",
            "  Val   Loss: 1.2368 | Val   Acc: 0.5369\n",
            "  Loss Gap:   -0.2075 | Acc  Gap:  0.0717\n",
            "  Loss Ratio: 0.8322 | Acc Ratio: 1.1335\n",
            "Epoch 12/20\n",
            "  Train Loss: 0.9759 | Train Acc: 0.6346\n",
            "  Val   Loss: 1.2241 | Val   Acc: 0.5430\n",
            "  Loss Gap:   -0.2482 | Acc  Gap:  0.0915\n",
            "  Loss Ratio: 0.7973 | Acc Ratio: 1.1686\n",
            "Epoch 13/20\n",
            "  Train Loss: 0.9259 | Train Acc: 0.6513\n",
            "  Val   Loss: 1.2701 | Val   Acc: 0.5460\n",
            "  Loss Gap:   -0.3442 | Acc  Gap:  0.1053\n",
            "  Loss Ratio: 0.7290 | Acc Ratio: 1.1930\n",
            "Epoch 14/20\n",
            "  Train Loss: 0.8804 | Train Acc: 0.6710\n",
            "  Val   Loss: 1.3059 | Val   Acc: 0.5357\n",
            "  Loss Gap:   -0.4255 | Acc  Gap:  0.1353\n",
            "  Loss Ratio: 0.6742 | Acc Ratio: 1.2526\n",
            "Epoch 15/20\n",
            "  Train Loss: 0.8342 | Train Acc: 0.6903\n",
            "  Val   Loss: 1.3317 | Val   Acc: 0.5552\n",
            "  Loss Gap:   -0.4975 | Acc  Gap:  0.1351\n",
            "  Loss Ratio: 0.6264 | Acc Ratio: 1.2433\n",
            "Epoch 16/20\n",
            "  Train Loss: 0.7798 | Train Acc: 0.7101\n",
            "  Val   Loss: 1.3573 | Val   Acc: 0.5479\n",
            "  Loss Gap:   -0.5774 | Acc  Gap:  0.1623\n",
            "  Loss Ratio: 0.5746 | Acc Ratio: 1.2961\n",
            "Epoch 17/20\n",
            "  Train Loss: 0.7551 | Train Acc: 0.7177\n",
            "  Val   Loss: 1.3761 | Val   Acc: 0.5479\n",
            "  Loss Gap:   -0.6210 | Acc  Gap:  0.1698\n",
            "  Loss Ratio: 0.5487 | Acc Ratio: 1.3099\n",
            "Epoch 18/20\n",
            "  Train Loss: 0.7207 | Train Acc: 0.7296\n",
            "  Val   Loss: 1.4022 | Val   Acc: 0.5460\n",
            "  Loss Gap:   -0.6816 | Acc  Gap:  0.1836\n",
            "  Loss Ratio: 0.5139 | Acc Ratio: 1.3363\n",
            "Epoch 19/20\n",
            "  Train Loss: 0.6755 | Train Acc: 0.7480\n",
            "  Val   Loss: 1.4116 | Val   Acc: 0.5484\n",
            "  Loss Gap:   -0.7361 | Acc  Gap:  0.1996\n",
            "  Loss Ratio: 0.4785 | Acc Ratio: 1.3639\n",
            "Epoch 20/20\n",
            "  Train Loss: 0.6482 | Train Acc: 0.7607\n",
            "  Val   Loss: 1.5172 | Val   Acc: 0.5486\n",
            "  Loss Gap:   -0.8690 | Acc  Gap:  0.2122\n",
            "  Loss Ratio: 0.4272 | Acc Ratio: 1.3867\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▂▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>acc_ratio</td><td>▁▂▃▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>loss_gap</td><td>████▇▇▇▇▇▆▆▅▅▄▄▃▃▂▂▁</td></tr><tr><td>loss_ratio</td><td>█████▇▇▇▆▆▅▅▄▄▃▃▂▂▂▁</td></tr><tr><td>train_accuracy</td><td>▁▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▆▆▇▇███▇██▇██████</td></tr><tr><td>val_loss</td><td>█▆▅▃▃▂▁▁▁▁▁▁▂▂▃▃▃▄▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.21215</td></tr><tr><td>acc_ratio</td><td>1.38673</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>loss_gap</td><td>-0.86902</td></tr><tr><td>loss_ratio</td><td>0.42722</td></tr><tr><td>train_accuracy</td><td>0.76074</td></tr><tr><td>train_loss</td><td>0.64819</td></tr><tr><td>val_accuracy</td><td>0.54859</td></tr><tr><td>val_loss</td><td>1.51721</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Model_Dropout_0.4_Pool_2x2_20Epochs</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/f1m5ht6m' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/f1m5ht6m</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_231958-f1m5ht6m/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 6 - add batchnorm"
      ],
      "metadata": {
        "id": "zETBVVIxH478"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FERCNN_With_BatchNorm(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_rate=0.3):\n",
        "        super(FERCNN_With_BatchNorm, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Calculate flattened size dynamically\n",
        "        dummy_input = torch.zeros(1, 1, 48, 48)\n",
        "        with torch.no_grad():\n",
        "            x = self.pool(torch.relu(self.bn1(self.conv1(dummy_input))))\n",
        "            x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "            x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "            flattened_size = x.view(1, -1).shape[1]\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.dropout(torch.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "q2MysUNBH7rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_batchnorm(epochs=20, lr=0.001, dropout_rate=0.3):\n",
        "    run = wandb.init(\n",
        "        project=\"ML_4\",\n",
        "        entity=\"nkhar21-student\",\n",
        "        name=f\"FER_BatchNorm_Dropout_{dropout_rate}_LR_{lr}_Epochs_{epochs}\",\n",
        "        config={\n",
        "            \"dropout_rate\": dropout_rate,\n",
        "            \"pool_kernel\": (2, 2),\n",
        "            \"epochs\": epochs,\n",
        "            \"batch_size\": 64,\n",
        "            \"lr\": lr\n",
        "        }\n",
        "    )\n",
        "\n",
        "    batch_size = 64\n",
        "    model = FERCNN_With_BatchNorm(num_classes=7, dropout_rate=dropout_rate).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    wandb.watch(model)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = evaluate_model(\n",
        "            x_train, y_train, model, loss_fn, device,\n",
        "            train_mode=True, optimizer=optimizer, batch_size=batch_size\n",
        "        )\n",
        "        val_loss, val_acc = evaluate_model(\n",
        "            x_val, y_val, model, loss_fn, device,\n",
        "            train_mode=False, optimizer=None, batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        # Overfitting metrics\n",
        "        loss_gap = train_loss - val_loss\n",
        "        acc_gap = train_acc - val_acc\n",
        "        loss_ratio = train_loss / val_loss if val_loss != 0 else float(\"inf\")\n",
        "        acc_ratio = train_acc / val_acc if val_acc != 0 else float(\"inf\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "        print(f\"  Loss Gap:   {loss_gap:.4f} | Acc  Gap:  {acc_gap:.4f}\")\n",
        "        print(f\"  Loss Ratio: {loss_ratio:.4f} | Acc Ratio: {acc_ratio:.4f}\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"loss_gap\": loss_gap,\n",
        "            \"acc_gap\": acc_gap,\n",
        "            \"loss_ratio\": loss_ratio,\n",
        "            \"acc_ratio\": acc_ratio,\n",
        "        })\n",
        "\n",
        "    run.finish()\n"
      ],
      "metadata": {
        "id": "F9EWZXDwH8gX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_with_batchnorm(epochs=20, lr=0.001, dropout_rate=0.25)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ideqiPUIICx1",
        "outputId": "f94cb524-332f-4651-99ce-16a4dd441a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_232507-hgb0zdf8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/hgb0zdf8' target=\"_blank\">FER_BatchNorm_Dropout_0.25_LR_0.001_Epochs_20</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/hgb0zdf8' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/hgb0zdf8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "  Train Loss: 1.6376 | Train Acc: 0.3454\n",
            "  Val   Loss: 1.4574 | Val   Acc: 0.4356\n",
            "  Loss Gap:   0.1802 | Acc  Gap:  -0.0902\n",
            "  Loss Ratio: 1.1237 | Acc Ratio: 0.7930\n",
            "Epoch 2/20\n",
            "  Train Loss: 1.4046 | Train Acc: 0.4594\n",
            "  Val   Loss: 1.4933 | Val   Acc: 0.4413\n",
            "  Loss Gap:   -0.0887 | Acc  Gap:  0.0181\n",
            "  Loss Ratio: 0.9406 | Acc Ratio: 1.0410\n",
            "Epoch 3/20\n",
            "  Train Loss: 1.3080 | Train Acc: 0.5010\n",
            "  Val   Loss: 1.2416 | Val   Acc: 0.5219\n",
            "  Loss Gap:   0.0665 | Acc  Gap:  -0.0209\n",
            "  Loss Ratio: 1.0535 | Acc Ratio: 0.9599\n",
            "Epoch 4/20\n",
            "  Train Loss: 1.2385 | Train Acc: 0.5304\n",
            "  Val   Loss: 1.2305 | Val   Acc: 0.5317\n",
            "  Loss Gap:   0.0080 | Acc  Gap:  -0.0013\n",
            "  Loss Ratio: 1.0065 | Acc Ratio: 0.9976\n",
            "Epoch 5/20\n",
            "  Train Loss: 1.1679 | Train Acc: 0.5528\n",
            "  Val   Loss: 1.2798 | Val   Acc: 0.5131\n",
            "  Loss Gap:   -0.1119 | Acc  Gap:  0.0397\n",
            "  Loss Ratio: 0.9126 | Acc Ratio: 1.0774\n",
            "Epoch 6/20\n",
            "  Train Loss: 1.1095 | Train Acc: 0.5816\n",
            "  Val   Loss: 1.1708 | Val   Acc: 0.5505\n",
            "  Loss Gap:   -0.0613 | Acc  Gap:  0.0311\n",
            "  Loss Ratio: 0.9477 | Acc Ratio: 1.0565\n",
            "Epoch 7/20\n",
            "  Train Loss: 1.0551 | Train Acc: 0.5982\n",
            "  Val   Loss: 1.1320 | Val   Acc: 0.5733\n",
            "  Loss Gap:   -0.0769 | Acc  Gap:  0.0249\n",
            "  Loss Ratio: 0.9321 | Acc Ratio: 1.0434\n",
            "Epoch 8/20\n",
            "  Train Loss: 0.9888 | Train Acc: 0.6224\n",
            "  Val   Loss: 1.1651 | Val   Acc: 0.5610\n",
            "  Loss Gap:   -0.1763 | Acc  Gap:  0.0614\n",
            "  Loss Ratio: 0.8487 | Acc Ratio: 1.1095\n",
            "Epoch 9/20\n",
            "  Train Loss: 0.9309 | Train Acc: 0.6438\n",
            "  Val   Loss: 1.1838 | Val   Acc: 0.5752\n",
            "  Loss Gap:   -0.2530 | Acc  Gap:  0.0686\n",
            "  Loss Ratio: 0.7863 | Acc Ratio: 1.1192\n",
            "Epoch 10/20\n",
            "  Train Loss: 0.8721 | Train Acc: 0.6666\n",
            "  Val   Loss: 1.1450 | Val   Acc: 0.5813\n",
            "  Loss Gap:   -0.2730 | Acc  Gap:  0.0852\n",
            "  Loss Ratio: 0.7616 | Acc Ratio: 1.1466\n",
            "Epoch 11/20\n",
            "  Train Loss: 0.8198 | Train Acc: 0.6867\n",
            "  Val   Loss: 1.2769 | Val   Acc: 0.5603\n",
            "  Loss Gap:   -0.4571 | Acc  Gap:  0.1264\n",
            "  Loss Ratio: 0.6420 | Acc Ratio: 1.2257\n",
            "Epoch 12/20\n",
            "  Train Loss: 0.7563 | Train Acc: 0.7128\n",
            "  Val   Loss: 1.1982 | Val   Acc: 0.5947\n",
            "  Loss Gap:   -0.4420 | Acc  Gap:  0.1181\n",
            "  Loss Ratio: 0.6312 | Acc Ratio: 1.1985\n",
            "Epoch 13/20\n",
            "  Train Loss: 0.6925 | Train Acc: 0.7364\n",
            "  Val   Loss: 1.2401 | Val   Acc: 0.5864\n",
            "  Loss Gap:   -0.5475 | Acc  Gap:  0.1500\n",
            "  Loss Ratio: 0.5585 | Acc Ratio: 1.2558\n",
            "Epoch 14/20\n",
            "  Train Loss: 0.6398 | Train Acc: 0.7553\n",
            "  Val   Loss: 1.2775 | Val   Acc: 0.5907\n",
            "  Loss Gap:   -0.6378 | Acc  Gap:  0.1646\n",
            "  Loss Ratio: 0.5008 | Acc Ratio: 1.2787\n",
            "Epoch 15/20\n",
            "  Train Loss: 0.5776 | Train Acc: 0.7813\n",
            "  Val   Loss: 1.3331 | Val   Acc: 0.5967\n",
            "  Loss Gap:   -0.7554 | Acc  Gap:  0.1846\n",
            "  Loss Ratio: 0.4333 | Acc Ratio: 1.3094\n",
            "Epoch 16/20\n",
            "  Train Loss: 0.5485 | Train Acc: 0.7910\n",
            "  Val   Loss: 1.3576 | Val   Acc: 0.5848\n",
            "  Loss Gap:   -0.8090 | Acc  Gap:  0.2062\n",
            "  Loss Ratio: 0.4041 | Acc Ratio: 1.3526\n",
            "Epoch 17/20\n",
            "  Train Loss: 0.5071 | Train Acc: 0.8093\n",
            "  Val   Loss: 1.3843 | Val   Acc: 0.5893\n",
            "  Loss Gap:   -0.8772 | Acc  Gap:  0.2200\n",
            "  Loss Ratio: 0.3663 | Acc Ratio: 1.3733\n",
            "Epoch 18/20\n",
            "  Train Loss: 0.4566 | Train Acc: 0.8254\n",
            "  Val   Loss: 1.5777 | Val   Acc: 0.5829\n",
            "  Loss Gap:   -1.1211 | Acc  Gap:  0.2425\n",
            "  Loss Ratio: 0.2894 | Acc Ratio: 1.4160\n",
            "Epoch 19/20\n",
            "  Train Loss: 0.4349 | Train Acc: 0.8359\n",
            "  Val   Loss: 1.4437 | Val   Acc: 0.5890\n",
            "  Loss Gap:   -1.0088 | Acc  Gap:  0.2469\n",
            "  Loss Ratio: 0.3012 | Acc Ratio: 1.4191\n",
            "Epoch 20/20\n",
            "  Train Loss: 0.4111 | Train Acc: 0.8450\n",
            "  Val   Loss: 1.5821 | Val   Acc: 0.5892\n",
            "  Loss Gap:   -1.1710 | Acc  Gap:  0.2559\n",
            "  Loss Ratio: 0.2599 | Acc Ratio: 1.4343\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▃▂▃▄▃▃▄▄▅▅▅▆▆▇▇▇███</td></tr><tr><td>acc_ratio</td><td>▁▄▃▃▄▄▄▄▅▅▆▅▆▆▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>loss_gap</td><td>█▇▇▇▆▇▇▆▆▆▅▅▄▄▃▃▃▁▂▁</td></tr><tr><td>loss_ratio</td><td>█▇▇▇▆▇▆▆▅▅▄▄▃▃▂▂▂▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▅▅▄▆▇▆▇▇▆████▇█▇██</td></tr><tr><td>val_loss</td><td>▆▇▃▃▃▂▁▂▂▁▃▂▃▃▄▅▅█▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.25587</td></tr><tr><td>acc_ratio</td><td>1.43429</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>loss_gap</td><td>-1.17096</td></tr><tr><td>loss_ratio</td><td>0.25987</td></tr><tr><td>train_accuracy</td><td>0.84504</td></tr><tr><td>train_loss</td><td>0.41114</td></tr><tr><td>val_accuracy</td><td>0.58917</td></tr><tr><td>val_loss</td><td>1.58209</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Dropout_0.25_LR_0.001_Epochs_20</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/hgb0zdf8' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/hgb0zdf8</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_232507-hgb0zdf8/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 7 - tune rl"
      ],
      "metadata": {
        "id": "tNnd5MphJEOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def train_and_find_best_lr(num_samples=5, lr_range=(1e-4, 1e-2), epochs=20, dropout_rate=0.3):\n",
        "    val_accuracies = {}\n",
        "    sampled_lrs = sorted([10 ** random.uniform(np.log10(lr_range[0]), np.log10(lr_range[1])) for _ in range(num_samples)])\n",
        "\n",
        "    for lr in sampled_lrs:\n",
        "        print(f\"\\n🔧 Training with learning rate: {lr:.6f}\")\n",
        "        run = wandb.init(\n",
        "            project=\"ML_4\",\n",
        "            entity=\"nkhar21-student\",\n",
        "            name=f\"FER_BatchNorm_LR_{lr:.6f}\",\n",
        "            config={\n",
        "                \"dropout_rate\": dropout_rate,\n",
        "                \"pool_kernel\": (2, 2),\n",
        "                \"epochs\": epochs,\n",
        "                \"batch_size\": 64,\n",
        "                \"lr\": lr\n",
        "            }\n",
        "        )\n",
        "\n",
        "        batch_size = 64\n",
        "        model = FERCNN_With_BatchNorm(num_classes=7, dropout_rate=dropout_rate).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        wandb.watch(model)\n",
        "\n",
        "        best_val_acc = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_loss, train_acc = evaluate_model(\n",
        "                x_train, y_train, model, loss_fn, device,\n",
        "                train_mode=True, optimizer=optimizer, batch_size=batch_size\n",
        "            )\n",
        "            val_loss, val_acc = evaluate_model(\n",
        "                x_val, y_val, model, loss_fn, device,\n",
        "                train_mode=False, optimizer=None, batch_size=batch_size\n",
        "            )\n",
        "\n",
        "            # Overfitting metrics\n",
        "            loss_gap = train_loss - val_loss\n",
        "            acc_gap = train_acc - val_acc\n",
        "            loss_ratio = train_loss / val_loss if val_loss != 0 else float(\"inf\")\n",
        "            acc_ratio = train_acc / val_acc if val_acc != 0 else float(\"inf\")\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "            print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "            print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "            print(f\"  Loss Gap:   {loss_gap:.4f} | Acc  Gap:  {acc_gap:.4f}\")\n",
        "            print(f\"  Loss Ratio: {loss_ratio:.4f} | Acc Ratio: {acc_ratio:.4f}\")\n",
        "\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_accuracy\": train_acc,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_accuracy\": val_acc,\n",
        "                \"loss_gap\": loss_gap,\n",
        "                \"acc_gap\": acc_gap,\n",
        "                \"loss_ratio\": loss_ratio,\n",
        "                \"acc_ratio\": acc_ratio,\n",
        "            })\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "\n",
        "        val_accuracies[lr] = best_val_acc\n",
        "        run.finish()\n",
        "\n",
        "    # ✅ Final result\n",
        "    best_lr = max(val_accuracies, key=val_accuracies.get)\n",
        "    print(\"\\n📈 Best Learning Rate:\")\n",
        "    print(f\"  ➤ LR: {best_lr:.6f} with Val Accuracy: {val_accuracies[best_lr]:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_yrlfZ82JFjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_find_best_lr(num_samples=6, lr_range=(1e-4, 1e-2), epochs=10, dropout_rate=0.3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PuWXPJqKJHye",
        "outputId": "95a875fa-17e2-4c6a-c464-af64ea965671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Training with learning rate: 0.000177\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_233231-y2320otu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/y2320otu' target=\"_blank\">FER_BatchNorm_LR_0.000177</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/y2320otu' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/y2320otu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  Train Loss: 1.6676 | Train Acc: 0.3377\n",
            "  Val   Loss: 1.4792 | Val   Acc: 0.4298\n",
            "  Loss Gap:   0.1885 | Acc  Gap:  -0.0921\n",
            "  Loss Ratio: 1.1274 | Acc Ratio: 0.7857\n",
            "Epoch 2/10\n",
            "  Train Loss: 1.4588 | Train Acc: 0.4377\n",
            "  Val   Loss: 1.3871 | Val   Acc: 0.4650\n",
            "  Loss Gap:   0.0718 | Acc  Gap:  -0.0273\n",
            "  Loss Ratio: 1.0517 | Acc Ratio: 0.9412\n",
            "Epoch 3/10\n",
            "  Train Loss: 1.3568 | Train Acc: 0.4806\n",
            "  Val   Loss: 1.2893 | Val   Acc: 0.5094\n",
            "  Loss Gap:   0.0675 | Acc  Gap:  -0.0288\n",
            "  Loss Ratio: 1.0524 | Acc Ratio: 0.9434\n",
            "Epoch 4/10\n",
            "  Train Loss: 1.2844 | Train Acc: 0.5112\n",
            "  Val   Loss: 1.2446 | Val   Acc: 0.5232\n",
            "  Loss Gap:   0.0398 | Acc  Gap:  -0.0120\n",
            "  Loss Ratio: 1.0319 | Acc Ratio: 0.9772\n",
            "Epoch 5/10\n",
            "  Train Loss: 1.2193 | Train Acc: 0.5396\n",
            "  Val   Loss: 1.2406 | Val   Acc: 0.5178\n",
            "  Loss Gap:   -0.0213 | Acc  Gap:  0.0219\n",
            "  Loss Ratio: 0.9828 | Acc Ratio: 1.0423\n",
            "Epoch 6/10\n",
            "  Train Loss: 1.1633 | Train Acc: 0.5622\n",
            "  Val   Loss: 1.1991 | Val   Acc: 0.5425\n",
            "  Loss Gap:   -0.0358 | Acc  Gap:  0.0197\n",
            "  Loss Ratio: 0.9701 | Acc Ratio: 1.0362\n",
            "Epoch 7/10\n",
            "  Train Loss: 1.1106 | Train Acc: 0.5827\n",
            "  Val   Loss: 1.1645 | Val   Acc: 0.5604\n",
            "  Loss Gap:   -0.0539 | Acc  Gap:  0.0223\n",
            "  Loss Ratio: 0.9537 | Acc Ratio: 1.0398\n",
            "Epoch 8/10\n",
            "  Train Loss: 1.0557 | Train Acc: 0.6020\n",
            "  Val   Loss: 1.1555 | Val   Acc: 0.5681\n",
            "  Loss Gap:   -0.0998 | Acc  Gap:  0.0339\n",
            "  Loss Ratio: 0.9137 | Acc Ratio: 1.0596\n",
            "Epoch 9/10\n",
            "  Train Loss: 0.9972 | Train Acc: 0.6244\n",
            "  Val   Loss: 1.1608 | Val   Acc: 0.5698\n",
            "  Loss Gap:   -0.1635 | Acc  Gap:  0.0546\n",
            "  Loss Ratio: 0.8591 | Acc Ratio: 1.0958\n",
            "Epoch 10/10\n",
            "  Train Loss: 0.9471 | Train Acc: 0.6412\n",
            "  Val   Loss: 1.1741 | Val   Acc: 0.5691\n",
            "  Loss Gap:   -0.2270 | Acc  Gap:  0.0721\n",
            "  Loss Ratio: 0.8066 | Acc Ratio: 1.1267\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▄▄▄▆▆▆▆▇█</td></tr><tr><td>acc_ratio</td><td>▁▄▄▅▆▆▆▇▇█</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss_gap</td><td>█▆▆▅▄▄▄▃▂▁</td></tr><tr><td>loss_ratio</td><td>█▆▆▆▅▅▄▃▂▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▆▅▇████</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.07208</td></tr><tr><td>acc_ratio</td><td>1.12666</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss_gap</td><td>-0.22703</td></tr><tr><td>loss_ratio</td><td>0.80664</td></tr><tr><td>train_accuracy</td><td>0.64122</td></tr><tr><td>train_loss</td><td>0.94711</td></tr><tr><td>val_accuracy</td><td>0.56914</td></tr><tr><td>val_loss</td><td>1.17414</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_LR_0.000177</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/y2320otu' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/y2320otu</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_233231-y2320otu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Training with learning rate: 0.001108\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_233307-3vt8ccay</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/3vt8ccay' target=\"_blank\">FER_BatchNorm_LR_0.001108</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/3vt8ccay' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/3vt8ccay</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  Train Loss: 1.6859 | Train Acc: 0.3215\n",
            "  Val   Loss: 1.4625 | Val   Acc: 0.4394\n",
            "  Loss Gap:   0.2234 | Acc  Gap:  -0.1179\n",
            "  Loss Ratio: 1.1528 | Acc Ratio: 0.7317\n",
            "Epoch 2/10\n",
            "  Train Loss: 1.4538 | Train Acc: 0.4379\n",
            "  Val   Loss: 1.3299 | Val   Acc: 0.4850\n",
            "  Loss Gap:   0.1239 | Acc  Gap:  -0.0471\n",
            "  Loss Ratio: 1.0931 | Acc Ratio: 0.9029\n",
            "Epoch 3/10\n",
            "  Train Loss: 1.3502 | Train Acc: 0.4817\n",
            "  Val   Loss: 1.2621 | Val   Acc: 0.5207\n",
            "  Loss Gap:   0.0881 | Acc  Gap:  -0.0390\n",
            "  Loss Ratio: 1.0698 | Acc Ratio: 0.9251\n",
            "Epoch 4/10\n",
            "  Train Loss: 1.2817 | Train Acc: 0.5105\n",
            "  Val   Loss: 1.2596 | Val   Acc: 0.5219\n",
            "  Loss Gap:   0.0220 | Acc  Gap:  -0.0114\n",
            "  Loss Ratio: 1.0175 | Acc Ratio: 0.9781\n",
            "Epoch 5/10\n",
            "  Train Loss: 1.2126 | Train Acc: 0.5334\n",
            "  Val   Loss: 1.2254 | Val   Acc: 0.5273\n",
            "  Loss Gap:   -0.0128 | Acc  Gap:  0.0061\n",
            "  Loss Ratio: 0.9896 | Acc Ratio: 1.0115\n",
            "Epoch 6/10\n",
            "  Train Loss: 1.1591 | Train Acc: 0.5521\n",
            "  Val   Loss: 1.2942 | Val   Acc: 0.4944\n",
            "  Loss Gap:   -0.1352 | Acc  Gap:  0.0576\n",
            "  Loss Ratio: 0.8956 | Acc Ratio: 1.1166\n",
            "Epoch 7/10\n",
            "  Train Loss: 1.1131 | Train Acc: 0.5768\n",
            "  Val   Loss: 1.2058 | Val   Acc: 0.5392\n",
            "  Loss Gap:   -0.0928 | Acc  Gap:  0.0376\n",
            "  Loss Ratio: 0.9231 | Acc Ratio: 1.0698\n",
            "Epoch 8/10\n",
            "  Train Loss: 1.0563 | Train Acc: 0.5981\n",
            "  Val   Loss: 1.1814 | Val   Acc: 0.5491\n",
            "  Loss Gap:   -0.1252 | Acc  Gap:  0.0490\n",
            "  Loss Ratio: 0.8941 | Acc Ratio: 1.0892\n",
            "Epoch 9/10\n",
            "  Train Loss: 1.0091 | Train Acc: 0.6120\n",
            "  Val   Loss: 1.1789 | Val   Acc: 0.5580\n",
            "  Loss Gap:   -0.1698 | Acc  Gap:  0.0540\n",
            "  Loss Ratio: 0.8559 | Acc Ratio: 1.0968\n",
            "Epoch 10/10\n",
            "  Train Loss: 0.9503 | Train Acc: 0.6370\n",
            "  Val   Loss: 1.1826 | Val   Acc: 0.5632\n",
            "  Loss Gap:   -0.2323 | Acc  Gap:  0.0737\n",
            "  Loss Ratio: 0.8036 | Acc Ratio: 1.1309\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▄▄▅▆▇▇▇▇█</td></tr><tr><td>acc_ratio</td><td>▁▄▄▅▆█▇▇▇█</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss_gap</td><td>█▆▆▅▄▂▃▃▂▁</td></tr><tr><td>loss_ratio</td><td>█▇▆▅▅▃▃▃▂▁</td></tr><tr><td>train_accuracy</td><td>▁▄▅▅▆▆▇▇▇█</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▃▂▂▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▆▄▇▇██</td></tr><tr><td>val_loss</td><td>█▅▃▃▂▄▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.07374</td></tr><tr><td>acc_ratio</td><td>1.13092</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss_gap</td><td>-0.2323</td></tr><tr><td>loss_ratio</td><td>0.80357</td></tr><tr><td>train_accuracy</td><td>0.63696</td></tr><tr><td>train_loss</td><td>0.95032</td></tr><tr><td>val_accuracy</td><td>0.56322</td></tr><tr><td>val_loss</td><td>1.18263</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_LR_0.001108</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/3vt8ccay' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/3vt8ccay</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_233307-3vt8ccay/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Training with learning rate: 0.004034\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_233343-ubw9p12b</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/ubw9p12b' target=\"_blank\">FER_BatchNorm_LR_0.004034</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/ubw9p12b' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/ubw9p12b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  Train Loss: 1.8075 | Train Acc: 0.2650\n",
            "  Val   Loss: 1.7009 | Val   Acc: 0.2811\n",
            "  Loss Gap:   0.1066 | Acc  Gap:  -0.0161\n",
            "  Loss Ratio: 1.0627 | Acc Ratio: 0.9429\n",
            "Epoch 2/10\n",
            "  Train Loss: 1.6419 | Train Acc: 0.3198\n",
            "  Val   Loss: 1.5298 | Val   Acc: 0.3873\n",
            "  Loss Gap:   0.1121 | Acc  Gap:  -0.0675\n",
            "  Loss Ratio: 1.0733 | Acc Ratio: 0.8257\n",
            "Epoch 3/10\n",
            "  Train Loss: 1.5420 | Train Acc: 0.3780\n",
            "  Val   Loss: 1.4600 | Val   Acc: 0.4157\n",
            "  Loss Gap:   0.0820 | Acc  Gap:  -0.0377\n",
            "  Loss Ratio: 1.0562 | Acc Ratio: 0.9093\n",
            "Epoch 4/10\n",
            "  Train Loss: 1.4709 | Train Acc: 0.4081\n",
            "  Val   Loss: 1.4387 | Val   Acc: 0.4265\n",
            "  Loss Gap:   0.0322 | Acc  Gap:  -0.0184\n",
            "  Loss Ratio: 1.0224 | Acc Ratio: 0.9569\n",
            "Epoch 5/10\n",
            "  Train Loss: 1.4299 | Train Acc: 0.4174\n",
            "  Val   Loss: 1.4012 | Val   Acc: 0.4295\n",
            "  Loss Gap:   0.0287 | Acc  Gap:  -0.0121\n",
            "  Loss Ratio: 1.0205 | Acc Ratio: 0.9719\n",
            "Epoch 6/10\n",
            "  Train Loss: 1.3788 | Train Acc: 0.4376\n",
            "  Val   Loss: 1.3977 | Val   Acc: 0.4175\n",
            "  Loss Gap:   -0.0190 | Acc  Gap:  0.0202\n",
            "  Loss Ratio: 0.9864 | Acc Ratio: 1.0483\n",
            "Epoch 7/10\n",
            "  Train Loss: 1.3439 | Train Acc: 0.4502\n",
            "  Val   Loss: 1.3282 | Val   Acc: 0.4545\n",
            "  Loss Gap:   0.0157 | Acc  Gap:  -0.0044\n",
            "  Loss Ratio: 1.0118 | Acc Ratio: 0.9904\n",
            "Epoch 8/10\n",
            "  Train Loss: 1.3063 | Train Acc: 0.4650\n",
            "  Val   Loss: 1.3427 | Val   Acc: 0.4619\n",
            "  Loss Gap:   -0.0363 | Acc  Gap:  0.0032\n",
            "  Loss Ratio: 0.9729 | Acc Ratio: 1.0068\n",
            "Epoch 9/10\n",
            "  Train Loss: 1.2645 | Train Acc: 0.4843\n",
            "  Val   Loss: 1.3101 | Val   Acc: 0.4852\n",
            "  Loss Gap:   -0.0456 | Acc  Gap:  -0.0008\n",
            "  Loss Ratio: 0.9652 | Acc Ratio: 0.9982\n",
            "Epoch 10/10\n",
            "  Train Loss: 1.2194 | Train Acc: 0.5123\n",
            "  Val   Loss: 1.2601 | Val   Acc: 0.5066\n",
            "  Loss Gap:   -0.0406 | Acc  Gap:  0.0057\n",
            "  Loss Ratio: 0.9677 | Acc Ratio: 1.0112\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▅▁▃▅▅█▆▇▆▇</td></tr><tr><td>acc_ratio</td><td>▅▁▄▅▆█▆▇▆▇</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss_gap</td><td>██▇▄▄▂▄▁▁▁</td></tr><tr><td>loss_ratio</td><td>▇█▇▅▅▂▄▂▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▅▆▆▇▇█</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▂▂▂▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▆▅▆▇▇█</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▃▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.00568</td></tr><tr><td>acc_ratio</td><td>1.01122</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss_gap</td><td>-0.04064</td></tr><tr><td>loss_ratio</td><td>0.96774</td></tr><tr><td>train_accuracy</td><td>0.5123</td></tr><tr><td>train_loss</td><td>1.21942</td></tr><tr><td>val_accuracy</td><td>0.50662</td></tr><tr><td>val_loss</td><td>1.26007</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_LR_0.004034</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/ubw9p12b' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/ubw9p12b</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_233343-ubw9p12b/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Training with learning rate: 0.004213\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_233418-iynr9tvy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/iynr9tvy' target=\"_blank\">FER_BatchNorm_LR_0.004213</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/iynr9tvy' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/iynr9tvy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  Train Loss: 1.8333 | Train Acc: 0.2500\n",
            "  Val   Loss: 1.7312 | Val   Acc: 0.2976\n",
            "  Loss Gap:   0.1020 | Acc  Gap:  -0.0476\n",
            "  Loss Ratio: 1.0589 | Acc Ratio: 0.8400\n",
            "Epoch 2/10\n",
            "  Train Loss: 1.6926 | Train Acc: 0.2980\n",
            "  Val   Loss: 1.5979 | Val   Acc: 0.3690\n",
            "  Loss Gap:   0.0947 | Acc  Gap:  -0.0710\n",
            "  Loss Ratio: 1.0593 | Acc Ratio: 0.8076\n",
            "Epoch 3/10\n",
            "  Train Loss: 1.6327 | Train Acc: 0.3238\n",
            "  Val   Loss: 1.5253 | Val   Acc: 0.3943\n",
            "  Loss Gap:   0.1073 | Acc  Gap:  -0.0705\n",
            "  Loss Ratio: 1.0703 | Acc Ratio: 0.8211\n",
            "Epoch 4/10\n",
            "  Train Loss: 1.5956 | Train Acc: 0.3409\n",
            "  Val   Loss: 1.4675 | Val   Acc: 0.4178\n",
            "  Loss Gap:   0.1281 | Acc  Gap:  -0.0769\n",
            "  Loss Ratio: 1.0873 | Acc Ratio: 0.8160\n",
            "Epoch 5/10\n",
            "  Train Loss: 1.5680 | Train Acc: 0.3493\n",
            "  Val   Loss: 1.4891 | Val   Acc: 0.4127\n",
            "  Loss Gap:   0.0789 | Acc  Gap:  -0.0635\n",
            "  Loss Ratio: 1.0530 | Acc Ratio: 0.8462\n",
            "Epoch 6/10\n",
            "  Train Loss: 1.5503 | Train Acc: 0.3571\n",
            "  Val   Loss: 1.4456 | Val   Acc: 0.4300\n",
            "  Loss Gap:   0.1047 | Acc  Gap:  -0.0729\n",
            "  Loss Ratio: 1.0724 | Acc Ratio: 0.8304\n",
            "Epoch 7/10\n",
            "  Train Loss: 1.5343 | Train Acc: 0.3591\n",
            "  Val   Loss: 1.4220 | Val   Acc: 0.4314\n",
            "  Loss Gap:   0.1123 | Acc  Gap:  -0.0723\n",
            "  Loss Ratio: 1.0790 | Acc Ratio: 0.8325\n",
            "Epoch 8/10\n",
            "  Train Loss: 1.5163 | Train Acc: 0.3695\n",
            "  Val   Loss: 1.4320 | Val   Acc: 0.4260\n",
            "  Loss Gap:   0.0843 | Acc  Gap:  -0.0565\n",
            "  Loss Ratio: 1.0588 | Acc Ratio: 0.8674\n",
            "Epoch 9/10\n",
            "  Train Loss: 1.4978 | Train Acc: 0.3729\n",
            "  Val   Loss: 1.4111 | Val   Acc: 0.4293\n",
            "  Loss Gap:   0.0868 | Acc  Gap:  -0.0564\n",
            "  Loss Ratio: 1.0615 | Acc Ratio: 0.8687\n",
            "Epoch 10/10\n",
            "  Train Loss: 1.4913 | Train Acc: 0.3756\n",
            "  Val   Loss: 1.3896 | Val   Acc: 0.4373\n",
            "  Loss Gap:   0.1017 | Acc  Gap:  -0.0617\n",
            "  Loss Ratio: 1.0732 | Acc Ratio: 0.8590\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>█▂▃▁▄▂▂▆▆▅</td></tr><tr><td>acc_ratio</td><td>▅▁▃▂▅▄▄██▇</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss_gap</td><td>▄▃▅█▁▅▆▂▂▄</td></tr><tr><td>loss_ratio</td><td>▂▂▅█▁▅▆▂▃▅</td></tr><tr><td>train_accuracy</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇██▇██</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>-0.06168</td></tr><tr><td>acc_ratio</td><td>0.85896</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss_gap</td><td>0.1017</td></tr><tr><td>loss_ratio</td><td>1.07319</td></tr><tr><td>train_accuracy</td><td>0.37563</td></tr><tr><td>train_loss</td><td>1.4913</td></tr><tr><td>val_accuracy</td><td>0.4373</td></tr><tr><td>val_loss</td><td>1.3896</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_LR_0.004213</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/iynr9tvy' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/iynr9tvy</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_233418-iynr9tvy/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Training with learning rate: 0.005207\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_233454-5ign4c42</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/5ign4c42' target=\"_blank\">FER_BatchNorm_LR_0.005207</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/5ign4c42' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/5ign4c42</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  Train Loss: 1.8794 | Train Acc: 0.2468\n",
            "  Val   Loss: 1.9054 | Val   Acc: 0.2005\n",
            "  Loss Gap:   -0.0261 | Acc  Gap:  0.0464\n",
            "  Loss Ratio: 0.9863 | Acc Ratio: 1.2314\n",
            "Epoch 2/10\n",
            "  Train Loss: 1.7290 | Train Acc: 0.2858\n",
            "  Val   Loss: 1.6311 | Val   Acc: 0.3335\n",
            "  Loss Gap:   0.0979 | Acc  Gap:  -0.0477\n",
            "  Loss Ratio: 1.0600 | Acc Ratio: 0.8570\n",
            "Epoch 3/10\n",
            "  Train Loss: 1.6330 | Train Acc: 0.3368\n",
            "  Val   Loss: 1.5310 | Val   Acc: 0.3974\n",
            "  Loss Gap:   0.1020 | Acc  Gap:  -0.0606\n",
            "  Loss Ratio: 1.0666 | Acc Ratio: 0.8474\n",
            "Epoch 4/10\n",
            "  Train Loss: 1.5842 | Train Acc: 0.3611\n",
            "  Val   Loss: 1.5919 | Val   Acc: 0.3398\n",
            "  Loss Gap:   -0.0078 | Acc  Gap:  0.0213\n",
            "  Loss Ratio: 0.9951 | Acc Ratio: 1.0628\n",
            "Epoch 5/10\n",
            "  Train Loss: 1.5545 | Train Acc: 0.3722\n",
            "  Val   Loss: 1.4638 | Val   Acc: 0.4230\n",
            "  Loss Gap:   0.0906 | Acc  Gap:  -0.0508\n",
            "  Loss Ratio: 1.0619 | Acc Ratio: 0.8798\n",
            "Epoch 6/10\n",
            "  Train Loss: 1.5185 | Train Acc: 0.3874\n",
            "  Val   Loss: 1.4879 | Val   Acc: 0.4115\n",
            "  Loss Gap:   0.0306 | Acc  Gap:  -0.0241\n",
            "  Loss Ratio: 1.0206 | Acc Ratio: 0.9414\n",
            "Epoch 7/10\n",
            "  Train Loss: 1.4944 | Train Acc: 0.3971\n",
            "  Val   Loss: 1.4465 | Val   Acc: 0.4185\n",
            "  Loss Gap:   0.0479 | Acc  Gap:  -0.0214\n",
            "  Loss Ratio: 1.0331 | Acc Ratio: 0.9490\n",
            "Epoch 8/10\n",
            "  Train Loss: 1.4776 | Train Acc: 0.3969\n",
            "  Val   Loss: 1.4213 | Val   Acc: 0.4269\n",
            "  Loss Gap:   0.0563 | Acc  Gap:  -0.0300\n",
            "  Loss Ratio: 1.0396 | Acc Ratio: 0.9298\n",
            "Epoch 9/10\n",
            "  Train Loss: 1.4501 | Train Acc: 0.4093\n",
            "  Val   Loss: 1.4224 | Val   Acc: 0.3971\n",
            "  Loss Gap:   0.0277 | Acc  Gap:  0.0123\n",
            "  Loss Ratio: 1.0195 | Acc Ratio: 1.0309\n",
            "Epoch 10/10\n",
            "  Train Loss: 1.4335 | Train Acc: 0.4120\n",
            "  Val   Loss: 1.4572 | Val   Acc: 0.3898\n",
            "  Loss Gap:   -0.0238 | Acc  Gap:  0.0223\n",
            "  Loss Ratio: 0.9837 | Acc Ratio: 1.0571\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>█▂▁▆▂▃▄▃▆▆</td></tr><tr><td>acc_ratio</td><td>█▁▁▅▂▃▃▃▄▅</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss_gap</td><td>▁██▂▇▄▅▆▄▁</td></tr><tr><td>loss_ratio</td><td>▁▇█▂█▄▅▆▄▁</td></tr><tr><td>train_accuracy</td><td>▁▃▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▆▄▃▃▂▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▇▅████▇▇</td></tr><tr><td>val_loss</td><td>█▄▃▃▂▂▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.02227</td></tr><tr><td>acc_ratio</td><td>1.05713</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss_gap</td><td>-0.02376</td></tr><tr><td>loss_ratio</td><td>0.9837</td></tr><tr><td>train_accuracy</td><td>0.41203</td></tr><tr><td>train_loss</td><td>1.43347</td></tr><tr><td>val_accuracy</td><td>0.38976</td></tr><tr><td>val_loss</td><td>1.45723</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_LR_0.005207</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/5ign4c42' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/5ign4c42</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_233454-5ign4c42/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Training with learning rate: 0.005665\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_233530-8dcaag84</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/8dcaag84' target=\"_blank\">FER_BatchNorm_LR_0.005665</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/8dcaag84' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/8dcaag84</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  Train Loss: 1.8870 | Train Acc: 0.2509\n",
            "  Val   Loss: 1.7679 | Val   Acc: 0.2652\n",
            "  Loss Gap:   0.1191 | Acc  Gap:  -0.0144\n",
            "  Loss Ratio: 1.0674 | Acc Ratio: 0.9459\n",
            "Epoch 2/10\n",
            "  Train Loss: 1.7560 | Train Acc: 0.2787\n",
            "  Val   Loss: 1.7641 | Val   Acc: 0.2717\n",
            "  Loss Gap:   -0.0081 | Acc  Gap:  0.0070\n",
            "  Loss Ratio: 0.9954 | Acc Ratio: 1.0258\n",
            "Epoch 3/10\n",
            "  Train Loss: 1.6931 | Train Acc: 0.3049\n",
            "  Val   Loss: 1.6014 | Val   Acc: 0.3596\n",
            "  Loss Gap:   0.0918 | Acc  Gap:  -0.0547\n",
            "  Loss Ratio: 1.0573 | Acc Ratio: 0.8479\n",
            "Epoch 4/10\n",
            "  Train Loss: 1.6105 | Train Acc: 0.3491\n",
            "  Val   Loss: 1.5299 | Val   Acc: 0.3591\n",
            "  Loss Gap:   0.0806 | Acc  Gap:  -0.0100\n",
            "  Loss Ratio: 1.0527 | Acc Ratio: 0.9720\n",
            "Epoch 5/10\n",
            "  Train Loss: 1.5644 | Train Acc: 0.3653\n",
            "  Val   Loss: 1.4812 | Val   Acc: 0.4058\n",
            "  Loss Gap:   0.0832 | Acc  Gap:  -0.0405\n",
            "  Loss Ratio: 1.0562 | Acc Ratio: 0.9001\n",
            "Epoch 6/10\n",
            "  Train Loss: 1.5183 | Train Acc: 0.3851\n",
            "  Val   Loss: 1.4245 | Val   Acc: 0.4276\n",
            "  Loss Gap:   0.0939 | Acc  Gap:  -0.0424\n",
            "  Loss Ratio: 1.0659 | Acc Ratio: 0.9008\n",
            "Epoch 7/10\n",
            "  Train Loss: 1.4801 | Train Acc: 0.4034\n",
            "  Val   Loss: 1.4318 | Val   Acc: 0.4089\n",
            "  Loss Gap:   0.0484 | Acc  Gap:  -0.0055\n",
            "  Loss Ratio: 1.0338 | Acc Ratio: 0.9865\n",
            "Epoch 8/10\n",
            "  Train Loss: 1.4573 | Train Acc: 0.4106\n",
            "  Val   Loss: 1.4358 | Val   Acc: 0.4354\n",
            "  Loss Gap:   0.0215 | Acc  Gap:  -0.0248\n",
            "  Loss Ratio: 1.0150 | Acc Ratio: 0.9431\n",
            "Epoch 9/10\n",
            "  Train Loss: 1.4315 | Train Acc: 0.4203\n",
            "  Val   Loss: 1.4261 | Val   Acc: 0.4227\n",
            "  Loss Gap:   0.0054 | Acc  Gap:  -0.0023\n",
            "  Loss Ratio: 1.0038 | Acc Ratio: 0.9945\n",
            "Epoch 10/10\n",
            "  Train Loss: 1.3974 | Train Acc: 0.4329\n",
            "  Val   Loss: 1.3807 | Val   Acc: 0.4345\n",
            "  Loss Gap:   0.0167 | Acc  Gap:  -0.0016\n",
            "  Loss Ratio: 1.0121 | Acc Ratio: 0.9962\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▆█▁▆▃▂▇▄▇▇</td></tr><tr><td>acc_ratio</td><td>▅█▁▆▃▃▆▅▇▇</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss_gap</td><td>█▁▆▆▆▇▄▃▂▂</td></tr><tr><td>loss_ratio</td><td>█▁▇▇▇█▅▃▂▃</td></tr><tr><td>train_accuracy</td><td>▁▂▃▅▅▆▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▅▅▇█▇█▇█</td></tr><tr><td>val_loss</td><td>██▅▄▃▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>-0.00164</td></tr><tr><td>acc_ratio</td><td>0.99624</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>loss_gap</td><td>0.01671</td></tr><tr><td>loss_ratio</td><td>1.0121</td></tr><tr><td>train_accuracy</td><td>0.43288</td></tr><tr><td>train_loss</td><td>1.39744</td></tr><tr><td>val_accuracy</td><td>0.43452</td></tr><tr><td>val_loss</td><td>1.38073</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_LR_0.005665</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/8dcaag84' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/8dcaag84</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_233530-8dcaag84/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📈 Best Learning Rate:\n",
            "  ➤ LR: 0.000177 with Val Accuracy: 0.5698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_and_find_best_lr(num_samples=5, lr_range=(1e-3, 1e-2), epochs=20, dropout_rate=0.3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kBngQ8PKKxjD",
        "outputId": "495dfbac-2408-4449-9cad-755378b749ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Training with learning rate: 0.001571\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_233656-8jpoxlfw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/8jpoxlfw' target=\"_blank\">FER_BatchNorm_LR_0.001571</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/8jpoxlfw' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/8jpoxlfw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "  Train Loss: 1.7230 | Train Acc: 0.2999\n",
            "  Val   Loss: 1.5193 | Val   Acc: 0.4190\n",
            "  Loss Gap:   0.2037 | Acc  Gap:  -0.1192\n",
            "  Loss Ratio: 1.1341 | Acc Ratio: 0.7156\n",
            "Epoch 2/20\n",
            "  Train Loss: 1.4906 | Train Acc: 0.4094\n",
            "  Val   Loss: 1.4297 | Val   Acc: 0.4521\n",
            "  Loss Gap:   0.0608 | Acc  Gap:  -0.0427\n",
            "  Loss Ratio: 1.0425 | Acc Ratio: 0.9055\n",
            "Epoch 3/20\n",
            "  Train Loss: 1.3910 | Train Acc: 0.4604\n",
            "  Val   Loss: 1.3049 | Val   Acc: 0.4991\n",
            "  Loss Gap:   0.0861 | Acc  Gap:  -0.0388\n",
            "  Loss Ratio: 1.0660 | Acc Ratio: 0.9223\n",
            "Epoch 4/20\n",
            "  Train Loss: 1.3205 | Train Acc: 0.4869\n",
            "  Val   Loss: 1.2853 | Val   Acc: 0.5138\n",
            "  Loss Gap:   0.0352 | Acc  Gap:  -0.0269\n",
            "  Loss Ratio: 1.0274 | Acc Ratio: 0.9477\n",
            "Epoch 5/20\n",
            "  Train Loss: 1.2588 | Train Acc: 0.5074\n",
            "  Val   Loss: 1.2934 | Val   Acc: 0.5111\n",
            "  Loss Gap:   -0.0346 | Acc  Gap:  -0.0037\n",
            "  Loss Ratio: 0.9733 | Acc Ratio: 0.9927\n",
            "Epoch 6/20\n",
            "  Train Loss: 1.2083 | Train Acc: 0.5341\n",
            "  Val   Loss: 1.2026 | Val   Acc: 0.5456\n",
            "  Loss Gap:   0.0057 | Acc  Gap:  -0.0116\n",
            "  Loss Ratio: 1.0047 | Acc Ratio: 0.9788\n",
            "Epoch 7/20\n",
            "  Train Loss: 1.1582 | Train Acc: 0.5536\n",
            "  Val   Loss: 1.1705 | Val   Acc: 0.5507\n",
            "  Loss Gap:   -0.0123 | Acc  Gap:  0.0029\n",
            "  Loss Ratio: 0.9895 | Acc Ratio: 1.0053\n",
            "Epoch 8/20\n",
            "  Train Loss: 1.1163 | Train Acc: 0.5741\n",
            "  Val   Loss: 1.1595 | Val   Acc: 0.5658\n",
            "  Loss Gap:   -0.0432 | Acc  Gap:  0.0083\n",
            "  Loss Ratio: 0.9628 | Acc Ratio: 1.0146\n",
            "Epoch 9/20\n",
            "  Train Loss: 1.0548 | Train Acc: 0.5948\n",
            "  Val   Loss: 1.1382 | Val   Acc: 0.5670\n",
            "  Loss Gap:   -0.0835 | Acc  Gap:  0.0278\n",
            "  Loss Ratio: 0.9267 | Acc Ratio: 1.0490\n",
            "Epoch 10/20\n",
            "  Train Loss: 1.0022 | Train Acc: 0.6188\n",
            "  Val   Loss: 1.1512 | Val   Acc: 0.5676\n",
            "  Loss Gap:   -0.1490 | Acc  Gap:  0.0513\n",
            "  Loss Ratio: 0.8706 | Acc Ratio: 1.0903\n",
            "Epoch 11/20\n",
            "  Train Loss: 0.9510 | Train Acc: 0.6367\n",
            "  Val   Loss: 1.1784 | Val   Acc: 0.5521\n",
            "  Loss Gap:   -0.2274 | Acc  Gap:  0.0846\n",
            "  Loss Ratio: 0.8070 | Acc Ratio: 1.1532\n",
            "Epoch 12/20\n",
            "  Train Loss: 0.8992 | Train Acc: 0.6592\n",
            "  Val   Loss: 1.1272 | Val   Acc: 0.5883\n",
            "  Loss Gap:   -0.2280 | Acc  Gap:  0.0709\n",
            "  Loss Ratio: 0.7977 | Acc Ratio: 1.1205\n",
            "Epoch 13/20\n",
            "  Train Loss: 0.8455 | Train Acc: 0.6796\n",
            "  Val   Loss: 1.2580 | Val   Acc: 0.5535\n",
            "  Loss Gap:   -0.4126 | Acc  Gap:  0.1262\n",
            "  Loss Ratio: 0.6720 | Acc Ratio: 1.2279\n",
            "Epoch 14/20\n",
            "  Train Loss: 0.8015 | Train Acc: 0.6967\n",
            "  Val   Loss: 1.2037 | Val   Acc: 0.5664\n",
            "  Loss Gap:   -0.4022 | Acc  Gap:  0.1303\n",
            "  Loss Ratio: 0.6658 | Acc Ratio: 1.2301\n",
            "Epoch 15/20\n",
            "  Train Loss: 0.7624 | Train Acc: 0.7099\n",
            "  Val   Loss: 1.2110 | Val   Acc: 0.5724\n",
            "  Loss Gap:   -0.4486 | Acc  Gap:  0.1375\n",
            "  Loss Ratio: 0.6296 | Acc Ratio: 1.2402\n",
            "Epoch 16/20\n",
            "  Train Loss: 0.7226 | Train Acc: 0.7267\n",
            "  Val   Loss: 1.2384 | Val   Acc: 0.5832\n",
            "  Loss Gap:   -0.5158 | Acc  Gap:  0.1435\n",
            "  Loss Ratio: 0.5835 | Acc Ratio: 1.2460\n",
            "Epoch 17/20\n",
            "  Train Loss: 0.6762 | Train Acc: 0.7460\n",
            "  Val   Loss: 1.2507 | Val   Acc: 0.5658\n",
            "  Loss Gap:   -0.5745 | Acc  Gap:  0.1802\n",
            "  Loss Ratio: 0.5406 | Acc Ratio: 1.3184\n",
            "Epoch 18/20\n",
            "  Train Loss: 0.6483 | Train Acc: 0.7562\n",
            "  Val   Loss: 1.3072 | Val   Acc: 0.5714\n",
            "  Loss Gap:   -0.6589 | Acc  Gap:  0.1848\n",
            "  Loss Ratio: 0.4960 | Acc Ratio: 1.3234\n",
            "Epoch 19/20\n",
            "  Train Loss: 0.6284 | Train Acc: 0.7631\n",
            "  Val   Loss: 1.3847 | Val   Acc: 0.5639\n",
            "  Loss Gap:   -0.7563 | Acc  Gap:  0.1992\n",
            "  Loss Ratio: 0.4538 | Acc Ratio: 1.3532\n",
            "Epoch 20/20\n",
            "  Train Loss: 0.5961 | Train Acc: 0.7763\n",
            "  Val   Loss: 1.3881 | Val   Acc: 0.5599\n",
            "  Loss Gap:   -0.7920 | Acc  Gap:  0.2164\n",
            "  Loss Ratio: 0.4294 | Acc Ratio: 1.3865\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇██</td></tr><tr><td>acc_ratio</td><td>▁▃▃▃▄▄▄▄▄▅▆▅▆▆▆▇▇▇██</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>loss_gap</td><td>█▇▇▇▆▇▆▆▆▆▅▅▄▄▃▃▃▂▁▁</td></tr><tr><td>loss_ratio</td><td>█▇▇▇▆▇▇▆▆▅▅▅▃▃▃▃▂▂▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▅▄▄▄▄▃▃▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▄▅▅▆▆▇▇▇▇█▇▇▇█▇▇▇▇</td></tr><tr><td>val_loss</td><td>█▆▄▄▄▂▂▂▁▁▂▁▃▂▂▃▃▄▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.21642</td></tr><tr><td>acc_ratio</td><td>1.38653</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>loss_gap</td><td>-0.792</td></tr><tr><td>loss_ratio</td><td>0.42943</td></tr><tr><td>train_accuracy</td><td>0.77633</td></tr><tr><td>train_loss</td><td>0.59609</td></tr><tr><td>val_accuracy</td><td>0.55991</td></tr><tr><td>val_loss</td><td>1.38809</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_LR_0.001571</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/8jpoxlfw' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/8jpoxlfw</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_233656-8jpoxlfw/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Training with learning rate: 0.002092\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_233805-yifxk0eq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/yifxk0eq' target=\"_blank\">FER_BatchNorm_LR_0.002092</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/yifxk0eq' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/yifxk0eq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "  Train Loss: 1.7127 | Train Acc: 0.3097\n",
            "  Val   Loss: 1.5846 | Val   Acc: 0.3871\n",
            "  Loss Gap:   0.1281 | Acc  Gap:  -0.0775\n",
            "  Loss Ratio: 1.0809 | Acc Ratio: 0.7999\n",
            "Epoch 2/20\n",
            "  Train Loss: 1.5079 | Train Acc: 0.4101\n",
            "  Val   Loss: 1.4162 | Val   Acc: 0.4653\n",
            "  Loss Gap:   0.0917 | Acc  Gap:  -0.0553\n",
            "  Loss Ratio: 1.0647 | Acc Ratio: 0.8812\n",
            "Epoch 3/20\n",
            "  Train Loss: 1.4102 | Train Acc: 0.4460\n",
            "  Val   Loss: 1.3445 | Val   Acc: 0.4882\n",
            "  Loss Gap:   0.0657 | Acc  Gap:  -0.0421\n",
            "  Loss Ratio: 1.0489 | Acc Ratio: 0.9137\n",
            "Epoch 4/20\n",
            "  Train Loss: 1.3414 | Train Acc: 0.4805\n",
            "  Val   Loss: 1.3335 | Val   Acc: 0.4697\n",
            "  Loss Gap:   0.0079 | Acc  Gap:  0.0108\n",
            "  Loss Ratio: 1.0060 | Acc Ratio: 1.0230\n",
            "Epoch 5/20\n",
            "  Train Loss: 1.2743 | Train Acc: 0.5093\n",
            "  Val   Loss: 1.2721 | Val   Acc: 0.5253\n",
            "  Loss Gap:   0.0021 | Acc  Gap:  -0.0159\n",
            "  Loss Ratio: 1.0017 | Acc Ratio: 0.9697\n",
            "Epoch 6/20\n",
            "  Train Loss: 1.2172 | Train Acc: 0.5362\n",
            "  Val   Loss: 1.2082 | Val   Acc: 0.5413\n",
            "  Loss Gap:   0.0090 | Acc  Gap:  -0.0050\n",
            "  Loss Ratio: 1.0075 | Acc Ratio: 0.9907\n",
            "Epoch 7/20\n",
            "  Train Loss: 1.1661 | Train Acc: 0.5510\n",
            "  Val   Loss: 1.2182 | Val   Acc: 0.5413\n",
            "  Loss Gap:   -0.0521 | Acc  Gap:  0.0097\n",
            "  Loss Ratio: 0.9572 | Acc Ratio: 1.0180\n",
            "Epoch 8/20\n",
            "  Train Loss: 1.1039 | Train Acc: 0.5760\n",
            "  Val   Loss: 1.1739 | Val   Acc: 0.5528\n",
            "  Loss Gap:   -0.0700 | Acc  Gap:  0.0232\n",
            "  Loss Ratio: 0.9404 | Acc Ratio: 1.0420\n",
            "Epoch 9/20\n",
            "  Train Loss: 1.0515 | Train Acc: 0.5975\n",
            "  Val   Loss: 1.1981 | Val   Acc: 0.5484\n",
            "  Loss Gap:   -0.1466 | Acc  Gap:  0.0491\n",
            "  Loss Ratio: 0.8776 | Acc Ratio: 1.0894\n",
            "Epoch 10/20\n",
            "  Train Loss: 0.9945 | Train Acc: 0.6178\n",
            "  Val   Loss: 1.2092 | Val   Acc: 0.5498\n",
            "  Loss Gap:   -0.2148 | Acc  Gap:  0.0679\n",
            "  Loss Ratio: 0.8224 | Acc Ratio: 1.1236\n",
            "Epoch 11/20\n",
            "  Train Loss: 0.9419 | Train Acc: 0.6373\n",
            "  Val   Loss: 1.1943 | Val   Acc: 0.5503\n",
            "  Loss Gap:   -0.2524 | Acc  Gap:  0.0869\n",
            "  Loss Ratio: 0.7887 | Acc Ratio: 1.1580\n",
            "Epoch 12/20\n",
            "  Train Loss: 0.9008 | Train Acc: 0.6514\n",
            "  Val   Loss: 1.1954 | Val   Acc: 0.5597\n",
            "  Loss Gap:   -0.2946 | Acc  Gap:  0.0916\n",
            "  Loss Ratio: 0.7535 | Acc Ratio: 1.1637\n",
            "Epoch 13/20\n",
            "  Train Loss: 0.8558 | Train Acc: 0.6701\n",
            "  Val   Loss: 1.2125 | Val   Acc: 0.5623\n",
            "  Loss Gap:   -0.3567 | Acc  Gap:  0.1078\n",
            "  Loss Ratio: 0.7058 | Acc Ratio: 1.1917\n",
            "Epoch 14/20\n",
            "  Train Loss: 0.8130 | Train Acc: 0.6835\n",
            "  Val   Loss: 1.2146 | Val   Acc: 0.5604\n",
            "  Loss Gap:   -0.4016 | Acc  Gap:  0.1231\n",
            "  Loss Ratio: 0.6694 | Acc Ratio: 1.2197\n",
            "Epoch 15/20\n",
            "  Train Loss: 0.7682 | Train Acc: 0.7034\n",
            "  Val   Loss: 1.2440 | Val   Acc: 0.5646\n",
            "  Loss Gap:   -0.4758 | Acc  Gap:  0.1388\n",
            "  Loss Ratio: 0.6175 | Acc Ratio: 1.2458\n",
            "Epoch 16/20\n",
            "  Train Loss: 0.7339 | Train Acc: 0.7171\n",
            "  Val   Loss: 1.3311 | Val   Acc: 0.5646\n",
            "  Loss Gap:   -0.5972 | Acc  Gap:  0.1525\n",
            "  Loss Ratio: 0.5514 | Acc Ratio: 1.2700\n",
            "Epoch 17/20\n",
            "  Train Loss: 0.7121 | Train Acc: 0.7276\n",
            "  Val   Loss: 1.3269 | Val   Acc: 0.5552\n",
            "  Loss Gap:   -0.6148 | Acc  Gap:  0.1724\n",
            "  Loss Ratio: 0.5367 | Acc Ratio: 1.3104\n",
            "Epoch 18/20\n",
            "  Train Loss: 0.6684 | Train Acc: 0.7424\n",
            "  Val   Loss: 1.3798 | Val   Acc: 0.5425\n",
            "  Loss Gap:   -0.7114 | Acc  Gap:  0.1999\n",
            "  Loss Ratio: 0.4844 | Acc Ratio: 1.3685\n",
            "Epoch 19/20\n",
            "  Train Loss: 0.6471 | Train Acc: 0.7473\n",
            "  Val   Loss: 1.3887 | Val   Acc: 0.5465\n",
            "  Loss Gap:   -0.7415 | Acc  Gap:  0.2008\n",
            "  Loss Ratio: 0.4660 | Acc Ratio: 1.3675\n",
            "Epoch 20/20\n",
            "  Train Loss: 0.6153 | Train Acc: 0.7644\n",
            "  Val   Loss: 1.3536 | Val   Acc: 0.5585\n",
            "  Loss Gap:   -0.7383 | Acc  Gap:  0.2059\n",
            "  Loss Ratio: 0.4546 | Acc Ratio: 1.3687\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▂▂▃▃▃▃▃▄▅▅▅▆▆▆▇▇███</td></tr><tr><td>acc_ratio</td><td>▁▂▂▄▃▃▄▄▅▅▅▅▆▆▆▇▇███</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>loss_gap</td><td>██▇▇▇▇▇▆▆▅▅▅▄▄▃▂▂▁▁▁</td></tr><tr><td>loss_ratio</td><td>███▇▇▇▇▆▆▅▅▄▄▃▃▂▂▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▃▄▄▄▅▅▅▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▅▅▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▄▆▇▇█▇▇▇██████▇▇█</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▂▂▁▁▂▁▁▂▂▂▄▄▅▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.20593</td></tr><tr><td>acc_ratio</td><td>1.36871</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>loss_gap</td><td>-0.7383</td></tr><tr><td>loss_ratio</td><td>0.45457</td></tr><tr><td>train_accuracy</td><td>0.76444</td></tr><tr><td>train_loss</td><td>0.61532</td></tr><tr><td>val_accuracy</td><td>0.55852</td></tr><tr><td>val_loss</td><td>1.35362</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_LR_0.002092</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/yifxk0eq' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/yifxk0eq</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_233805-yifxk0eq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Training with learning rate: 0.002475\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_233914-ubek0iv3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/ubek0iv3' target=\"_blank\">FER_BatchNorm_LR_0.002475</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/ubek0iv3' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/ubek0iv3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "  Train Loss: 1.8085 | Train Acc: 0.2573\n",
            "  Val   Loss: 1.6899 | Val   Acc: 0.3514\n",
            "  Loss Gap:   0.1186 | Acc  Gap:  -0.0941\n",
            "  Loss Ratio: 1.0702 | Acc Ratio: 0.7322\n",
            "Epoch 2/20\n",
            "  Train Loss: 1.6242 | Train Acc: 0.3475\n",
            "  Val   Loss: 1.6113 | Val   Acc: 0.3452\n",
            "  Loss Gap:   0.0130 | Acc  Gap:  0.0024\n",
            "  Loss Ratio: 1.0081 | Acc Ratio: 1.0069\n",
            "Epoch 3/20\n",
            "  Train Loss: 1.5266 | Train Acc: 0.3908\n",
            "  Val   Loss: 1.4953 | Val   Acc: 0.4047\n",
            "  Loss Gap:   0.0313 | Acc  Gap:  -0.0140\n",
            "  Loss Ratio: 1.0209 | Acc Ratio: 0.9655\n",
            "Epoch 4/20\n",
            "  Train Loss: 1.4579 | Train Acc: 0.4240\n",
            "  Val   Loss: 1.4470 | Val   Acc: 0.4336\n",
            "  Loss Gap:   0.0109 | Acc  Gap:  -0.0096\n",
            "  Loss Ratio: 1.0076 | Acc Ratio: 0.9779\n",
            "Epoch 5/20\n",
            "  Train Loss: 1.3988 | Train Acc: 0.4526\n",
            "  Val   Loss: 1.3670 | Val   Acc: 0.4831\n",
            "  Loss Gap:   0.0318 | Acc  Gap:  -0.0305\n",
            "  Loss Ratio: 1.0232 | Acc Ratio: 0.9368\n",
            "Epoch 6/20\n",
            "  Train Loss: 1.3539 | Train Acc: 0.4716\n",
            "  Val   Loss: 1.4079 | Val   Acc: 0.4700\n",
            "  Loss Gap:   -0.0539 | Acc  Gap:  0.0016\n",
            "  Loss Ratio: 0.9617 | Acc Ratio: 1.0034\n",
            "Epoch 7/20\n",
            "  Train Loss: 1.3067 | Train Acc: 0.4981\n",
            "  Val   Loss: 1.3499 | Val   Acc: 0.4800\n",
            "  Loss Gap:   -0.0431 | Acc  Gap:  0.0182\n",
            "  Loss Ratio: 0.9681 | Acc Ratio: 1.0379\n",
            "Epoch 8/20\n",
            "  Train Loss: 1.2640 | Train Acc: 0.5109\n",
            "  Val   Loss: 1.3064 | Val   Acc: 0.4932\n",
            "  Loss Gap:   -0.0423 | Acc  Gap:  0.0177\n",
            "  Loss Ratio: 0.9676 | Acc Ratio: 1.0359\n",
            "Epoch 9/20\n",
            "  Train Loss: 1.2191 | Train Acc: 0.5298\n",
            "  Val   Loss: 1.3005 | Val   Acc: 0.5082\n",
            "  Loss Gap:   -0.0815 | Acc  Gap:  0.0216\n",
            "  Loss Ratio: 0.9374 | Acc Ratio: 1.0425\n",
            "Epoch 10/20\n",
            "  Train Loss: 1.1832 | Train Acc: 0.5530\n",
            "  Val   Loss: 1.2587 | Val   Acc: 0.5319\n",
            "  Loss Gap:   -0.0754 | Acc  Gap:  0.0211\n",
            "  Loss Ratio: 0.9401 | Acc Ratio: 1.0397\n",
            "Epoch 11/20\n",
            "  Train Loss: 1.1371 | Train Acc: 0.5690\n",
            "  Val   Loss: 1.2419 | Val   Acc: 0.5319\n",
            "  Loss Gap:   -0.1048 | Acc  Gap:  0.0372\n",
            "  Loss Ratio: 0.9156 | Acc Ratio: 1.0699\n",
            "Epoch 12/20\n",
            "  Train Loss: 1.0947 | Train Acc: 0.5901\n",
            "  Val   Loss: 1.2261 | Val   Acc: 0.5448\n",
            "  Loss Gap:   -0.1314 | Acc  Gap:  0.0453\n",
            "  Loss Ratio: 0.8928 | Acc Ratio: 1.0832\n",
            "Epoch 13/20\n",
            "  Train Loss: 1.0486 | Train Acc: 0.6087\n",
            "  Val   Loss: 1.2627 | Val   Acc: 0.5315\n",
            "  Loss Gap:   -0.2140 | Acc  Gap:  0.0772\n",
            "  Loss Ratio: 0.8305 | Acc Ratio: 1.1453\n",
            "Epoch 14/20\n",
            "  Train Loss: 1.0139 | Train Acc: 0.6250\n",
            "  Val   Loss: 1.2610 | Val   Acc: 0.5361\n",
            "  Loss Gap:   -0.2471 | Acc  Gap:  0.0890\n",
            "  Loss Ratio: 0.8040 | Acc Ratio: 1.1660\n",
            "Epoch 15/20\n",
            "  Train Loss: 0.9871 | Train Acc: 0.6336\n",
            "  Val   Loss: 1.2257 | Val   Acc: 0.5514\n",
            "  Loss Gap:   -0.2386 | Acc  Gap:  0.0822\n",
            "  Loss Ratio: 0.8053 | Acc Ratio: 1.1491\n",
            "Epoch 16/20\n",
            "  Train Loss: 0.9378 | Train Acc: 0.6549\n",
            "  Val   Loss: 1.2479 | Val   Acc: 0.5423\n",
            "  Loss Gap:   -0.3100 | Acc  Gap:  0.1126\n",
            "  Loss Ratio: 0.7516 | Acc Ratio: 1.2077\n",
            "Epoch 17/20\n",
            "  Train Loss: 0.9113 | Train Acc: 0.6682\n",
            "  Val   Loss: 1.3144 | Val   Acc: 0.5131\n",
            "  Loss Gap:   -0.4031 | Acc  Gap:  0.1552\n",
            "  Loss Ratio: 0.6933 | Acc Ratio: 1.3024\n",
            "Epoch 18/20\n",
            "  Train Loss: 0.8767 | Train Acc: 0.6859\n",
            "  Val   Loss: 1.3201 | Val   Acc: 0.5280\n",
            "  Loss Gap:   -0.4434 | Acc  Gap:  0.1579\n",
            "  Loss Ratio: 0.6641 | Acc Ratio: 1.2990\n",
            "Epoch 19/20\n",
            "  Train Loss: 0.8566 | Train Acc: 0.6904\n",
            "  Val   Loss: 1.3191 | Val   Acc: 0.5408\n",
            "  Loss Gap:   -0.4625 | Acc  Gap:  0.1497\n",
            "  Loss Ratio: 0.6494 | Acc Ratio: 1.2768\n",
            "Epoch 20/20\n",
            "  Train Loss: 0.8333 | Train Acc: 0.7016\n",
            "  Val   Loss: 1.3398 | Val   Acc: 0.5361\n",
            "  Loss Gap:   -0.5065 | Acc  Gap:  0.1655\n",
            "  Loss Ratio: 0.6220 | Acc Ratio: 1.3088\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▄▃▃▃▄▄▄▄▄▅▅▆▆▆▇████</td></tr><tr><td>acc_ratio</td><td>▁▄▄▄▃▄▅▅▅▅▅▅▆▆▆▇████</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>loss_gap</td><td>█▇▇▇▇▆▆▆▆▆▅▅▄▄▄▃▂▂▁▁</td></tr><tr><td>loss_ratio</td><td>█▇▇▇▇▆▆▆▆▆▆▅▄▄▄▃▂▂▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▄▄▄▅▅▅▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▃▄▆▅▆▆▇▇▇█▇▇██▇▇█▇</td></tr><tr><td>val_loss</td><td>█▇▅▄▃▄▃▂▂▁▁▁▂▂▁▁▂▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.16552</td></tr><tr><td>acc_ratio</td><td>1.30878</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>loss_gap</td><td>-0.50648</td></tr><tr><td>loss_ratio</td><td>0.62197</td></tr><tr><td>train_accuracy</td><td>0.70157</td></tr><tr><td>train_loss</td><td>0.83333</td></tr><tr><td>val_accuracy</td><td>0.53605</td></tr><tr><td>val_loss</td><td>1.33981</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_LR_0.002475</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/ubek0iv3' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/ubek0iv3</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_233914-ubek0iv3/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Training with learning rate: 0.002977\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_234104-yr8jvdgf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/yr8jvdgf' target=\"_blank\">FER_BatchNorm_LR_0.002977</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/yr8jvdgf' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/yr8jvdgf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "  Train Loss: 1.8086 | Train Acc: 0.2626\n",
            "  Val   Loss: 1.7058 | Val   Acc: 0.2928\n",
            "  Loss Gap:   0.1028 | Acc  Gap:  -0.0302\n",
            "  Loss Ratio: 1.0603 | Acc Ratio: 0.8968\n",
            "Epoch 2/20\n",
            "  Train Loss: 1.6196 | Train Acc: 0.3565\n",
            "  Val   Loss: 1.4996 | Val   Acc: 0.4284\n",
            "  Loss Gap:   0.1200 | Acc  Gap:  -0.0720\n",
            "  Loss Ratio: 1.0800 | Acc Ratio: 0.8320\n",
            "Epoch 3/20\n",
            "  Train Loss: 1.4665 | Train Acc: 0.4332\n",
            "  Val   Loss: 1.3873 | Val   Acc: 0.4707\n",
            "  Loss Gap:   0.0791 | Acc  Gap:  -0.0375\n",
            "  Loss Ratio: 1.0570 | Acc Ratio: 0.9203\n",
            "Epoch 4/20\n",
            "  Train Loss: 1.3837 | Train Acc: 0.4675\n",
            "  Val   Loss: 1.4503 | Val   Acc: 0.4528\n",
            "  Loss Gap:   -0.0665 | Acc  Gap:  0.0146\n",
            "  Loss Ratio: 0.9541 | Acc Ratio: 1.0324\n",
            "Epoch 5/20\n",
            "  Train Loss: 1.3052 | Train Acc: 0.5015\n",
            "  Val   Loss: 1.2907 | Val   Acc: 0.5037\n",
            "  Loss Gap:   0.0146 | Acc  Gap:  -0.0021\n",
            "  Loss Ratio: 1.0113 | Acc Ratio: 0.9958\n",
            "Epoch 6/20\n",
            "  Train Loss: 1.2480 | Train Acc: 0.5266\n",
            "  Val   Loss: 1.2796 | Val   Acc: 0.5108\n",
            "  Loss Gap:   -0.0317 | Acc  Gap:  0.0158\n",
            "  Loss Ratio: 0.9753 | Acc Ratio: 1.0310\n",
            "Epoch 7/20\n",
            "  Train Loss: 1.1996 | Train Acc: 0.5453\n",
            "  Val   Loss: 1.2264 | Val   Acc: 0.5291\n",
            "  Loss Gap:   -0.0268 | Acc  Gap:  0.0162\n",
            "  Loss Ratio: 0.9782 | Acc Ratio: 1.0306\n",
            "Epoch 8/20\n",
            "  Train Loss: 1.1452 | Train Acc: 0.5662\n",
            "  Val   Loss: 1.2920 | Val   Acc: 0.5091\n",
            "  Loss Gap:   -0.1468 | Acc  Gap:  0.0572\n",
            "  Loss Ratio: 0.8864 | Acc Ratio: 1.1123\n",
            "Epoch 9/20\n",
            "  Train Loss: 1.0959 | Train Acc: 0.5897\n",
            "  Val   Loss: 1.2373 | Val   Acc: 0.5357\n",
            "  Loss Gap:   -0.1414 | Acc  Gap:  0.0540\n",
            "  Loss Ratio: 0.8857 | Acc Ratio: 1.1007\n",
            "Epoch 10/20\n",
            "  Train Loss: 1.0496 | Train Acc: 0.6038\n",
            "  Val   Loss: 1.2438 | Val   Acc: 0.5246\n",
            "  Loss Gap:   -0.1941 | Acc  Gap:  0.0793\n",
            "  Loss Ratio: 0.8439 | Acc Ratio: 1.1511\n",
            "Epoch 11/20\n",
            "  Train Loss: 1.0040 | Train Acc: 0.6222\n",
            "  Val   Loss: 1.1956 | Val   Acc: 0.5519\n",
            "  Loss Gap:   -0.1916 | Acc  Gap:  0.0703\n",
            "  Loss Ratio: 0.8397 | Acc Ratio: 1.1273\n",
            "Epoch 12/20\n",
            "  Train Loss: 0.9638 | Train Acc: 0.6401\n",
            "  Val   Loss: 1.2277 | Val   Acc: 0.5395\n",
            "  Loss Gap:   -0.2639 | Acc  Gap:  0.1006\n",
            "  Loss Ratio: 0.7851 | Acc Ratio: 1.1864\n",
            "Epoch 13/20\n",
            "  Train Loss: 0.9125 | Train Acc: 0.6581\n",
            "  Val   Loss: 1.1972 | Val   Acc: 0.5665\n",
            "  Loss Gap:   -0.2847 | Acc  Gap:  0.0916\n",
            "  Loss Ratio: 0.7622 | Acc Ratio: 1.1617\n",
            "Epoch 14/20\n",
            "  Train Loss: 0.8696 | Train Acc: 0.6739\n",
            "  Val   Loss: 1.2486 | Val   Acc: 0.5528\n",
            "  Loss Gap:   -0.3790 | Acc  Gap:  0.1211\n",
            "  Loss Ratio: 0.6964 | Acc Ratio: 1.2191\n",
            "Epoch 15/20\n",
            "  Train Loss: 0.8361 | Train Acc: 0.6872\n",
            "  Val   Loss: 1.2327 | Val   Acc: 0.5590\n",
            "  Loss Gap:   -0.3966 | Acc  Gap:  0.1282\n",
            "  Loss Ratio: 0.6783 | Acc Ratio: 1.2293\n",
            "Epoch 16/20\n",
            "  Train Loss: 0.8071 | Train Acc: 0.6978\n",
            "  Val   Loss: 1.2478 | Val   Acc: 0.5637\n",
            "  Loss Gap:   -0.4407 | Acc  Gap:  0.1341\n",
            "  Loss Ratio: 0.6468 | Acc Ratio: 1.2379\n",
            "Epoch 17/20\n",
            "  Train Loss: 0.7679 | Train Acc: 0.7107\n",
            "  Val   Loss: 1.2644 | Val   Acc: 0.5623\n",
            "  Loss Gap:   -0.4964 | Acc  Gap:  0.1483\n",
            "  Loss Ratio: 0.6074 | Acc Ratio: 1.2638\n",
            "Epoch 18/20\n",
            "  Train Loss: 0.7437 | Train Acc: 0.7203\n",
            "  Val   Loss: 1.3531 | Val   Acc: 0.5535\n",
            "  Loss Gap:   -0.6094 | Acc  Gap:  0.1669\n",
            "  Loss Ratio: 0.5496 | Acc Ratio: 1.3015\n",
            "Epoch 19/20\n",
            "  Train Loss: 0.7094 | Train Acc: 0.7328\n",
            "  Val   Loss: 1.3110 | Val   Acc: 0.5569\n",
            "  Loss Gap:   -0.6016 | Acc  Gap:  0.1758\n",
            "  Loss Ratio: 0.5411 | Acc Ratio: 1.3157\n",
            "Epoch 20/20\n",
            "  Train Loss: 0.6894 | Train Acc: 0.7421\n",
            "  Val   Loss: 1.3821 | Val   Acc: 0.5559\n",
            "  Loss Gap:   -0.6927 | Acc  Gap:  0.1862\n",
            "  Loss Ratio: 0.4988 | Acc Ratio: 1.3350\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▂▁▂▃▃▃▃▅▄▅▅▆▅▆▆▇▇▇██</td></tr><tr><td>acc_ratio</td><td>▂▁▂▄▃▄▄▅▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>loss_gap</td><td>███▆▇▇▇▆▆▅▅▅▅▄▄▃▃▂▂▁</td></tr><tr><td>loss_ratio</td><td>███▆▇▇▇▆▆▅▅▄▄▃▃▃▂▂▂▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▄▄▅▅▅▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▅▆▇▇▇▇▇█▇████████</td></tr><tr><td>val_loss</td><td>█▅▄▄▂▂▁▂▂▂▁▁▁▂▂▂▂▃▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.1862</td></tr><tr><td>acc_ratio</td><td>1.33496</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>loss_gap</td><td>-0.69269</td></tr><tr><td>loss_ratio</td><td>0.49882</td></tr><tr><td>train_accuracy</td><td>0.74211</td></tr><tr><td>train_loss</td><td>0.68943</td></tr><tr><td>val_accuracy</td><td>0.5559</td></tr><tr><td>val_loss</td><td>1.38212</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_LR_0.002977</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/yr8jvdgf' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/yr8jvdgf</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_234104-yr8jvdgf/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔧 Training with learning rate: 0.003733\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250603_234213-ch5yt6m9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/ch5yt6m9' target=\"_blank\">FER_BatchNorm_LR_0.003733</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/ch5yt6m9' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/ch5yt6m9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "  Train Loss: 1.8381 | Train Acc: 0.2519\n",
            "  Val   Loss: 1.7025 | Val   Acc: 0.3049\n",
            "  Loss Gap:   0.1356 | Acc  Gap:  -0.0531\n",
            "  Loss Ratio: 1.0796 | Acc Ratio: 0.8260\n",
            "Epoch 2/20\n",
            "  Train Loss: 1.6527 | Train Acc: 0.3253\n",
            "  Val   Loss: 1.5613 | Val   Acc: 0.3713\n",
            "  Loss Gap:   0.0914 | Acc  Gap:  -0.0460\n",
            "  Loss Ratio: 1.0585 | Acc Ratio: 0.8761\n",
            "Epoch 3/20\n",
            "  Train Loss: 1.5610 | Train Acc: 0.3680\n",
            "  Val   Loss: 1.5615 | Val   Acc: 0.3753\n",
            "  Loss Gap:   -0.0005 | Acc  Gap:  -0.0073\n",
            "  Loss Ratio: 0.9997 | Acc Ratio: 0.9806\n",
            "Epoch 4/20\n",
            "  Train Loss: 1.5091 | Train Acc: 0.3912\n",
            "  Val   Loss: 1.4835 | Val   Acc: 0.4117\n",
            "  Loss Gap:   0.0256 | Acc  Gap:  -0.0205\n",
            "  Loss Ratio: 1.0173 | Acc Ratio: 0.9502\n",
            "Epoch 5/20\n",
            "  Train Loss: 1.4622 | Train Acc: 0.4104\n",
            "  Val   Loss: 1.4333 | Val   Acc: 0.4222\n",
            "  Loss Gap:   0.0289 | Acc  Gap:  -0.0117\n",
            "  Loss Ratio: 1.0202 | Acc Ratio: 0.9722\n",
            "Epoch 6/20\n",
            "  Train Loss: 1.4068 | Train Acc: 0.4345\n",
            "  Val   Loss: 1.3562 | Val   Acc: 0.4439\n",
            "  Loss Gap:   0.0506 | Acc  Gap:  -0.0094\n",
            "  Loss Ratio: 1.0373 | Acc Ratio: 0.9789\n",
            "Epoch 7/20\n",
            "  Train Loss: 1.3712 | Train Acc: 0.4480\n",
            "  Val   Loss: 1.3611 | Val   Acc: 0.4655\n",
            "  Loss Gap:   0.0102 | Acc  Gap:  -0.0175\n",
            "  Loss Ratio: 1.0075 | Acc Ratio: 0.9624\n",
            "Epoch 8/20\n",
            "  Train Loss: 1.3265 | Train Acc: 0.4706\n",
            "  Val   Loss: 1.3615 | Val   Acc: 0.4638\n",
            "  Loss Gap:   -0.0351 | Acc  Gap:  0.0068\n",
            "  Loss Ratio: 0.9742 | Acc Ratio: 1.0147\n",
            "Epoch 9/20\n",
            "  Train Loss: 1.2916 | Train Acc: 0.4874\n",
            "  Val   Loss: 1.2883 | Val   Acc: 0.4981\n",
            "  Loss Gap:   0.0034 | Acc  Gap:  -0.0107\n",
            "  Loss Ratio: 1.0026 | Acc Ratio: 0.9785\n",
            "Epoch 10/20\n",
            "  Train Loss: 1.2553 | Train Acc: 0.4978\n",
            "  Val   Loss: 1.3917 | Val   Acc: 0.4669\n",
            "  Loss Gap:   -0.1363 | Acc  Gap:  0.0309\n",
            "  Loss Ratio: 0.9020 | Acc Ratio: 1.0663\n",
            "Epoch 11/20\n",
            "  Train Loss: 1.2220 | Train Acc: 0.5134\n",
            "  Val   Loss: 1.2759 | Val   Acc: 0.5063\n",
            "  Loss Gap:   -0.0538 | Acc  Gap:  0.0071\n",
            "  Loss Ratio: 0.9578 | Acc Ratio: 1.0141\n",
            "Epoch 12/20\n",
            "  Train Loss: 1.1797 | Train Acc: 0.5305\n",
            "  Val   Loss: 1.3136 | Val   Acc: 0.5005\n",
            "  Loss Gap:   -0.1339 | Acc  Gap:  0.0300\n",
            "  Loss Ratio: 0.8980 | Acc Ratio: 1.0600\n",
            "Epoch 13/20\n",
            "  Train Loss: 1.1465 | Train Acc: 0.5461\n",
            "  Val   Loss: 1.3019 | Val   Acc: 0.4951\n",
            "  Loss Gap:   -0.1554 | Acc  Gap:  0.0510\n",
            "  Loss Ratio: 0.8806 | Acc Ratio: 1.1029\n",
            "Epoch 14/20\n",
            "  Train Loss: 1.1058 | Train Acc: 0.5593\n",
            "  Val   Loss: 1.2774 | Val   Acc: 0.5063\n",
            "  Loss Gap:   -0.1716 | Acc  Gap:  0.0530\n",
            "  Loss Ratio: 0.8657 | Acc Ratio: 1.1047\n",
            "Epoch 15/20\n",
            "  Train Loss: 1.0659 | Train Acc: 0.5780\n",
            "  Val   Loss: 1.2817 | Val   Acc: 0.5104\n",
            "  Loss Gap:   -0.2158 | Acc  Gap:  0.0675\n",
            "  Loss Ratio: 0.8316 | Acc Ratio: 1.1323\n",
            "Epoch 16/20\n",
            "  Train Loss: 1.0399 | Train Acc: 0.5835\n",
            "  Val   Loss: 1.2927 | Val   Acc: 0.5152\n",
            "  Loss Gap:   -0.2528 | Acc  Gap:  0.0684\n",
            "  Loss Ratio: 0.8044 | Acc Ratio: 1.1327\n",
            "Epoch 17/20\n",
            "  Train Loss: 1.0078 | Train Acc: 0.6039\n",
            "  Val   Loss: 1.3212 | Val   Acc: 0.5044\n",
            "  Loss Gap:   -0.3134 | Acc  Gap:  0.0995\n",
            "  Loss Ratio: 0.7628 | Acc Ratio: 1.1973\n",
            "Epoch 18/20\n",
            "  Train Loss: 0.9760 | Train Acc: 0.6117\n",
            "  Val   Loss: 1.3414 | Val   Acc: 0.4983\n",
            "  Loss Gap:   -0.3653 | Acc  Gap:  0.1135\n",
            "  Loss Ratio: 0.7276 | Acc Ratio: 1.2278\n",
            "Epoch 19/20\n",
            "  Train Loss: 0.9388 | Train Acc: 0.6272\n",
            "  Val   Loss: 1.3176 | Val   Acc: 0.5178\n",
            "  Loss Gap:   -0.3788 | Acc  Gap:  0.1094\n",
            "  Loss Ratio: 0.7125 | Acc Ratio: 1.2114\n",
            "Epoch 20/20\n",
            "  Train Loss: 0.9180 | Train Acc: 0.6338\n",
            "  Val   Loss: 1.3227 | Val   Acc: 0.5146\n",
            "  Loss Gap:   -0.4047 | Acc  Gap:  0.1191\n",
            "  Loss Ratio: 0.6941 | Acc Ratio: 1.2315\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▁▃▂▃▃▂▃▃▄▃▄▅▅▆▆▇███</td></tr><tr><td>acc_ratio</td><td>▁▂▄▃▄▄▃▄▄▅▄▅▆▆▆▆▇███</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>loss_gap</td><td>█▇▆▇▇▇▆▆▆▄▆▅▄▄▃▃▂▂▁▁</td></tr><tr><td>loss_ratio</td><td>██▇▇▇▇▇▆▇▅▆▅▄▄▃▃▂▂▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▃▅▅▆▆▆▇▆█▇▇████▇██</td></tr><tr><td>val_loss</td><td>█▆▆▄▄▂▂▂▁▃▁▂▁▁▁▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.11915</td></tr><tr><td>acc_ratio</td><td>1.23153</td></tr><tr><td>epoch</td><td>20</td></tr><tr><td>loss_gap</td><td>-0.40467</td></tr><tr><td>loss_ratio</td><td>0.69406</td></tr><tr><td>train_accuracy</td><td>0.63378</td></tr><tr><td>train_loss</td><td>0.91804</td></tr><tr><td>val_accuracy</td><td>0.51463</td></tr><tr><td>val_loss</td><td>1.32271</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_LR_0.003733</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/ch5yt6m9' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/ch5yt6m9</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250603_234213-ch5yt6m9/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📈 Best Learning Rate:\n",
            "  ➤ LR: 0.001571 with Val Accuracy: 0.5883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 8 - Layernorm"
      ],
      "metadata": {
        "id": "63NCoHdsNRYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8Kk_9iTTx6SF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "\n",
        "class FERCNN_With_LayerNorm(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_rate=0.3):\n",
        "        super(FERCNN_With_LayerNorm, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.ln1 = nn.LayerNorm([32, 48, 48])\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.ln2 = nn.LayerNorm([64, 24, 24])\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.ln3 = nn.LayerNorm([128, 12, 12])\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Calculate flattened size dynamically\n",
        "        dummy_input = torch.zeros(1, 1, 48, 48)\n",
        "        with torch.no_grad():\n",
        "            x = self.pool(torch.relu(self.ln1(self.conv1(dummy_input))))\n",
        "            x = self.pool(torch.relu(self.ln2(self.conv2(x))))\n",
        "            x = self.pool(torch.relu(self.ln3(self.conv3(x))))\n",
        "            flattened_size = x.view(1, -1).shape[1]\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.ln1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.ln2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.ln3(self.conv3(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.dropout(torch.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "N087IIkhx5sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_layernorm(epochs=15, lr=0.0015, dropout_rate=0.3):\n",
        "    config = {\n",
        "        \"dropout_rate\": dropout_rate,\n",
        "        \"pool_kernel\": (2, 2),\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_size\": 64,\n",
        "        \"lr\": lr,\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"normalization\": \"LayerNorm\",\n",
        "        \"model\": \"FERCNN_With_LayerNorm\"\n",
        "    }\n",
        "\n",
        "    run = wandb.init(\n",
        "        project=\"ML_4\",\n",
        "        entity=\"nkhar21-student\",\n",
        "        name=f\"FER_LayerNorm_Dropout_{dropout_rate}_LR_{lr}_Epochs_{epochs}\",\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    batch_size = config[\"batch_size\"]\n",
        "    model = FERCNN_With_LayerNorm(num_classes=7, dropout_rate=dropout_rate).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    wandb.watch(model)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = evaluate_model(\n",
        "            x_train, y_train, model, loss_fn, device,\n",
        "            train_mode=True, optimizer=optimizer, batch_size=batch_size\n",
        "        )\n",
        "        val_loss, val_acc = evaluate_model(\n",
        "            x_val, y_val, model, loss_fn, device,\n",
        "            train_mode=False, optimizer=None, batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        # Overfitting metrics\n",
        "        loss_gap = train_loss - val_loss\n",
        "        acc_gap = train_acc - val_acc\n",
        "        loss_ratio = train_loss / val_loss if val_loss != 0 else float(\"inf\")\n",
        "        acc_ratio = train_acc / val_acc if val_acc != 0 else float(\"inf\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "        print(f\"  Loss Gap:   {loss_gap:.4f} | Acc  Gap:  {acc_gap:.4f}\")\n",
        "        print(f\"  Loss Ratio: {loss_ratio:.4f} | Acc Ratio: {acc_ratio:.4f}\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"loss_gap\": loss_gap,\n",
        "            \"acc_gap\": acc_gap,\n",
        "            \"loss_ratio\": loss_ratio,\n",
        "            \"acc_ratio\": acc_ratio,\n",
        "            # log config again for visibility\n",
        "            \"dropout_rate\": dropout_rate,\n",
        "            \"lr\": lr,\n",
        "            \"epochs\": epochs,\n",
        "            \"optimizer\": \"Adam\",\n",
        "            \"normalization\": \"LayerNorm\"\n",
        "        })\n",
        "\n",
        "    run.finish()\n"
      ],
      "metadata": {
        "id": "twSIFFSwzLuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_with_layernorm(epochs=15, lr=0.0015, dropout_rate=0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jsgdKTSozOL2",
        "outputId": "9c3d19e1-f678-4dd7-ab67-a580ec49f9a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_115437-yjd6ft29</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/yjd6ft29' target=\"_blank\">FER_LayerNorm_Dropout_0.3_LR_0.0015_Epochs_15</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/yjd6ft29' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/yjd6ft29</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "  Train Loss: 1.8160 | Train Acc: 0.2464\n",
            "  Val   Loss: 1.7183 | Val   Acc: 0.2767\n",
            "  Loss Gap:   0.0976 | Acc  Gap:  -0.0303\n",
            "  Loss Ratio: 1.0568 | Acc Ratio: 0.8905\n",
            "Epoch 2/15\n",
            "  Train Loss: 1.6876 | Train Acc: 0.3104\n",
            "  Val   Loss: 1.6350 | Val   Acc: 0.3502\n",
            "  Loss Gap:   0.0526 | Acc  Gap:  -0.0399\n",
            "  Loss Ratio: 1.0321 | Acc Ratio: 0.8862\n",
            "Epoch 3/15\n",
            "  Train Loss: 1.6317 | Train Acc: 0.3419\n",
            "  Val   Loss: 1.5726 | Val   Acc: 0.3802\n",
            "  Loss Gap:   0.0591 | Acc  Gap:  -0.0383\n",
            "  Loss Ratio: 1.0376 | Acc Ratio: 0.8994\n",
            "Epoch 4/15\n",
            "  Train Loss: 1.5782 | Train Acc: 0.3665\n",
            "  Val   Loss: 1.5413 | Val   Acc: 0.3878\n",
            "  Loss Gap:   0.0368 | Acc  Gap:  -0.0214\n",
            "  Loss Ratio: 1.0239 | Acc Ratio: 0.9449\n",
            "Epoch 5/15\n",
            "  Train Loss: 1.5373 | Train Acc: 0.3839\n",
            "  Val   Loss: 1.5398 | Val   Acc: 0.3966\n",
            "  Loss Gap:   -0.0025 | Acc  Gap:  -0.0127\n",
            "  Loss Ratio: 0.9984 | Acc Ratio: 0.9681\n",
            "Epoch 6/15\n",
            "  Train Loss: 1.5117 | Train Acc: 0.3914\n",
            "  Val   Loss: 1.5255 | Val   Acc: 0.3992\n",
            "  Loss Gap:   -0.0138 | Acc  Gap:  -0.0077\n",
            "  Loss Ratio: 0.9910 | Acc Ratio: 0.9806\n",
            "Epoch 7/15\n",
            "  Train Loss: 1.4752 | Train Acc: 0.4098\n",
            "  Val   Loss: 1.4839 | Val   Acc: 0.4136\n",
            "  Loss Gap:   -0.0087 | Acc  Gap:  -0.0038\n",
            "  Loss Ratio: 0.9941 | Acc Ratio: 0.9908\n",
            "Epoch 8/15\n",
            "  Train Loss: 1.4475 | Train Acc: 0.4153\n",
            "  Val   Loss: 1.5245 | Val   Acc: 0.3931\n",
            "  Loss Gap:   -0.0770 | Acc  Gap:  0.0223\n",
            "  Loss Ratio: 0.9495 | Acc Ratio: 1.0566\n",
            "Epoch 9/15\n",
            "  Train Loss: 1.4283 | Train Acc: 0.4192\n",
            "  Val   Loss: 1.4633 | Val   Acc: 0.4105\n",
            "  Loss Gap:   -0.0350 | Acc  Gap:  0.0087\n",
            "  Loss Ratio: 0.9761 | Acc Ratio: 1.0212\n",
            "Epoch 10/15\n",
            "  Train Loss: 1.3988 | Train Acc: 0.4304\n",
            "  Val   Loss: 1.4639 | Val   Acc: 0.4046\n",
            "  Loss Gap:   -0.0651 | Acc  Gap:  0.0258\n",
            "  Loss Ratio: 0.9555 | Acc Ratio: 1.0639\n",
            "Epoch 11/15\n",
            "  Train Loss: 1.3854 | Train Acc: 0.4362\n",
            "  Val   Loss: 1.4533 | Val   Acc: 0.4075\n",
            "  Loss Gap:   -0.0678 | Acc  Gap:  0.0287\n",
            "  Loss Ratio: 0.9533 | Acc Ratio: 1.0703\n",
            "Epoch 12/15\n",
            "  Train Loss: 1.3472 | Train Acc: 0.4524\n",
            "  Val   Loss: 1.5221 | Val   Acc: 0.3906\n",
            "  Loss Gap:   -0.1749 | Acc  Gap:  0.0618\n",
            "  Loss Ratio: 0.8851 | Acc Ratio: 1.1582\n",
            "Epoch 13/15\n",
            "  Train Loss: 1.3344 | Train Acc: 0.4577\n",
            "  Val   Loss: 1.5129 | Val   Acc: 0.3875\n",
            "  Loss Gap:   -0.1785 | Acc  Gap:  0.0702\n",
            "  Loss Ratio: 0.8820 | Acc Ratio: 1.1813\n",
            "Epoch 14/15\n",
            "  Train Loss: 1.3038 | Train Acc: 0.4685\n",
            "  Val   Loss: 1.4475 | Val   Acc: 0.4194\n",
            "  Loss Gap:   -0.1437 | Acc  Gap:  0.0491\n",
            "  Loss Ratio: 0.9007 | Acc Ratio: 1.1171\n",
            "Epoch 15/15\n",
            "  Train Loss: 1.2869 | Train Acc: 0.4743\n",
            "  Val   Loss: 1.4252 | Val   Acc: 0.4265\n",
            "  Loss Gap:   -0.1383 | Acc  Gap:  0.0478\n",
            "  Loss Ratio: 0.9030 | Acc Ratio: 1.1121\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▂▁▁▂▃▃▃▅▄▅▅▇█▇▇</td></tr><tr><td>acc_ratio</td><td>▁▁▁▂▃▃▃▅▄▅▅▇█▆▆</td></tr><tr><td>dropout_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>epochs</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_gap</td><td>█▇▇▆▅▅▅▄▅▄▄▁▁▂▂</td></tr><tr><td>loss_ratio</td><td>█▇▇▇▆▅▅▄▅▄▄▁▁▂▂</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>train_loss</td><td>█▆▆▅▄▄▃▃▃▂▂▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▆▆▇▇▇▆▇▇▇▆▆██</td></tr><tr><td>val_loss</td><td>█▆▅▄▄▃▂▃▂▂▂▃▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.04783</td></tr><tr><td>acc_ratio</td><td>1.11213</td></tr><tr><td>dropout_rate</td><td>0.3</td></tr><tr><td>epoch</td><td>15</td></tr><tr><td>epochs</td><td>15</td></tr><tr><td>loss_gap</td><td>-0.13828</td></tr><tr><td>loss_ratio</td><td>0.90298</td></tr><tr><td>lr</td><td>0.0015</td></tr><tr><td>normalization</td><td>LayerNorm</td></tr><tr><td>optimizer</td><td>Adam</td></tr><tr><td>train_accuracy</td><td>0.47433</td></tr><tr><td>train_loss</td><td>1.28692</td></tr><tr><td>val_accuracy</td><td>0.42651</td></tr><tr><td>val_loss</td><td>1.42519</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_LayerNorm_Dropout_0.3_LR_0.0015_Epochs_15</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/yjd6ft29' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/yjd6ft29</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_115437-yjd6ft29/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 8 - Groupnorm"
      ],
      "metadata": {
        "id": "aydKuSF40BJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FERCNN_With_GroupNorm(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_rate=0.3):\n",
        "        super(FERCNN_With_GroupNorm, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.gn1 = nn.GroupNorm(num_groups=4, num_channels=32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.gn2 = nn.GroupNorm(num_groups=8, num_channels=64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.gn3 = nn.GroupNorm(num_groups=16, num_channels=128)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Dynamically determine the input size to the first FC layer\n",
        "        dummy_input = torch.zeros(1, 1, 48, 48)\n",
        "        with torch.no_grad():\n",
        "            x = self.pool(torch.relu(self.gn1(self.conv1(dummy_input))))\n",
        "            x = self.pool(torch.relu(self.gn2(self.conv2(x))))\n",
        "            x = self.pool(torch.relu(self.gn3(self.conv3(x))))\n",
        "            flattened_size = x.view(1, -1).shape[1]\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.gn1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.gn2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.gn3(self.conv3(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.dropout(torch.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Z9qcR88y0AvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_groupnorm(epochs=15, lr=0.0015, dropout_rate=0.3):\n",
        "    config = {\n",
        "        \"dropout_rate\": dropout_rate,\n",
        "        \"pool_kernel\": (2, 2),\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_size\": 64,\n",
        "        \"lr\": lr,\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"normalization\": \"GroupNorm\",\n",
        "        \"model\": \"FERCNN_With_GroupNorm\"\n",
        "    }\n",
        "\n",
        "    run = wandb.init(\n",
        "        project=\"ML_4\",\n",
        "        entity=\"nkhar21-student\",\n",
        "        name=f\"FER_GroupNorm_Dropout_{dropout_rate}_LR_{lr}_Epochs_{epochs}\",\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    batch_size = config[\"batch_size\"]\n",
        "    model = FERCNN_With_GroupNorm(num_classes=7, dropout_rate=dropout_rate).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    wandb.watch(model)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = evaluate_model(\n",
        "            x_train, y_train, model, loss_fn, device,\n",
        "            train_mode=True, optimizer=optimizer, batch_size=batch_size\n",
        "        )\n",
        "        val_loss, val_acc = evaluate_model(\n",
        "            x_val, y_val, model, loss_fn, device,\n",
        "            train_mode=False, optimizer=None, batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        # Overfitting metrics\n",
        "        loss_gap = train_loss - val_loss\n",
        "        acc_gap = train_acc - val_acc\n",
        "        loss_ratio = train_loss / val_loss if val_loss != 0 else float(\"inf\")\n",
        "        acc_ratio = train_acc / val_acc if val_acc != 0 else float(\"inf\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "        print(f\"  Loss Gap:   {loss_gap:.4f} | Acc  Gap:  {acc_gap:.4f}\")\n",
        "        print(f\"  Loss Ratio: {loss_ratio:.4f} | Acc Ratio: {acc_ratio:.4f}\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"loss_gap\": loss_gap,\n",
        "            \"acc_gap\": acc_gap,\n",
        "            \"loss_ratio\": loss_ratio,\n",
        "            \"acc_ratio\": acc_ratio,\n",
        "            \"dropout_rate\": dropout_rate,\n",
        "            \"lr\": lr,\n",
        "            \"epochs\": epochs,\n",
        "            \"optimizer\": \"Adam\",\n",
        "            \"normalization\": \"GroupNorm\"\n",
        "        })\n",
        "\n",
        "    run.finish()\n"
      ],
      "metadata": {
        "id": "bgKuxbn-0FFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_with_groupnorm()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CPyo8CNE0LM_",
        "outputId": "dffcf7a8-f67f-49b0-fb2e-b939554aee8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_115923-clxjij6h</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/clxjij6h' target=\"_blank\">FER_GroupNorm_Dropout_0.3_LR_0.0015_Epochs_15</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/clxjij6h' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/clxjij6h</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "  Train Loss: 1.8272 | Train Acc: 0.2434\n",
            "  Val   Loss: 1.8007 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0264 | Acc  Gap:  -0.0079\n",
            "  Loss Ratio: 1.0147 | Acc Ratio: 0.9685\n",
            "Epoch 2/15\n",
            "  Train Loss: 1.7662 | Train Acc: 0.2619\n",
            "  Val   Loss: 1.6828 | Val   Acc: 0.3159\n",
            "  Loss Gap:   0.0834 | Acc  Gap:  -0.0540\n",
            "  Loss Ratio: 1.0496 | Acc Ratio: 0.8291\n",
            "Epoch 3/15\n",
            "  Train Loss: 1.6484 | Train Acc: 0.3330\n",
            "  Val   Loss: 1.5785 | Val   Acc: 0.3753\n",
            "  Loss Gap:   0.0699 | Acc  Gap:  -0.0423\n",
            "  Loss Ratio: 1.0443 | Acc Ratio: 0.8873\n",
            "Epoch 4/15\n",
            "  Train Loss: 1.5825 | Train Acc: 0.3666\n",
            "  Val   Loss: 1.5047 | Val   Acc: 0.4073\n",
            "  Loss Gap:   0.0778 | Acc  Gap:  -0.0408\n",
            "  Loss Ratio: 1.0517 | Acc Ratio: 0.8999\n",
            "Epoch 5/15\n",
            "  Train Loss: 1.5330 | Train Acc: 0.3970\n",
            "  Val   Loss: 1.4746 | Val   Acc: 0.4235\n",
            "  Loss Gap:   0.0585 | Acc  Gap:  -0.0265\n",
            "  Loss Ratio: 1.0397 | Acc Ratio: 0.9373\n",
            "Epoch 6/15\n",
            "  Train Loss: 1.4770 | Train Acc: 0.4162\n",
            "  Val   Loss: 1.4173 | Val   Acc: 0.4568\n",
            "  Loss Gap:   0.0597 | Acc  Gap:  -0.0406\n",
            "  Loss Ratio: 1.0421 | Acc Ratio: 0.9110\n",
            "Epoch 7/15\n",
            "  Train Loss: 1.4367 | Train Acc: 0.4330\n",
            "  Val   Loss: 1.3998 | Val   Acc: 0.4697\n",
            "  Loss Gap:   0.0369 | Acc  Gap:  -0.0367\n",
            "  Loss Ratio: 1.0263 | Acc Ratio: 0.9218\n",
            "Epoch 8/15\n",
            "  Train Loss: 1.3997 | Train Acc: 0.4527\n",
            "  Val   Loss: 1.3388 | Val   Acc: 0.4862\n",
            "  Loss Gap:   0.0609 | Acc  Gap:  -0.0335\n",
            "  Loss Ratio: 1.0455 | Acc Ratio: 0.9310\n",
            "Epoch 9/15\n",
            "  Train Loss: 1.3651 | Train Acc: 0.4649\n",
            "  Val   Loss: 1.3591 | Val   Acc: 0.5019\n",
            "  Loss Gap:   0.0060 | Acc  Gap:  -0.0370\n",
            "  Loss Ratio: 1.0044 | Acc Ratio: 0.9263\n",
            "Epoch 10/15\n",
            "  Train Loss: 1.3228 | Train Acc: 0.4894\n",
            "  Val   Loss: 1.2854 | Val   Acc: 0.5056\n",
            "  Loss Gap:   0.0374 | Acc  Gap:  -0.0161\n",
            "  Loss Ratio: 1.0291 | Acc Ratio: 0.9681\n",
            "Epoch 11/15\n",
            "  Train Loss: 1.2881 | Train Acc: 0.5024\n",
            "  Val   Loss: 1.2639 | Val   Acc: 0.5270\n",
            "  Loss Gap:   0.0242 | Acc  Gap:  -0.0246\n",
            "  Loss Ratio: 1.0192 | Acc Ratio: 0.9534\n",
            "Epoch 12/15\n",
            "  Train Loss: 1.2382 | Train Acc: 0.5230\n",
            "  Val   Loss: 1.2747 | Val   Acc: 0.5179\n",
            "  Loss Gap:   -0.0365 | Acc  Gap:  0.0050\n",
            "  Loss Ratio: 0.9714 | Acc Ratio: 1.0097\n",
            "Epoch 13/15\n",
            "  Train Loss: 1.2020 | Train Acc: 0.5390\n",
            "  Val   Loss: 1.2867 | Val   Acc: 0.5221\n",
            "  Loss Gap:   -0.0848 | Acc  Gap:  0.0169\n",
            "  Loss Ratio: 0.9341 | Acc Ratio: 1.0324\n",
            "Epoch 14/15\n",
            "  Train Loss: 1.1701 | Train Acc: 0.5491\n",
            "  Val   Loss: 1.2571 | Val   Acc: 0.5387\n",
            "  Loss Gap:   -0.0871 | Acc  Gap:  0.0104\n",
            "  Loss Ratio: 0.9307 | Acc Ratio: 1.0194\n",
            "Epoch 15/15\n",
            "  Train Loss: 1.1315 | Train Acc: 0.5643\n",
            "  Val   Loss: 1.2389 | Val   Acc: 0.5246\n",
            "  Loss Gap:   -0.1074 | Acc  Gap:  0.0397\n",
            "  Loss Ratio: 0.9133 | Acc Ratio: 1.0757\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▄▁▂▂▃▂▂▃▂▄▃▅▆▆█</td></tr><tr><td>acc_ratio</td><td>▅▁▃▃▄▃▄▄▄▅▅▆▇▆█</td></tr><tr><td>dropout_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>epochs</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_gap</td><td>▆███▇▇▆▇▅▆▆▄▂▂▁</td></tr><tr><td>loss_ratio</td><td>▆███▇█▇█▆▇▆▄▂▂▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▁▃▄▄▅▅▆▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▄▄▄▃▃▃▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▅▅▆▆▇▇▇█▇███</td></tr><tr><td>val_loss</td><td>█▇▅▄▄▃▃▂▂▂▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.03973</td></tr><tr><td>acc_ratio</td><td>1.07574</td></tr><tr><td>dropout_rate</td><td>0.3</td></tr><tr><td>epoch</td><td>15</td></tr><tr><td>epochs</td><td>15</td></tr><tr><td>loss_gap</td><td>-0.10744</td></tr><tr><td>loss_ratio</td><td>0.91328</td></tr><tr><td>lr</td><td>0.0015</td></tr><tr><td>normalization</td><td>GroupNorm</td></tr><tr><td>optimizer</td><td>Adam</td></tr><tr><td>train_accuracy</td><td>0.56429</td></tr><tr><td>train_loss</td><td>1.13148</td></tr><tr><td>val_accuracy</td><td>0.52456</td></tr><tr><td>val_loss</td><td>1.23892</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_GroupNorm_Dropout_0.3_LR_0.0015_Epochs_15</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/clxjij6h' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/clxjij6h</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_115923-clxjij6h/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_with_groupnorm(epochs=30, lr=0.0012, dropout_rate=0.35)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ssClov3Z1Bdq",
        "outputId": "a467d523-536c-487f-c7db-1cfe34004ca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_120055-kf46eqbq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/kf46eqbq' target=\"_blank\">FER_GroupNorm_Dropout_0.35_LR_0.0012_Epochs_30</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/kf46eqbq' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/kf46eqbq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.8327 | Train Acc: 0.2355\n",
            "  Val   Loss: 1.8081 | Val   Acc: 0.2513\n",
            "  Loss Gap:   0.0246 | Acc  Gap:  -0.0158\n",
            "  Loss Ratio: 1.0136 | Acc Ratio: 0.9372\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.8107 | Train Acc: 0.2490\n",
            "  Val   Loss: 1.7773 | Val   Acc: 0.2546\n",
            "  Loss Gap:   0.0335 | Acc  Gap:  -0.0056\n",
            "  Loss Ratio: 1.0188 | Acc Ratio: 0.9778\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.7293 | Train Acc: 0.2809\n",
            "  Val   Loss: 1.6702 | Val   Acc: 0.3269\n",
            "  Loss Gap:   0.0591 | Acc  Gap:  -0.0460\n",
            "  Loss Ratio: 1.0354 | Acc Ratio: 0.8593\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.6563 | Train Acc: 0.3159\n",
            "  Val   Loss: 1.5828 | Val   Acc: 0.3650\n",
            "  Loss Gap:   0.0735 | Acc  Gap:  -0.0491\n",
            "  Loss Ratio: 1.0464 | Acc Ratio: 0.8655\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.6005 | Train Acc: 0.3536\n",
            "  Val   Loss: 1.5465 | Val   Acc: 0.3821\n",
            "  Loss Gap:   0.0540 | Acc  Gap:  -0.0285\n",
            "  Loss Ratio: 1.0349 | Acc Ratio: 0.9253\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.5483 | Train Acc: 0.3861\n",
            "  Val   Loss: 1.5098 | Val   Acc: 0.4188\n",
            "  Loss Gap:   0.0385 | Acc  Gap:  -0.0328\n",
            "  Loss Ratio: 1.0255 | Acc Ratio: 0.9218\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.4997 | Train Acc: 0.4044\n",
            "  Val   Loss: 1.4698 | Val   Acc: 0.4431\n",
            "  Loss Gap:   0.0298 | Acc  Gap:  -0.0386\n",
            "  Loss Ratio: 1.0203 | Acc Ratio: 0.9129\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.4704 | Train Acc: 0.4212\n",
            "  Val   Loss: 1.4695 | Val   Acc: 0.4525\n",
            "  Loss Gap:   0.0009 | Acc  Gap:  -0.0312\n",
            "  Loss Ratio: 1.0006 | Acc Ratio: 0.9309\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.4391 | Train Acc: 0.4358\n",
            "  Val   Loss: 1.3922 | Val   Acc: 0.4579\n",
            "  Loss Gap:   0.0469 | Acc  Gap:  -0.0221\n",
            "  Loss Ratio: 1.0337 | Acc Ratio: 0.9518\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.4050 | Train Acc: 0.4484\n",
            "  Val   Loss: 1.3659 | Val   Acc: 0.4831\n",
            "  Loss Gap:   0.0391 | Acc  Gap:  -0.0347\n",
            "  Loss Ratio: 1.0286 | Acc Ratio: 0.9282\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.3697 | Train Acc: 0.4646\n",
            "  Val   Loss: 1.3628 | Val   Acc: 0.5009\n",
            "  Loss Gap:   0.0069 | Acc  Gap:  -0.0363\n",
            "  Loss Ratio: 1.0051 | Acc Ratio: 0.9275\n",
            "Epoch 12/30\n",
            "  Train Loss: 1.3364 | Train Acc: 0.4793\n",
            "  Val   Loss: 1.3181 | Val   Acc: 0.5028\n",
            "  Loss Gap:   0.0182 | Acc  Gap:  -0.0235\n",
            "  Loss Ratio: 1.0138 | Acc Ratio: 0.9532\n",
            "Epoch 13/30\n",
            "  Train Loss: 1.2978 | Train Acc: 0.4968\n",
            "  Val   Loss: 1.3325 | Val   Acc: 0.4960\n",
            "  Loss Gap:   -0.0346 | Acc  Gap:  0.0008\n",
            "  Loss Ratio: 0.9740 | Acc Ratio: 1.0017\n",
            "Epoch 14/30\n",
            "  Train Loss: 1.2711 | Train Acc: 0.5066\n",
            "  Val   Loss: 1.2729 | Val   Acc: 0.5084\n",
            "  Loss Gap:   -0.0018 | Acc  Gap:  -0.0018\n",
            "  Loss Ratio: 0.9986 | Acc Ratio: 0.9965\n",
            "Epoch 15/30\n",
            "  Train Loss: 1.2338 | Train Acc: 0.5249\n",
            "  Val   Loss: 1.2688 | Val   Acc: 0.5176\n",
            "  Loss Gap:   -0.0350 | Acc  Gap:  0.0073\n",
            "  Loss Ratio: 0.9724 | Acc Ratio: 1.0142\n",
            "Epoch 16/30\n",
            "  Train Loss: 1.2050 | Train Acc: 0.5355\n",
            "  Val   Loss: 1.2771 | Val   Acc: 0.5218\n",
            "  Loss Gap:   -0.0721 | Acc  Gap:  0.0137\n",
            "  Loss Ratio: 0.9435 | Acc Ratio: 1.0263\n",
            "Epoch 17/30\n",
            "  Train Loss: 1.1749 | Train Acc: 0.5468\n",
            "  Val   Loss: 1.2490 | Val   Acc: 0.5268\n",
            "  Loss Gap:   -0.0741 | Acc  Gap:  0.0200\n",
            "  Loss Ratio: 0.9407 | Acc Ratio: 1.0379\n",
            "Epoch 18/30\n",
            "  Train Loss: 1.1569 | Train Acc: 0.5543\n",
            "  Val   Loss: 1.2444 | Val   Acc: 0.5223\n",
            "  Loss Gap:   -0.0875 | Acc  Gap:  0.0320\n",
            "  Loss Ratio: 0.9297 | Acc Ratio: 1.0613\n",
            "Epoch 19/30\n",
            "  Train Loss: 1.1172 | Train Acc: 0.5714\n",
            "  Val   Loss: 1.2459 | Val   Acc: 0.5326\n",
            "  Loss Gap:   -0.1286 | Acc  Gap:  0.0389\n",
            "  Loss Ratio: 0.8968 | Acc Ratio: 1.0730\n",
            "Epoch 20/30\n",
            "  Train Loss: 1.0907 | Train Acc: 0.5819\n",
            "  Val   Loss: 1.2405 | Val   Acc: 0.5293\n",
            "  Loss Gap:   -0.1498 | Acc  Gap:  0.0526\n",
            "  Loss Ratio: 0.8793 | Acc Ratio: 1.0994\n",
            "Epoch 21/30\n",
            "  Train Loss: 1.0577 | Train Acc: 0.5969\n",
            "  Val   Loss: 1.2227 | Val   Acc: 0.5404\n",
            "  Loss Gap:   -0.1651 | Acc  Gap:  0.0565\n",
            "  Loss Ratio: 0.8650 | Acc Ratio: 1.1046\n",
            "Epoch 22/30\n",
            "  Train Loss: 1.0261 | Train Acc: 0.6084\n",
            "  Val   Loss: 1.2214 | Val   Acc: 0.5331\n",
            "  Loss Gap:   -0.1953 | Acc  Gap:  0.0753\n",
            "  Loss Ratio: 0.8401 | Acc Ratio: 1.1412\n",
            "Epoch 23/30\n",
            "  Train Loss: 0.9920 | Train Acc: 0.6221\n",
            "  Val   Loss: 1.2617 | Val   Acc: 0.5138\n",
            "  Loss Gap:   -0.2697 | Acc  Gap:  0.1084\n",
            "  Loss Ratio: 0.7863 | Acc Ratio: 1.2109\n",
            "Epoch 24/30\n",
            "  Train Loss: 0.9626 | Train Acc: 0.6317\n",
            "  Val   Loss: 1.2364 | Val   Acc: 0.5239\n",
            "  Loss Gap:   -0.2738 | Acc  Gap:  0.1079\n",
            "  Loss Ratio: 0.7785 | Acc Ratio: 1.2059\n",
            "Epoch 25/30\n",
            "  Train Loss: 0.9262 | Train Acc: 0.6399\n",
            "  Val   Loss: 1.2335 | Val   Acc: 0.5366\n",
            "  Loss Gap:   -0.3074 | Acc  Gap:  0.1033\n",
            "  Loss Ratio: 0.7508 | Acc Ratio: 1.1925\n",
            "Epoch 26/30\n",
            "  Train Loss: 0.8908 | Train Acc: 0.6586\n",
            "  Val   Loss: 1.2588 | Val   Acc: 0.5284\n",
            "  Loss Gap:   -0.3680 | Acc  Gap:  0.1302\n",
            "  Loss Ratio: 0.7076 | Acc Ratio: 1.2463\n",
            "Epoch 27/30\n",
            "  Train Loss: 0.8784 | Train Acc: 0.6628\n",
            "  Val   Loss: 1.2759 | Val   Acc: 0.5186\n",
            "  Loss Gap:   -0.3975 | Acc  Gap:  0.1441\n",
            "  Loss Ratio: 0.6885 | Acc Ratio: 1.2779\n",
            "Epoch 28/30\n",
            "  Train Loss: 0.8448 | Train Acc: 0.6735\n",
            "  Val   Loss: 1.3216 | Val   Acc: 0.5047\n",
            "  Loss Gap:   -0.4768 | Acc  Gap:  0.1688\n",
            "  Loss Ratio: 0.6392 | Acc Ratio: 1.3345\n",
            "Epoch 29/30\n",
            "  Train Loss: 0.8189 | Train Acc: 0.6822\n",
            "  Val   Loss: 1.2935 | Val   Acc: 0.5235\n",
            "  Loss Gap:   -0.4745 | Acc  Gap:  0.1586\n",
            "  Loss Ratio: 0.6331 | Acc Ratio: 1.3030\n",
            "Epoch 30/30\n",
            "  Train Loss: 0.8063 | Train Acc: 0.6898\n",
            "  Val   Loss: 1.3277 | Val   Acc: 0.5117\n",
            "  Loss Gap:   -0.5214 | Acc  Gap:  0.1781\n",
            "  Loss Ratio: 0.6073 | Acc Ratio: 1.3482\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▂▂▁▁▂▂▁▂▂▁▁▂▃▂▃▃▃▃▄▄▄▅▆▆▆▇▇█▇█</td></tr><tr><td>acc_ratio</td><td>▂▃▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▆▆▆▇▇█▇█</td></tr><tr><td>dropout_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>epochs</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_gap</td><td>▇█████▇▇██▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▂▂▂▁</td></tr><tr><td>loss_ratio</td><td>▇██████▇██▇▇▇▇▇▆▆▆▆▅▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▁▂▂▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>██▇▇▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▃▄▄▅▆▆▆▇▇▇▇▇▇███████▇███▇▇█▇</td></tr><tr><td>val_loss</td><td>██▆▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.17815</td></tr><tr><td>acc_ratio</td><td>1.34817</td></tr><tr><td>dropout_rate</td><td>0.35</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>epochs</td><td>30</td></tr><tr><td>loss_gap</td><td>-0.52142</td></tr><tr><td>loss_ratio</td><td>0.60728</td></tr><tr><td>lr</td><td>0.0012</td></tr><tr><td>normalization</td><td>GroupNorm</td></tr><tr><td>optimizer</td><td>Adam</td></tr><tr><td>train_accuracy</td><td>0.68982</td></tr><tr><td>train_loss</td><td>0.8063</td></tr><tr><td>val_accuracy</td><td>0.51167</td></tr><tr><td>val_loss</td><td>1.32773</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_GroupNorm_Dropout_0.35_LR_0.0012_Epochs_30</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/kf46eqbq' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/kf46eqbq</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_120055-kf46eqbq/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 9 - Batchnorm and tune params\n",
        "add scheduler, Weight Decay, Early stopping to avoid overfitt"
      ],
      "metadata": {
        "id": "wGrGqPUB2Ors"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "import numpy as np\n",
        "\n",
        "class FERCNN_With_BatchNorm(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_rate=0.3):\n",
        "        super(FERCNN_With_BatchNorm, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Calculate flattened size dynamically\n",
        "        dummy_input = torch.zeros(1, 1, 48, 48)\n",
        "        with torch.no_grad():\n",
        "            x = self.pool(torch.relu(self.bn1(self.conv1(dummy_input))))\n",
        "            x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "            x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "            flattened_size = x.view(1, -1).shape[1]\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.dropout(torch.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jYDgJsCV1J8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_batchnorm(epochs=20, lr=0.001, dropout_rate=0.3, weight_decay=1e-5, patience=5):\n",
        "    run = wandb.init(\n",
        "        project=\"ML_4\",\n",
        "        entity=\"nkhar21-student\",\n",
        "        name=f\"FER_BatchNorm_Dropout_{dropout_rate}_LR_{lr}_Epochs_{epochs}_WD_{weight_decay}\",\n",
        "        config={\n",
        "            \"dropout_rate\": dropout_rate,\n",
        "            \"pool_kernel\": (2, 2),\n",
        "            \"epochs\": epochs,\n",
        "            \"batch_size\": 64,\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"early_stopping_patience\": patience\n",
        "        }\n",
        "    )\n",
        "\n",
        "    batch_size = 64\n",
        "    model = FERCNN_With_BatchNorm(num_classes=7, dropout_rate=dropout_rate).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience, verbose=True)\n",
        "    wandb.watch(model)\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = evaluate_model(\n",
        "            x_train, y_train, model, loss_fn, device,\n",
        "            train_mode=True, optimizer=optimizer, batch_size=batch_size\n",
        "        )\n",
        "        val_loss, val_acc = evaluate_model(\n",
        "            x_val, y_val, model, loss_fn, device,\n",
        "            train_mode=False, optimizer=None, batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Overfitting metrics\n",
        "        loss_gap = train_loss - val_loss\n",
        "        acc_gap = train_acc - val_acc\n",
        "        loss_ratio = train_loss / val_loss if val_loss != 0 else float(\"inf\")\n",
        "        acc_ratio = train_acc / val_acc if val_acc != 0 else float(\"inf\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "        print(f\"  Loss Gap:   {loss_gap:.4f} | Acc  Gap:  {acc_gap:.4f}\")\n",
        "        print(f\"  Loss Ratio: {loss_ratio:.4f} | Acc Ratio: {acc_ratio:.4f}\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"loss_gap\": loss_gap,\n",
        "            \"acc_gap\": acc_gap,\n",
        "            \"loss_ratio\": loss_ratio,\n",
        "            \"acc_ratio\": acc_ratio,\n",
        "            \"lr\": optimizer.param_groups[0][\"lr\"]\n",
        "        })\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            print(f\"  Early stopping patience: {epochs_without_improvement}/{patience}\")\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(\"  ⛔ Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    run.finish()\n",
        "    return {\n",
        "        \"val_accuracy\": val_acc,\n",
        "        \"train_accuracy\": train_acc,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"dropout_rate\": dropout_rate,\n",
        "        \"lr\": lr,\n",
        "        \"patience\": patience\n",
        "    }"
      ],
      "metadata": {
        "id": "camofYvo3NYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_with_batchnorm(epochs=30, lr=0.0015, dropout_rate=0.35, weight_decay=1e-5, patience=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l2DSuKl-3The",
        "outputId": "815e4ed5-99c3-4f33-d664-a718d453c4ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_121044-95cwwmk5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/95cwwmk5' target=\"_blank\">FER_BatchNorm_Dropout_0.35_LR_0.0015_Epochs_30_WD_1e-05</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/95cwwmk5' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/95cwwmk5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.7310 | Train Acc: 0.2909\n",
            "  Val   Loss: 1.6251 | Val   Acc: 0.3568\n",
            "  Loss Gap:   0.1059 | Acc  Gap:  -0.0659\n",
            "  Loss Ratio: 1.0652 | Acc Ratio: 0.8153\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.5158 | Train Acc: 0.4091\n",
            "  Val   Loss: 1.4124 | Val   Acc: 0.4444\n",
            "  Loss Gap:   0.1034 | Acc  Gap:  -0.0354\n",
            "  Loss Ratio: 1.0732 | Acc Ratio: 0.9204\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.4017 | Train Acc: 0.4572\n",
            "  Val   Loss: 1.3413 | Val   Acc: 0.4929\n",
            "  Loss Gap:   0.0603 | Acc  Gap:  -0.0356\n",
            "  Loss Ratio: 1.0450 | Acc Ratio: 0.9277\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.3337 | Train Acc: 0.4850\n",
            "  Val   Loss: 1.2672 | Val   Acc: 0.5129\n",
            "  Loss Gap:   0.0665 | Acc  Gap:  -0.0279\n",
            "  Loss Ratio: 1.0525 | Acc Ratio: 0.9456\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.2792 | Train Acc: 0.5069\n",
            "  Val   Loss: 1.3234 | Val   Acc: 0.4934\n",
            "  Loss Gap:   -0.0442 | Acc  Gap:  0.0135\n",
            "  Loss Ratio: 0.9666 | Acc Ratio: 1.0273\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.2311 | Train Acc: 0.5253\n",
            "  Val   Loss: 1.2590 | Val   Acc: 0.5202\n",
            "  Loss Gap:   -0.0279 | Acc  Gap:  0.0051\n",
            "  Loss Ratio: 0.9779 | Acc Ratio: 1.0098\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.1896 | Train Acc: 0.5410\n",
            "  Val   Loss: 1.1752 | Val   Acc: 0.5571\n",
            "  Loss Gap:   0.0144 | Acc  Gap:  -0.0161\n",
            "  Loss Ratio: 1.0123 | Acc Ratio: 0.9710\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.1445 | Train Acc: 0.5616\n",
            "  Val   Loss: 1.2201 | Val   Acc: 0.5430\n",
            "  Loss Gap:   -0.0756 | Acc  Gap:  0.0186\n",
            "  Loss Ratio: 0.9381 | Acc Ratio: 1.0343\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.1002 | Train Acc: 0.5797\n",
            "  Val   Loss: 1.2035 | Val   Acc: 0.5554\n",
            "  Loss Gap:   -0.1034 | Acc  Gap:  0.0243\n",
            "  Loss Ratio: 0.9141 | Acc Ratio: 1.0438\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.0421 | Train Acc: 0.5999\n",
            "  Val   Loss: 1.1651 | Val   Acc: 0.5657\n",
            "  Loss Gap:   -0.1231 | Acc  Gap:  0.0342\n",
            "  Loss Ratio: 0.8944 | Acc Ratio: 1.0605\n",
            "Epoch 11/30\n",
            "  Train Loss: 0.9979 | Train Acc: 0.6169\n",
            "  Val   Loss: 1.1792 | Val   Acc: 0.5646\n",
            "  Loss Gap:   -0.1813 | Acc  Gap:  0.0523\n",
            "  Loss Ratio: 0.8463 | Acc Ratio: 1.0927\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 12/30\n",
            "  Train Loss: 0.9558 | Train Acc: 0.6349\n",
            "  Val   Loss: 1.1685 | Val   Acc: 0.5737\n",
            "  Loss Gap:   -0.2126 | Acc  Gap:  0.0612\n",
            "  Loss Ratio: 0.8180 | Acc Ratio: 1.1068\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 13/30\n",
            "  Train Loss: 0.9093 | Train Acc: 0.6537\n",
            "  Val   Loss: 1.2023 | Val   Acc: 0.5606\n",
            "  Loss Gap:   -0.2929 | Acc  Gap:  0.0931\n",
            "  Loss Ratio: 0.7563 | Acc Ratio: 1.1661\n",
            "  Early stopping patience: 3/5\n",
            "Epoch 14/30\n",
            "  Train Loss: 0.8000 | Train Acc: 0.6909\n",
            "  Val   Loss: 1.2062 | Val   Acc: 0.5655\n",
            "  Loss Gap:   -0.4063 | Acc  Gap:  0.1254\n",
            "  Loss Ratio: 0.6632 | Acc Ratio: 1.2218\n",
            "  Early stopping patience: 4/5\n",
            "Epoch 15/30\n",
            "  Train Loss: 0.7400 | Train Acc: 0.7121\n",
            "  Val   Loss: 1.2342 | Val   Acc: 0.5718\n",
            "  Loss Gap:   -0.4942 | Acc  Gap:  0.1404\n",
            "  Loss Ratio: 0.5996 | Acc Ratio: 1.2455\n",
            "  Early stopping patience: 5/5\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▂▂▂▄▃▃▄▄▄▅▅▆▇█</td></tr><tr><td>acc_ratio</td><td>▁▃▃▃▄▄▄▅▅▅▆▆▇██</td></tr><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>loss_gap</td><td>██▇█▆▆▇▆▆▅▅▄▃▂▁</td></tr><tr><td>loss_ratio</td><td>████▆▇▇▆▆▅▅▄▃▂▁</td></tr><tr><td>lr</td><td>████████████▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▄▅▅▅▅▆▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▆▆▅▅▄▄▄▄▃▃▃▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▅▆▇▇▇██████</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▁▂▂▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.14036</td></tr><tr><td>acc_ratio</td><td>1.24549</td></tr><tr><td>epoch</td><td>15</td></tr><tr><td>loss_gap</td><td>-0.49421</td></tr><tr><td>loss_ratio</td><td>0.59958</td></tr><tr><td>lr</td><td>0.00075</td></tr><tr><td>train_accuracy</td><td>0.71211</td></tr><tr><td>train_loss</td><td>0.74001</td></tr><tr><td>val_accuracy</td><td>0.57175</td></tr><tr><td>val_loss</td><td>1.23423</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Dropout_0.35_LR_0.0015_Epochs_30_WD_1e-05</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/95cwwmk5' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/95cwwmk5</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_121044-95cwwmk5/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_with_batchnorm(epochs=30, lr=0.0015, dropout_rate=0.25, weight_decay=1e-5, patience=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X55th9NK3Zu-",
        "outputId": "d51f5179-f90a-4b2a-e5c4-629cdef9acaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_121226-l3sf4z2p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/l3sf4z2p' target=\"_blank\">FER_BatchNorm_Dropout_0.25_LR_0.0015_Epochs_30_WD_1e-05</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/l3sf4z2p' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/l3sf4z2p</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.7153 | Train Acc: 0.3100\n",
            "  Val   Loss: 1.5604 | Val   Acc: 0.3675\n",
            "  Loss Gap:   0.1550 | Acc  Gap:  -0.0575\n",
            "  Loss Ratio: 1.0993 | Acc Ratio: 0.8436\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.4721 | Train Acc: 0.4296\n",
            "  Val   Loss: 1.3943 | Val   Acc: 0.4680\n",
            "  Loss Gap:   0.0777 | Acc  Gap:  -0.0384\n",
            "  Loss Ratio: 1.0558 | Acc Ratio: 0.9180\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.3616 | Train Acc: 0.4718\n",
            "  Val   Loss: 1.3239 | Val   Acc: 0.4814\n",
            "  Loss Gap:   0.0377 | Acc  Gap:  -0.0096\n",
            "  Loss Ratio: 1.0285 | Acc Ratio: 0.9801\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.2995 | Train Acc: 0.5017\n",
            "  Val   Loss: 1.3108 | Val   Acc: 0.5098\n",
            "  Loss Gap:   -0.0113 | Acc  Gap:  -0.0081\n",
            "  Loss Ratio: 0.9914 | Acc Ratio: 0.9842\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.2399 | Train Acc: 0.5235\n",
            "  Val   Loss: 1.2278 | Val   Acc: 0.5298\n",
            "  Loss Gap:   0.0121 | Acc  Gap:  -0.0062\n",
            "  Loss Ratio: 1.0098 | Acc Ratio: 0.9882\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.1907 | Train Acc: 0.5410\n",
            "  Val   Loss: 1.2567 | Val   Acc: 0.5197\n",
            "  Loss Gap:   -0.0660 | Acc  Gap:  0.0213\n",
            "  Loss Ratio: 0.9475 | Acc Ratio: 1.0409\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.1321 | Train Acc: 0.5682\n",
            "  Val   Loss: 1.1737 | Val   Acc: 0.5658\n",
            "  Loss Gap:   -0.0416 | Acc  Gap:  0.0023\n",
            "  Loss Ratio: 0.9646 | Acc Ratio: 1.0041\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.0856 | Train Acc: 0.5911\n",
            "  Val   Loss: 1.1711 | Val   Acc: 0.5648\n",
            "  Loss Gap:   -0.0855 | Acc  Gap:  0.0263\n",
            "  Loss Ratio: 0.9270 | Acc Ratio: 1.0466\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.0310 | Train Acc: 0.6082\n",
            "  Val   Loss: 1.1424 | Val   Acc: 0.5691\n",
            "  Loss Gap:   -0.1114 | Acc  Gap:  0.0390\n",
            "  Loss Ratio: 0.9025 | Acc Ratio: 1.0686\n",
            "Epoch 10/30\n",
            "  Train Loss: 0.9761 | Train Acc: 0.6317\n",
            "  Val   Loss: 1.1941 | Val   Acc: 0.5467\n",
            "  Loss Gap:   -0.2180 | Acc  Gap:  0.0851\n",
            "  Loss Ratio: 0.8174 | Acc Ratio: 1.1556\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 11/30\n",
            "  Train Loss: 0.9198 | Train Acc: 0.6505\n",
            "  Val   Loss: 1.1632 | Val   Acc: 0.5712\n",
            "  Loss Gap:   -0.2434 | Acc  Gap:  0.0793\n",
            "  Loss Ratio: 0.7908 | Acc Ratio: 1.1388\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 12/30\n",
            "  Train Loss: 0.8731 | Train Acc: 0.6705\n",
            "  Val   Loss: 1.1674 | Val   Acc: 0.5697\n",
            "  Loss Gap:   -0.2943 | Acc  Gap:  0.1008\n",
            "  Loss Ratio: 0.7479 | Acc Ratio: 1.1770\n",
            "  Early stopping patience: 3/5\n",
            "Epoch 13/30\n",
            "  Train Loss: 0.7412 | Train Acc: 0.7195\n",
            "  Val   Loss: 1.1873 | Val   Acc: 0.5822\n",
            "  Loss Gap:   -0.4461 | Acc  Gap:  0.1373\n",
            "  Loss Ratio: 0.6243 | Acc Ratio: 1.2358\n",
            "  Early stopping patience: 4/5\n",
            "Epoch 14/30\n",
            "  Train Loss: 0.6697 | Train Acc: 0.7439\n",
            "  Val   Loss: 1.2412 | Val   Acc: 0.5899\n",
            "  Loss Gap:   -0.5715 | Acc  Gap:  0.1540\n",
            "  Loss Ratio: 0.5395 | Acc Ratio: 1.2611\n",
            "  Early stopping patience: 5/5\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▂▃▃▃▄▃▄▄▆▆▆▇█</td></tr><tr><td>acc_ratio</td><td>▁▂▃▃▃▄▄▄▅▆▆▇██</td></tr><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>loss_gap</td><td>█▇▇▆▇▆▆▆▅▄▄▄▂▁</td></tr><tr><td>loss_ratio</td><td>█▇▇▇▇▆▆▆▆▄▄▄▂▁</td></tr><tr><td>lr</td><td>███████████▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▄▄▅▅▆▆▆▆▇██</td></tr><tr><td>train_loss</td><td>█▆▆▅▅▄▄▄▃▃▃▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▅▆▆▇▇▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▄▄▂▃▂▁▁▂▁▁▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.15403</td></tr><tr><td>acc_ratio</td><td>1.26113</td></tr><tr><td>epoch</td><td>14</td></tr><tr><td>loss_gap</td><td>-0.57155</td></tr><tr><td>loss_ratio</td><td>0.53954</td></tr><tr><td>lr</td><td>0.00075</td></tr><tr><td>train_accuracy</td><td>0.74389</td></tr><tr><td>train_loss</td><td>0.6697</td></tr><tr><td>val_accuracy</td><td>0.58986</td></tr><tr><td>val_loss</td><td>1.24124</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Dropout_0.25_LR_0.0015_Epochs_30_WD_1e-05</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/l3sf4z2p' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/l3sf4z2p</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_121226-l3sf4z2p/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def hyperparameter_search(trials=10):\n",
        "    best_model = None\n",
        "    best_val_acc = 0.0\n",
        "    results = []\n",
        "\n",
        "    for i in range(trials):\n",
        "        lr = round(random.uniform(0.0005, 0.005), 10)\n",
        "        dropout_rate = round(random.uniform(0.2, 0.5), 10)\n",
        "        patience = 5\n",
        "        weight_decay = round(random.uniform(1e-5, 1e-3), 10)\n",
        "\n",
        "        print(f\"\\n🔁 Trial {i+1}/{trials}\")\n",
        "        print(f\"   ➤ LR={lr}, Dropout={dropout_rate}, Weight_decay = {weight_decay}\")\n",
        "\n",
        "        result = train_model_with_batchnorm(\n",
        "            epochs=30,\n",
        "            lr=lr,\n",
        "            dropout_rate=dropout_rate,\n",
        "            weight_decay=weight_decay,\n",
        "            patience=patience\n",
        "        )\n",
        "\n",
        "        results.append(result)\n",
        "\n",
        "        if result[\"val_accuracy\"] > best_val_acc:\n",
        "            best_val_acc = result[\"val_accuracy\"]\n",
        "            best_model = result\n",
        "\n",
        "    print(\"\\n🏆 Best Model Found:\")\n",
        "    print(f\"   ➤ Val Acc: {best_model['val_accuracy']:.4f}\")\n",
        "    print(f\"   ➤ LR: {best_model['lr']}, Dropout: {best_model['dropout_rate']}, Patience: {best_model['patience']}\")\n",
        "    return best_model, results\n"
      ],
      "metadata": {
        "id": "IhyZwmG-5Kse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model, all_trials = hyperparameter_search(trials=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EClHwrg25XDy",
        "outputId": "dce36110-d3ee-4fad-d7ff-14c56b3464b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔁 Trial 1/10\n",
            "   ➤ LR=0.0020054909, Dropout=0.4383999049, Weight_decay = 0.0009416916\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▄▅▇█▇</td></tr><tr><td>acc_ratio</td><td>▁▄▅▇█▇</td></tr><tr><td>epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>loss_gap</td><td>█▅▄▂▁▁</td></tr><tr><td>loss_ratio</td><td>█▆▄▂▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇▇█</td></tr><tr><td>train_loss</td><td>█▅▄▂▂▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▆▆█</td></tr><tr><td>val_loss</td><td>█▄▄▃▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.00377</td></tr><tr><td>acc_ratio</td><td>1.0069</td></tr><tr><td>epoch</td><td>6</td></tr><tr><td>loss_gap</td><td>-0.01162</td></tr><tr><td>loss_ratio</td><td>0.99024</td></tr><tr><td>lr</td><td>0.00099</td></tr><tr><td>train_accuracy</td><td>0.55009</td></tr><tr><td>train_loss</td><td>1.17878</td></tr><tr><td>val_accuracy</td><td>0.54633</td></tr><tr><td>val_loss</td><td>1.19039</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Dropout_0.3194633455_LR_0.0009948282_Epochs_30_WD_0.0006050251</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/juvafw63' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/juvafw63</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_122434-juvafw63/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_122528-xhadd1e5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/xhadd1e5' target=\"_blank\">FER_BatchNorm_Dropout_0.4383999049_LR_0.0020054909_Epochs_30_WD_0.0009416916</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/xhadd1e5' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/xhadd1e5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.7846 | Train Acc: 0.2715\n",
            "  Val   Loss: 1.6450 | Val   Acc: 0.3161\n",
            "  Loss Gap:   0.1396 | Acc  Gap:  -0.0446\n",
            "  Loss Ratio: 1.0849 | Acc Ratio: 0.8589\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.5868 | Train Acc: 0.3665\n",
            "  Val   Loss: 1.6010 | Val   Acc: 0.3400\n",
            "  Loss Gap:   -0.0142 | Acc  Gap:  0.0265\n",
            "  Loss Ratio: 0.9911 | Acc Ratio: 1.0780\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.4944 | Train Acc: 0.4096\n",
            "  Val   Loss: 1.4215 | Val   Acc: 0.4361\n",
            "  Loss Gap:   0.0729 | Acc  Gap:  -0.0265\n",
            "  Loss Ratio: 1.0513 | Acc Ratio: 0.9393\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.4498 | Train Acc: 0.4310\n",
            "  Val   Loss: 1.4393 | Val   Acc: 0.4462\n",
            "  Loss Gap:   0.0105 | Acc  Gap:  -0.0152\n",
            "  Loss Ratio: 1.0073 | Acc Ratio: 0.9659\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.4058 | Train Acc: 0.4515\n",
            "  Val   Loss: 1.4546 | Val   Acc: 0.4565\n",
            "  Loss Gap:   -0.0487 | Acc  Gap:  -0.0049\n",
            "  Loss Ratio: 0.9665 | Acc Ratio: 0.9892\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.3605 | Train Acc: 0.4692\n",
            "  Val   Loss: 1.3696 | Val   Acc: 0.4713\n",
            "  Loss Gap:   -0.0091 | Acc  Gap:  -0.0020\n",
            "  Loss Ratio: 0.9933 | Acc Ratio: 0.9957\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.3310 | Train Acc: 0.4823\n",
            "  Val   Loss: 1.3503 | Val   Acc: 0.4801\n",
            "  Loss Gap:   -0.0193 | Acc  Gap:  0.0021\n",
            "  Loss Ratio: 0.9857 | Acc Ratio: 1.0044\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.2810 | Train Acc: 0.5029\n",
            "  Val   Loss: 1.2902 | Val   Acc: 0.5075\n",
            "  Loss Gap:   -0.0091 | Acc  Gap:  -0.0046\n",
            "  Loss Ratio: 0.9929 | Acc Ratio: 0.9909\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.2486 | Train Acc: 0.5191\n",
            "  Val   Loss: 1.3118 | Val   Acc: 0.5052\n",
            "  Loss Gap:   -0.0633 | Acc  Gap:  0.0139\n",
            "  Loss Ratio: 0.9518 | Acc Ratio: 1.0275\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.2133 | Train Acc: 0.5364\n",
            "  Val   Loss: 1.2006 | Val   Acc: 0.5463\n",
            "  Loss Gap:   0.0127 | Acc  Gap:  -0.0099\n",
            "  Loss Ratio: 1.0106 | Acc Ratio: 0.9818\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.1966 | Train Acc: 0.5443\n",
            "  Val   Loss: 1.3554 | Val   Acc: 0.4850\n",
            "  Loss Gap:   -0.1588 | Acc  Gap:  0.0592\n",
            "  Loss Ratio: 0.8829 | Acc Ratio: 1.1221\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 12/30\n",
            "  Train Loss: 1.1638 | Train Acc: 0.5608\n",
            "  Val   Loss: 1.2660 | Val   Acc: 0.5279\n",
            "  Loss Gap:   -0.1023 | Acc  Gap:  0.0329\n",
            "  Loss Ratio: 0.9192 | Acc Ratio: 1.0623\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 13/30\n",
            "  Train Loss: 1.1442 | Train Acc: 0.5696\n",
            "  Val   Loss: 1.1721 | Val   Acc: 0.5583\n",
            "  Loss Gap:   -0.0279 | Acc  Gap:  0.0113\n",
            "  Loss Ratio: 0.9762 | Acc Ratio: 1.0202\n",
            "Epoch 14/30\n",
            "  Train Loss: 1.1205 | Train Acc: 0.5821\n",
            "  Val   Loss: 1.2232 | Val   Acc: 0.5355\n",
            "  Loss Gap:   -0.1028 | Acc  Gap:  0.0466\n",
            "  Loss Ratio: 0.9160 | Acc Ratio: 1.0870\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 15/30\n",
            "  Train Loss: 1.1019 | Train Acc: 0.5912\n",
            "  Val   Loss: 1.1806 | Val   Acc: 0.5601\n",
            "  Loss Gap:   -0.0787 | Acc  Gap:  0.0312\n",
            "  Loss Ratio: 0.9333 | Acc Ratio: 1.0556\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 16/30\n",
            "  Train Loss: 1.0787 | Train Acc: 0.5958\n",
            "  Val   Loss: 1.2307 | Val   Acc: 0.5430\n",
            "  Loss Gap:   -0.1520 | Acc  Gap:  0.0528\n",
            "  Loss Ratio: 0.8765 | Acc Ratio: 1.0971\n",
            "  Early stopping patience: 3/5\n",
            "Epoch 17/30\n",
            "  Train Loss: 1.0683 | Train Acc: 0.6013\n",
            "  Val   Loss: 1.1790 | Val   Acc: 0.5489\n",
            "  Loss Gap:   -0.1106 | Acc  Gap:  0.0524\n",
            "  Loss Ratio: 0.9062 | Acc Ratio: 1.0954\n",
            "  Early stopping patience: 4/5\n",
            "Epoch 18/30\n",
            "  Train Loss: 1.0466 | Train Acc: 0.6133\n",
            "  Val   Loss: 1.2298 | Val   Acc: 0.5411\n",
            "  Loss Gap:   -0.1832 | Acc  Gap:  0.0722\n",
            "  Loss Ratio: 0.8510 | Acc Ratio: 1.1335\n",
            "  Early stopping patience: 5/5\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▅▂▃▃▄▄▃▅▃▇▆▄▆▆▇▇█</td></tr><tr><td>acc_ratio</td><td>▁▇▃▄▄▄▅▄▅▄█▆▅▇▆▇▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>loss_gap</td><td>█▅▇▅▄▅▅▅▄▅▂▃▄▃▃▂▃▁</td></tr><tr><td>loss_ratio</td><td>█▅▇▆▄▅▅▅▄▆▂▃▅▃▃▂▃▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▄▅▅▅▆▆▆▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▄▅▅▅▆▆▆█▆▇█▇███▇</td></tr><tr><td>val_loss</td><td>█▇▅▅▅▄▄▃▃▁▄▂▁▂▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.07221</td></tr><tr><td>acc_ratio</td><td>1.13346</td></tr><tr><td>epoch</td><td>18</td></tr><tr><td>loss_gap</td><td>-0.18323</td></tr><tr><td>loss_ratio</td><td>0.85101</td></tr><tr><td>lr</td><td>0.00201</td></tr><tr><td>train_accuracy</td><td>0.61331</td></tr><tr><td>train_loss</td><td>1.04661</td></tr><tr><td>val_accuracy</td><td>0.5411</td></tr><tr><td>val_loss</td><td>1.22984</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Dropout_0.4383999049_LR_0.0020054909_Epochs_30_WD_0.0009416916</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/xhadd1e5' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/xhadd1e5</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_122528-xhadd1e5/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔁 Trial 2/10\n",
            "   ➤ LR=0.0020625365, Dropout=0.2517204526, Weight_decay = 0.0007595663\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_122630-5j0cnijs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/5j0cnijs' target=\"_blank\">FER_BatchNorm_Dropout_0.2517204526_LR_0.0020625365_Epochs_30_WD_0.0007595663</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/5j0cnijs' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/5j0cnijs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.7333 | Train Acc: 0.3030\n",
            "  Val   Loss: 1.5504 | Val   Acc: 0.4037\n",
            "  Loss Gap:   0.1829 | Acc  Gap:  -0.1007\n",
            "  Loss Ratio: 1.1180 | Acc Ratio: 0.7505\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.4844 | Train Acc: 0.4262\n",
            "  Val   Loss: 1.4896 | Val   Acc: 0.4277\n",
            "  Loss Gap:   -0.0052 | Acc  Gap:  -0.0015\n",
            "  Loss Ratio: 0.9965 | Acc Ratio: 0.9965\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.3757 | Train Acc: 0.4720\n",
            "  Val   Loss: 1.3568 | Val   Acc: 0.4714\n",
            "  Loss Gap:   0.0189 | Acc  Gap:  0.0005\n",
            "  Loss Ratio: 1.0139 | Acc Ratio: 1.0012\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.3068 | Train Acc: 0.5047\n",
            "  Val   Loss: 1.2780 | Val   Acc: 0.5082\n",
            "  Loss Gap:   0.0288 | Acc  Gap:  -0.0035\n",
            "  Loss Ratio: 1.0225 | Acc Ratio: 0.9931\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.2504 | Train Acc: 0.5230\n",
            "  Val   Loss: 1.2541 | Val   Acc: 0.5266\n",
            "  Loss Gap:   -0.0037 | Acc  Gap:  -0.0037\n",
            "  Loss Ratio: 0.9970 | Acc Ratio: 0.9930\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.2040 | Train Acc: 0.5426\n",
            "  Val   Loss: 1.1945 | Val   Acc: 0.5463\n",
            "  Loss Gap:   0.0094 | Acc  Gap:  -0.0038\n",
            "  Loss Ratio: 1.0079 | Acc Ratio: 0.9931\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.1565 | Train Acc: 0.5618\n",
            "  Val   Loss: 1.2347 | Val   Acc: 0.5338\n",
            "  Loss Gap:   -0.0783 | Acc  Gap:  0.0281\n",
            "  Loss Ratio: 0.9366 | Acc Ratio: 1.0526\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.1219 | Train Acc: 0.5750\n",
            "  Val   Loss: 1.2280 | Val   Acc: 0.5394\n",
            "  Loss Gap:   -0.1062 | Acc  Gap:  0.0357\n",
            "  Loss Ratio: 0.9135 | Acc Ratio: 1.0662\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.0877 | Train Acc: 0.5909\n",
            "  Val   Loss: 1.1668 | Val   Acc: 0.5587\n",
            "  Loss Gap:   -0.0792 | Acc  Gap:  0.0322\n",
            "  Loss Ratio: 0.9322 | Acc Ratio: 1.0576\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.0519 | Train Acc: 0.6063\n",
            "  Val   Loss: 1.1892 | Val   Acc: 0.5575\n",
            "  Loss Gap:   -0.1373 | Acc  Gap:  0.0488\n",
            "  Loss Ratio: 0.8845 | Acc Ratio: 1.0875\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.0207 | Train Acc: 0.6178\n",
            "  Val   Loss: 1.1666 | Val   Acc: 0.5664\n",
            "  Loss Gap:   -0.1459 | Acc  Gap:  0.0515\n",
            "  Loss Ratio: 0.8750 | Acc Ratio: 1.0909\n",
            "Epoch 12/30\n",
            "  Train Loss: 0.9870 | Train Acc: 0.6281\n",
            "  Val   Loss: 1.1478 | Val   Acc: 0.5697\n",
            "  Loss Gap:   -0.1608 | Acc  Gap:  0.0584\n",
            "  Loss Ratio: 0.8599 | Acc Ratio: 1.1025\n",
            "Epoch 13/30\n",
            "  Train Loss: 0.9500 | Train Acc: 0.6445\n",
            "  Val   Loss: 1.1746 | Val   Acc: 0.5599\n",
            "  Loss Gap:   -0.2245 | Acc  Gap:  0.0846\n",
            "  Loss Ratio: 0.8088 | Acc Ratio: 1.1511\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 14/30\n",
            "  Train Loss: 0.9187 | Train Acc: 0.6578\n",
            "  Val   Loss: 1.1760 | Val   Acc: 0.5618\n",
            "  Loss Gap:   -0.2572 | Acc  Gap:  0.0960\n",
            "  Loss Ratio: 0.7813 | Acc Ratio: 1.1709\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 15/30\n",
            "  Train Loss: 0.8957 | Train Acc: 0.6671\n",
            "  Val   Loss: 1.1625 | Val   Acc: 0.5733\n",
            "  Loss Gap:   -0.2668 | Acc  Gap:  0.0938\n",
            "  Loss Ratio: 0.7705 | Acc Ratio: 1.1636\n",
            "  Early stopping patience: 3/5\n",
            "Epoch 16/30\n",
            "  Train Loss: 0.8722 | Train Acc: 0.6761\n",
            "  Val   Loss: 1.1582 | Val   Acc: 0.5744\n",
            "  Loss Gap:   -0.2859 | Acc  Gap:  0.1017\n",
            "  Loss Ratio: 0.7531 | Acc Ratio: 1.1771\n",
            "  Early stopping patience: 4/5\n",
            "Epoch 17/30\n",
            "  Train Loss: 0.8403 | Train Acc: 0.6865\n",
            "  Val   Loss: 1.2292 | Val   Acc: 0.5578\n",
            "  Loss Gap:   -0.3889 | Acc  Gap:  0.1286\n",
            "  Loss Ratio: 0.6836 | Acc Ratio: 1.2306\n",
            "  Early stopping patience: 5/5\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▄▄▄▄▄▅▅▅▆▆▆▇▇▇▇█</td></tr><tr><td>acc_ratio</td><td>▁▅▅▅▅▅▅▆▅▆▆▆▇▇▇▇█</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██</td></tr><tr><td>loss_gap</td><td>█▆▆▆▆▆▅▄▅▄▄▄▃▃▂▂▁</td></tr><tr><td>loss_ratio</td><td>█▆▆▆▆▆▅▅▅▄▄▄▃▃▂▂▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▅▅▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▄▅▆▇▆▇▇▇██▇▇██▇</td></tr><tr><td>val_loss</td><td>█▇▅▃▃▂▃▂▁▂▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.12864</td></tr><tr><td>acc_ratio</td><td>1.23062</td></tr><tr><td>epoch</td><td>17</td></tr><tr><td>loss_gap</td><td>-0.38894</td></tr><tr><td>loss_ratio</td><td>0.68358</td></tr><tr><td>lr</td><td>0.00206</td></tr><tr><td>train_accuracy</td><td>0.68646</td></tr><tr><td>train_loss</td><td>0.84025</td></tr><tr><td>val_accuracy</td><td>0.55782</td></tr><tr><td>val_loss</td><td>1.2292</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Dropout_0.2517204526_LR_0.0020625365_Epochs_30_WD_0.0007595663</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/5j0cnijs' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/5j0cnijs</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_122630-5j0cnijs/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔁 Trial 3/10\n",
            "   ➤ LR=0.0038934425, Dropout=0.2939386854, Weight_decay = 0.0004513917\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_122727-7pco22os</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/7pco22os' target=\"_blank\">FER_BatchNorm_Dropout_0.2939386854_LR_0.0038934425_Epochs_30_WD_0.0004513917</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/7pco22os' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/7pco22os</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.8555 | Train Acc: 0.2537\n",
            "  Val   Loss: 1.7194 | Val   Acc: 0.2905\n",
            "  Loss Gap:   0.1361 | Acc  Gap:  -0.0368\n",
            "  Loss Ratio: 1.0791 | Acc Ratio: 0.8732\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.6764 | Train Acc: 0.3060\n",
            "  Val   Loss: 1.5863 | Val   Acc: 0.3408\n",
            "  Loss Gap:   0.0901 | Acc  Gap:  -0.0348\n",
            "  Loss Ratio: 1.0568 | Acc Ratio: 0.8980\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.5904 | Train Acc: 0.3576\n",
            "  Val   Loss: 1.5582 | Val   Acc: 0.3708\n",
            "  Loss Gap:   0.0322 | Acc  Gap:  -0.0132\n",
            "  Loss Ratio: 1.0206 | Acc Ratio: 0.9643\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.5109 | Train Acc: 0.3938\n",
            "  Val   Loss: 1.4493 | Val   Acc: 0.4281\n",
            "  Loss Gap:   0.0616 | Acc  Gap:  -0.0342\n",
            "  Loss Ratio: 1.0425 | Acc Ratio: 0.9200\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.4390 | Train Acc: 0.4249\n",
            "  Val   Loss: 1.3874 | Val   Acc: 0.4451\n",
            "  Loss Gap:   0.0515 | Acc  Gap:  -0.0202\n",
            "  Loss Ratio: 1.0371 | Acc Ratio: 0.9546\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.3790 | Train Acc: 0.4680\n",
            "  Val   Loss: 1.3595 | Val   Acc: 0.4793\n",
            "  Loss Gap:   0.0195 | Acc  Gap:  -0.0113\n",
            "  Loss Ratio: 1.0144 | Acc Ratio: 0.9765\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.3217 | Train Acc: 0.4932\n",
            "  Val   Loss: 1.3208 | Val   Acc: 0.4875\n",
            "  Loss Gap:   0.0009 | Acc  Gap:  0.0058\n",
            "  Loss Ratio: 1.0007 | Acc Ratio: 1.0118\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.2712 | Train Acc: 0.5148\n",
            "  Val   Loss: 1.3475 | Val   Acc: 0.4805\n",
            "  Loss Gap:   -0.0764 | Acc  Gap:  0.0343\n",
            "  Loss Ratio: 0.9433 | Acc Ratio: 1.0714\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.2355 | Train Acc: 0.5327\n",
            "  Val   Loss: 1.2638 | Val   Acc: 0.5225\n",
            "  Loss Gap:   -0.0283 | Acc  Gap:  0.0102\n",
            "  Loss Ratio: 0.9776 | Acc Ratio: 1.0195\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.2018 | Train Acc: 0.5442\n",
            "  Val   Loss: 1.3242 | Val   Acc: 0.4967\n",
            "  Loss Gap:   -0.1224 | Acc  Gap:  0.0475\n",
            "  Loss Ratio: 0.9075 | Acc Ratio: 1.0957\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.1726 | Train Acc: 0.5530\n",
            "  Val   Loss: 1.2843 | Val   Acc: 0.5169\n",
            "  Loss Gap:   -0.1117 | Acc  Gap:  0.0361\n",
            "  Loss Ratio: 0.9130 | Acc Ratio: 1.0699\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 12/30\n",
            "  Train Loss: 1.1436 | Train Acc: 0.5709\n",
            "  Val   Loss: 1.1981 | Val   Acc: 0.5510\n",
            "  Loss Gap:   -0.0546 | Acc  Gap:  0.0198\n",
            "  Loss Ratio: 0.9545 | Acc Ratio: 1.0360\n",
            "Epoch 13/30\n",
            "  Train Loss: 1.1178 | Train Acc: 0.5804\n",
            "  Val   Loss: 1.2365 | Val   Acc: 0.5265\n",
            "  Loss Gap:   -0.1188 | Acc  Gap:  0.0539\n",
            "  Loss Ratio: 0.9040 | Acc Ratio: 1.1023\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 14/30\n",
            "  Train Loss: 1.1016 | Train Acc: 0.5892\n",
            "  Val   Loss: 1.2214 | Val   Acc: 0.5367\n",
            "  Loss Gap:   -0.1199 | Acc  Gap:  0.0524\n",
            "  Loss Ratio: 0.9019 | Acc Ratio: 1.0977\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 15/30\n",
            "  Train Loss: 1.0802 | Train Acc: 0.5987\n",
            "  Val   Loss: 1.2273 | Val   Acc: 0.5347\n",
            "  Loss Gap:   -0.1470 | Acc  Gap:  0.0641\n",
            "  Loss Ratio: 0.8802 | Acc Ratio: 1.1198\n",
            "  Early stopping patience: 3/5\n",
            "Epoch 16/30\n",
            "  Train Loss: 1.0619 | Train Acc: 0.6037\n",
            "  Val   Loss: 1.2221 | Val   Acc: 0.5385\n",
            "  Loss Gap:   -0.1602 | Acc  Gap:  0.0652\n",
            "  Loss Ratio: 0.8689 | Acc Ratio: 1.1211\n",
            "  Early stopping patience: 4/5\n",
            "Epoch 17/30\n",
            "  Train Loss: 1.0462 | Train Acc: 0.6076\n",
            "  Val   Loss: 1.1720 | Val   Acc: 0.5623\n",
            "  Loss Gap:   -0.1258 | Acc  Gap:  0.0453\n",
            "  Loss Ratio: 0.8927 | Acc Ratio: 1.0805\n",
            "Epoch 18/30\n",
            "  Train Loss: 1.0351 | Train Acc: 0.6173\n",
            "  Val   Loss: 1.1918 | Val   Acc: 0.5432\n",
            "  Loss Gap:   -0.1566 | Acc  Gap:  0.0741\n",
            "  Loss Ratio: 0.8686 | Acc Ratio: 1.1364\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 19/30\n",
            "  Train Loss: 1.0250 | Train Acc: 0.6209\n",
            "  Val   Loss: 1.1670 | Val   Acc: 0.5569\n",
            "  Loss Gap:   -0.1421 | Acc  Gap:  0.0639\n",
            "  Loss Ratio: 0.8783 | Acc Ratio: 1.1148\n",
            "Epoch 20/30\n",
            "  Train Loss: 1.0053 | Train Acc: 0.6254\n",
            "  Val   Loss: 1.1653 | Val   Acc: 0.5636\n",
            "  Loss Gap:   -0.1600 | Acc  Gap:  0.0618\n",
            "  Loss Ratio: 0.8627 | Acc Ratio: 1.1097\n",
            "Epoch 21/30\n",
            "  Train Loss: 0.9840 | Train Acc: 0.6346\n",
            "  Val   Loss: 1.2172 | Val   Acc: 0.5343\n",
            "  Loss Gap:   -0.2332 | Acc  Gap:  0.1003\n",
            "  Loss Ratio: 0.8084 | Acc Ratio: 1.1878\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 22/30\n",
            "  Train Loss: 0.9710 | Train Acc: 0.6426\n",
            "  Val   Loss: 1.2261 | Val   Acc: 0.5428\n",
            "  Loss Gap:   -0.2551 | Acc  Gap:  0.0997\n",
            "  Loss Ratio: 0.7919 | Acc Ratio: 1.1837\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 23/30\n",
            "  Train Loss: 0.9699 | Train Acc: 0.6395\n",
            "  Val   Loss: 1.2167 | Val   Acc: 0.5448\n",
            "  Loss Gap:   -0.2468 | Acc  Gap:  0.0948\n",
            "  Loss Ratio: 0.7971 | Acc Ratio: 1.1740\n",
            "  Early stopping patience: 3/5\n",
            "Epoch 24/30\n",
            "  Train Loss: 0.9530 | Train Acc: 0.6473\n",
            "  Val   Loss: 1.1833 | Val   Acc: 0.5632\n",
            "  Loss Gap:   -0.2303 | Acc  Gap:  0.0841\n",
            "  Loss Ratio: 0.8054 | Acc Ratio: 1.1493\n",
            "  Early stopping patience: 4/5\n",
            "Epoch 25/30\n",
            "  Train Loss: 0.9422 | Train Acc: 0.6524\n",
            "  Val   Loss: 1.1775 | Val   Acc: 0.5618\n",
            "  Loss Gap:   -0.2354 | Acc  Gap:  0.0906\n",
            "  Loss Ratio: 0.8001 | Acc Ratio: 1.1612\n",
            "  Early stopping patience: 5/5\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▁▂▁▂▂▃▅▃▅▅▄▆▆▆▆▅▇▆▆███▇█</td></tr><tr><td>acc_ratio</td><td>▁▂▃▂▃▃▄▅▄▆▅▅▆▆▆▇▆▇▆▆███▇▇</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>loss_gap</td><td>█▇▆▇▆▆▆▄▅▃▄▅▃▃▃▃▃▃▃▃▁▁▁▁▁</td></tr><tr><td>loss_ratio</td><td>█▇▇▇▇▆▆▅▆▄▄▅▄▄▃▃▃▃▃▃▁▁▁▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▃▅▅▆▆▆▇▆▇█▇▇▇▇█▇██▇▇███</td></tr><tr><td>val_loss</td><td>█▆▆▅▄▃▃▃▂▃▃▁▂▂▂▂▁▁▁▁▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.09059</td></tr><tr><td>acc_ratio</td><td>1.16124</td></tr><tr><td>epoch</td><td>25</td></tr><tr><td>loss_gap</td><td>-0.23536</td></tr><tr><td>loss_ratio</td><td>0.80012</td></tr><tr><td>lr</td><td>0.00389</td></tr><tr><td>train_accuracy</td><td>0.65241</td></tr><tr><td>train_loss</td><td>0.94216</td></tr><tr><td>val_accuracy</td><td>0.56183</td></tr><tr><td>val_loss</td><td>1.17752</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Dropout_0.2939386854_LR_0.0038934425_Epochs_30_WD_0.0004513917</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/7pco22os' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/7pco22os</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_122727-7pco22os/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔁 Trial 4/10\n",
            "   ➤ LR=0.003370767, Dropout=0.2205149216, Weight_decay = 0.0003962556\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_122852-c4l57229</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/c4l57229' target=\"_blank\">FER_BatchNorm_Dropout_0.2205149216_LR_0.003370767_Epochs_30_WD_0.0003962556</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/c4l57229' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/c4l57229</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.7944 | Train Acc: 0.2748\n",
            "  Val   Loss: 1.6042 | Val   Acc: 0.3645\n",
            "  Loss Gap:   0.1902 | Acc  Gap:  -0.0897\n",
            "  Loss Ratio: 1.1185 | Acc Ratio: 0.7539\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.5608 | Train Acc: 0.3818\n",
            "  Val   Loss: 1.6135 | Val   Acc: 0.3661\n",
            "  Loss Gap:   -0.0527 | Acc  Gap:  0.0157\n",
            "  Loss Ratio: 0.9673 | Acc Ratio: 1.0429\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.4460 | Train Acc: 0.4346\n",
            "  Val   Loss: 1.3961 | Val   Acc: 0.4603\n",
            "  Loss Gap:   0.0500 | Acc  Gap:  -0.0257\n",
            "  Loss Ratio: 1.0358 | Acc Ratio: 0.9441\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.3709 | Train Acc: 0.4662\n",
            "  Val   Loss: 1.4227 | Val   Acc: 0.4317\n",
            "  Loss Gap:   -0.0518 | Acc  Gap:  0.0345\n",
            "  Loss Ratio: 0.9636 | Acc Ratio: 1.0799\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.3191 | Train Acc: 0.4866\n",
            "  Val   Loss: 1.3416 | Val   Acc: 0.4800\n",
            "  Loss Gap:   -0.0226 | Acc  Gap:  0.0066\n",
            "  Loss Ratio: 0.9832 | Acc Ratio: 1.0138\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.2747 | Train Acc: 0.5117\n",
            "  Val   Loss: 1.2699 | Val   Acc: 0.5026\n",
            "  Loss Gap:   0.0048 | Acc  Gap:  0.0091\n",
            "  Loss Ratio: 1.0038 | Acc Ratio: 1.0181\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.2345 | Train Acc: 0.5211\n",
            "  Val   Loss: 1.2240 | Val   Acc: 0.5354\n",
            "  Loss Gap:   0.0105 | Acc  Gap:  -0.0142\n",
            "  Loss Ratio: 1.0086 | Acc Ratio: 0.9734\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.2018 | Train Acc: 0.5430\n",
            "  Val   Loss: 1.2695 | Val   Acc: 0.5082\n",
            "  Loss Gap:   -0.0677 | Acc  Gap:  0.0348\n",
            "  Loss Ratio: 0.9467 | Acc Ratio: 1.0684\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.1724 | Train Acc: 0.5548\n",
            "  Val   Loss: 1.2424 | Val   Acc: 0.5354\n",
            "  Loss Gap:   -0.0701 | Acc  Gap:  0.0194\n",
            "  Loss Ratio: 0.9436 | Acc Ratio: 1.0362\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.1381 | Train Acc: 0.5688\n",
            "  Val   Loss: 1.1993 | Val   Acc: 0.5437\n",
            "  Loss Gap:   -0.0612 | Acc  Gap:  0.0251\n",
            "  Loss Ratio: 0.9490 | Acc Ratio: 1.0461\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.1116 | Train Acc: 0.5797\n",
            "  Val   Loss: 1.1512 | Val   Acc: 0.5677\n",
            "  Loss Gap:   -0.0396 | Acc  Gap:  0.0120\n",
            "  Loss Ratio: 0.9656 | Acc Ratio: 1.0211\n",
            "Epoch 12/30\n",
            "  Train Loss: 1.0760 | Train Acc: 0.5923\n",
            "  Val   Loss: 1.1620 | Val   Acc: 0.5672\n",
            "  Loss Gap:   -0.0859 | Acc  Gap:  0.0251\n",
            "  Loss Ratio: 0.9260 | Acc Ratio: 1.0442\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 13/30\n",
            "  Train Loss: 1.0408 | Train Acc: 0.6063\n",
            "  Val   Loss: 1.2769 | Val   Acc: 0.5263\n",
            "  Loss Gap:   -0.2361 | Acc  Gap:  0.0800\n",
            "  Loss Ratio: 0.8151 | Acc Ratio: 1.1520\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 14/30\n",
            "  Train Loss: 1.0106 | Train Acc: 0.6173\n",
            "  Val   Loss: 1.2202 | Val   Acc: 0.5395\n",
            "  Loss Gap:   -0.2096 | Acc  Gap:  0.0777\n",
            "  Loss Ratio: 0.8282 | Acc Ratio: 1.1441\n",
            "  Early stopping patience: 3/5\n",
            "Epoch 15/30\n",
            "  Train Loss: 0.9838 | Train Acc: 0.6294\n",
            "  Val   Loss: 1.1746 | Val   Acc: 0.5599\n",
            "  Loss Gap:   -0.1907 | Acc  Gap:  0.0695\n",
            "  Loss Ratio: 0.8376 | Acc Ratio: 1.1242\n",
            "  Early stopping patience: 4/5\n",
            "Epoch 16/30\n",
            "  Train Loss: 0.9598 | Train Acc: 0.6424\n",
            "  Val   Loss: 1.1780 | Val   Acc: 0.5585\n",
            "  Loss Gap:   -0.2181 | Acc  Gap:  0.0839\n",
            "  Loss Ratio: 0.8148 | Acc Ratio: 1.1502\n",
            "  Early stopping patience: 5/5\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▅▄▆▅▅▄▆▅▆▅▆██▇█</td></tr><tr><td>acc_ratio</td><td>▁▆▄▇▆▆▅▇▆▆▆▆████</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>loss_gap</td><td>█▄▆▄▅▅▅▄▄▄▄▃▁▁▂▁</td></tr><tr><td>loss_ratio</td><td>█▅▆▄▅▅▅▄▄▄▄▄▁▁▂▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▄▃▅▆▇▆▇▇██▇▇██</td></tr><tr><td>val_loss</td><td>██▅▅▄▃▂▃▂▂▁▁▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.08388</td></tr><tr><td>acc_ratio</td><td>1.15019</td></tr><tr><td>epoch</td><td>16</td></tr><tr><td>loss_gap</td><td>-0.21815</td></tr><tr><td>loss_ratio</td><td>0.81481</td></tr><tr><td>lr</td><td>0.00337</td></tr><tr><td>train_accuracy</td><td>0.6424</td></tr><tr><td>train_loss</td><td>0.95981</td></tr><tr><td>val_accuracy</td><td>0.55852</td></tr><tr><td>val_loss</td><td>1.17796</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Dropout_0.2205149216_LR_0.003370767_Epochs_30_WD_0.0003962556</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/c4l57229' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/c4l57229</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_122852-c4l57229/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔁 Trial 5/10\n",
            "   ➤ LR=0.0010060383, Dropout=0.4065336964, Weight_decay = 0.000104693\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_122946-smw1rgz2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/smw1rgz2' target=\"_blank\">FER_BatchNorm_Dropout_0.4065336964_LR_0.0010060383_Epochs_30_WD_0.000104693</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/smw1rgz2' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/smw1rgz2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.7402 | Train Acc: 0.2929\n",
            "  Val   Loss: 1.5139 | Val   Acc: 0.4143\n",
            "  Loss Gap:   0.2262 | Acc  Gap:  -0.1214\n",
            "  Loss Ratio: 1.1494 | Acc Ratio: 0.7069\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.5202 | Train Acc: 0.4098\n",
            "  Val   Loss: 1.4361 | Val   Acc: 0.4525\n",
            "  Loss Gap:   0.0841 | Acc  Gap:  -0.0426\n",
            "  Loss Ratio: 1.0586 | Acc Ratio: 0.9058\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.4203 | Train Acc: 0.4567\n",
            "  Val   Loss: 1.3602 | Val   Acc: 0.4735\n",
            "  Loss Gap:   0.0601 | Acc  Gap:  -0.0169\n",
            "  Loss Ratio: 1.0442 | Acc Ratio: 0.9644\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.3564 | Train Acc: 0.4813\n",
            "  Val   Loss: 1.3355 | Val   Acc: 0.4869\n",
            "  Loss Gap:   0.0209 | Acc  Gap:  -0.0057\n",
            "  Loss Ratio: 1.0157 | Acc Ratio: 0.9883\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.3100 | Train Acc: 0.4984\n",
            "  Val   Loss: 1.3361 | Val   Acc: 0.4833\n",
            "  Loss Gap:   -0.0260 | Acc  Gap:  0.0151\n",
            "  Loss Ratio: 0.9805 | Acc Ratio: 1.0312\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.2588 | Train Acc: 0.5179\n",
            "  Val   Loss: 1.2332 | Val   Acc: 0.5326\n",
            "  Loss Gap:   0.0256 | Acc  Gap:  -0.0147\n",
            "  Loss Ratio: 1.0208 | Acc Ratio: 0.9725\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.2243 | Train Acc: 0.5309\n",
            "  Val   Loss: 1.2216 | Val   Acc: 0.5448\n",
            "  Loss Gap:   0.0027 | Acc  Gap:  -0.0139\n",
            "  Loss Ratio: 1.0022 | Acc Ratio: 0.9745\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.1814 | Train Acc: 0.5459\n",
            "  Val   Loss: 1.2263 | Val   Acc: 0.5326\n",
            "  Loss Gap:   -0.0448 | Acc  Gap:  0.0133\n",
            "  Loss Ratio: 0.9634 | Acc Ratio: 1.0250\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.1441 | Train Acc: 0.5581\n",
            "  Val   Loss: 1.2247 | Val   Acc: 0.5408\n",
            "  Loss Gap:   -0.0806 | Acc  Gap:  0.0174\n",
            "  Loss Ratio: 0.9342 | Acc Ratio: 1.0322\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.1040 | Train Acc: 0.5781\n",
            "  Val   Loss: 1.1726 | Val   Acc: 0.5597\n",
            "  Loss Gap:   -0.0685 | Acc  Gap:  0.0184\n",
            "  Loss Ratio: 0.9416 | Acc Ratio: 1.0328\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.0658 | Train Acc: 0.5935\n",
            "  Val   Loss: 1.1832 | Val   Acc: 0.5549\n",
            "  Loss Gap:   -0.1174 | Acc  Gap:  0.0386\n",
            "  Loss Ratio: 0.9008 | Acc Ratio: 1.0696\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 12/30\n",
            "  Train Loss: 1.0268 | Train Acc: 0.6028\n",
            "  Val   Loss: 1.1660 | Val   Acc: 0.5549\n",
            "  Loss Gap:   -0.1392 | Acc  Gap:  0.0479\n",
            "  Loss Ratio: 0.8806 | Acc Ratio: 1.0864\n",
            "Epoch 13/30\n",
            "  Train Loss: 0.9826 | Train Acc: 0.6241\n",
            "  Val   Loss: 1.1649 | Val   Acc: 0.5674\n",
            "  Loss Gap:   -0.1823 | Acc  Gap:  0.0567\n",
            "  Loss Ratio: 0.8435 | Acc Ratio: 1.1000\n",
            "Epoch 14/30\n",
            "  Train Loss: 0.9428 | Train Acc: 0.6400\n",
            "  Val   Loss: 1.1569 | Val   Acc: 0.5765\n",
            "  Loss Gap:   -0.2140 | Acc  Gap:  0.0635\n",
            "  Loss Ratio: 0.8150 | Acc Ratio: 1.1102\n",
            "Epoch 15/30\n",
            "  Train Loss: 0.9066 | Train Acc: 0.6519\n",
            "  Val   Loss: 1.1954 | Val   Acc: 0.5660\n",
            "  Loss Gap:   -0.2888 | Acc  Gap:  0.0859\n",
            "  Loss Ratio: 0.7584 | Acc Ratio: 1.1518\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 16/30\n",
            "  Train Loss: 0.8683 | Train Acc: 0.6694\n",
            "  Val   Loss: 1.2309 | Val   Acc: 0.5526\n",
            "  Loss Gap:   -0.3626 | Acc  Gap:  0.1168\n",
            "  Loss Ratio: 0.7054 | Acc Ratio: 1.2114\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 17/30\n",
            "  Train Loss: 0.8357 | Train Acc: 0.6833\n",
            "  Val   Loss: 1.2287 | Val   Acc: 0.5686\n",
            "  Loss Gap:   -0.3930 | Acc  Gap:  0.1147\n",
            "  Loss Ratio: 0.6802 | Acc Ratio: 1.2017\n",
            "  Early stopping patience: 3/5\n",
            "Epoch 18/30\n",
            "  Train Loss: 0.7951 | Train Acc: 0.6982\n",
            "  Val   Loss: 1.2853 | Val   Acc: 0.5366\n",
            "  Loss Gap:   -0.4902 | Acc  Gap:  0.1616\n",
            "  Loss Ratio: 0.6186 | Acc Ratio: 1.3013\n",
            "  Early stopping patience: 4/5\n",
            "Epoch 19/30\n",
            "  Train Loss: 0.7732 | Train Acc: 0.7095\n",
            "  Val   Loss: 1.2608 | Val   Acc: 0.5615\n",
            "  Loss Gap:   -0.4876 | Acc  Gap:  0.1480\n",
            "  Loss Ratio: 0.6133 | Acc Ratio: 1.2635\n",
            "  Early stopping patience: 5/5\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▃▄▄▄▄▄▄▄▄▅▅▅▆▆▇▇██</td></tr><tr><td>acc_ratio</td><td>▁▃▄▄▅▄▄▅▅▅▅▅▆▆▆▇▇██</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>loss_gap</td><td>█▇▆▆▆▆▆▅▅▅▅▄▄▄▃▂▂▁▁</td></tr><tr><td>loss_ratio</td><td>█▇▇▆▆▆▆▆▅▅▅▄▄▄▃▂▂▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▄▄▆▇▆▆▇▇▇███▇█▆▇</td></tr><tr><td>val_loss</td><td>█▆▅▅▅▂▂▂▂▁▂▁▁▁▂▂▂▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.14798</td></tr><tr><td>acc_ratio</td><td>1.26355</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>loss_gap</td><td>-0.48762</td></tr><tr><td>loss_ratio</td><td>0.61326</td></tr><tr><td>lr</td><td>0.00101</td></tr><tr><td>train_accuracy</td><td>0.70945</td></tr><tr><td>train_loss</td><td>0.77321</td></tr><tr><td>val_accuracy</td><td>0.56148</td></tr><tr><td>val_loss</td><td>1.26083</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Dropout_0.4065336964_LR_0.0010060383_Epochs_30_WD_0.000104693</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/smw1rgz2' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/smw1rgz2</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_122946-smw1rgz2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔁 Trial 6/10\n",
            "   ➤ LR=0.0005977104, Dropout=0.4092261198, Weight_decay = 0.0009644818\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_123051-ii24ghip</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/ii24ghip' target=\"_blank\">FER_BatchNorm_Dropout_0.4092261198_LR_0.0005977104_Epochs_30_WD_0.0009644818</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/ii24ghip' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/ii24ghip</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.6842 | Train Acc: 0.3170\n",
            "  Val   Loss: 1.4866 | Val   Acc: 0.4216\n",
            "  Loss Gap:   0.1977 | Acc  Gap:  -0.1046\n",
            "  Loss Ratio: 1.1330 | Acc Ratio: 0.7519\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.4753 | Train Acc: 0.4280\n",
            "  Val   Loss: 1.3511 | Val   Acc: 0.4720\n",
            "  Loss Gap:   0.1242 | Acc  Gap:  -0.0440\n",
            "  Loss Ratio: 1.0920 | Acc Ratio: 0.9069\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.3761 | Train Acc: 0.4712\n",
            "  Val   Loss: 1.2814 | Val   Acc: 0.5021\n",
            "  Loss Gap:   0.0948 | Acc  Gap:  -0.0309\n",
            "  Loss Ratio: 1.0739 | Acc Ratio: 0.9384\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.3246 | Train Acc: 0.4949\n",
            "  Val   Loss: 1.2479 | Val   Acc: 0.5192\n",
            "  Loss Gap:   0.0767 | Acc  Gap:  -0.0242\n",
            "  Loss Ratio: 1.0614 | Acc Ratio: 0.9533\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.2690 | Train Acc: 0.5139\n",
            "  Val   Loss: 1.2394 | Val   Acc: 0.5249\n",
            "  Loss Gap:   0.0296 | Acc  Gap:  -0.0110\n",
            "  Loss Ratio: 1.0239 | Acc Ratio: 0.9790\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.2223 | Train Acc: 0.5322\n",
            "  Val   Loss: 1.1890 | Val   Acc: 0.5455\n",
            "  Loss Gap:   0.0333 | Acc  Gap:  -0.0133\n",
            "  Loss Ratio: 1.0280 | Acc Ratio: 0.9757\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.1860 | Train Acc: 0.5469\n",
            "  Val   Loss: 1.1611 | Val   Acc: 0.5658\n",
            "  Loss Gap:   0.0248 | Acc  Gap:  -0.0190\n",
            "  Loss Ratio: 1.0214 | Acc Ratio: 0.9665\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.1446 | Train Acc: 0.5639\n",
            "  Val   Loss: 1.1670 | Val   Acc: 0.5566\n",
            "  Loss Gap:   -0.0224 | Acc  Gap:  0.0073\n",
            "  Loss Ratio: 0.9808 | Acc Ratio: 1.0130\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.1125 | Train Acc: 0.5774\n",
            "  Val   Loss: 1.1548 | Val   Acc: 0.5627\n",
            "  Loss Gap:   -0.0423 | Acc  Gap:  0.0147\n",
            "  Loss Ratio: 0.9634 | Acc Ratio: 1.0261\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.0650 | Train Acc: 0.5938\n",
            "  Val   Loss: 1.1288 | Val   Acc: 0.5731\n",
            "  Loss Gap:   -0.0637 | Acc  Gap:  0.0207\n",
            "  Loss Ratio: 0.9435 | Acc Ratio: 1.0361\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.0401 | Train Acc: 0.6033\n",
            "  Val   Loss: 1.1342 | Val   Acc: 0.5778\n",
            "  Loss Gap:   -0.0941 | Acc  Gap:  0.0255\n",
            "  Loss Ratio: 0.9170 | Acc Ratio: 1.0440\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 12/30\n",
            "  Train Loss: 0.9929 | Train Acc: 0.6235\n",
            "  Val   Loss: 1.1450 | Val   Acc: 0.5772\n",
            "  Loss Gap:   -0.1521 | Acc  Gap:  0.0463\n",
            "  Loss Ratio: 0.8671 | Acc Ratio: 1.0802\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 13/30\n",
            "  Train Loss: 0.9531 | Train Acc: 0.6389\n",
            "  Val   Loss: 1.1896 | Val   Acc: 0.5587\n",
            "  Loss Gap:   -0.2365 | Acc  Gap:  0.0802\n",
            "  Loss Ratio: 0.8012 | Acc Ratio: 1.1436\n",
            "  Early stopping patience: 3/5\n",
            "Epoch 14/30\n",
            "  Train Loss: 0.9116 | Train Acc: 0.6546\n",
            "  Val   Loss: 1.2590 | Val   Acc: 0.5414\n",
            "  Loss Gap:   -0.3474 | Acc  Gap:  0.1132\n",
            "  Loss Ratio: 0.7241 | Acc Ratio: 1.2090\n",
            "  Early stopping patience: 4/5\n",
            "Epoch 15/30\n",
            "  Train Loss: 0.8658 | Train Acc: 0.6750\n",
            "  Val   Loss: 1.1487 | Val   Acc: 0.5742\n",
            "  Loss Gap:   -0.2829 | Acc  Gap:  0.1008\n",
            "  Loss Ratio: 0.7537 | Acc Ratio: 1.1756\n",
            "  Early stopping patience: 5/5\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▃▃▄▄▄▄▅▅▅▅▆▇██</td></tr><tr><td>acc_ratio</td><td>▁▃▄▄▄▄▄▅▅▅▅▆▇█▇</td></tr><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>loss_gap</td><td>█▇▇▆▆▆▆▅▅▅▄▄▂▁▂</td></tr><tr><td>loss_ratio</td><td>█▇▇▇▆▆▆▅▅▅▄▃▂▁▂</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▅▆▇▇▇▇███▇▆█</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▂▂▂▁▁▁▂▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.10082</td></tr><tr><td>acc_ratio</td><td>1.17559</td></tr><tr><td>epoch</td><td>15</td></tr><tr><td>loss_gap</td><td>-0.28289</td></tr><tr><td>loss_ratio</td><td>0.75373</td></tr><tr><td>lr</td><td>0.0006</td></tr><tr><td>train_accuracy</td><td>0.67501</td></tr><tr><td>train_loss</td><td>0.86581</td></tr><tr><td>val_accuracy</td><td>0.57419</td></tr><tr><td>val_loss</td><td>1.1487</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Dropout_0.4092261198_LR_0.0005977104_Epochs_30_WD_0.0009644818</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/ii24ghip' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/ii24ghip</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_123051-ii24ghip/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔁 Trial 7/10\n",
            "   ➤ LR=0.0038107659, Dropout=0.4167672049, Weight_decay = 0.000164257\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_123142-gxzrsne0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/gxzrsne0' target=\"_blank\">FER_BatchNorm_Dropout_0.4167672049_LR_0.0038107659_Epochs_30_WD_0.000164257</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/gxzrsne0' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/gxzrsne0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.8303 | Train Acc: 0.2535\n",
            "  Val   Loss: 1.7061 | Val   Acc: 0.3210\n",
            "  Loss Gap:   0.1242 | Acc  Gap:  -0.0674\n",
            "  Loss Ratio: 1.0728 | Acc Ratio: 0.7899\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.7204 | Train Acc: 0.2992\n",
            "  Val   Loss: 1.7288 | Val   Acc: 0.2954\n",
            "  Loss Gap:   -0.0084 | Acc  Gap:  0.0038\n",
            "  Loss Ratio: 0.9951 | Acc Ratio: 1.0130\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.6893 | Train Acc: 0.3128\n",
            "  Val   Loss: 1.6521 | Val   Acc: 0.3339\n",
            "  Loss Gap:   0.0372 | Acc  Gap:  -0.0210\n",
            "  Loss Ratio: 1.0225 | Acc Ratio: 0.9371\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.6646 | Train Acc: 0.3221\n",
            "  Val   Loss: 1.6268 | Val   Acc: 0.3314\n",
            "  Loss Gap:   0.0378 | Acc  Gap:  -0.0093\n",
            "  Loss Ratio: 1.0232 | Acc Ratio: 0.9719\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.6482 | Train Acc: 0.3338\n",
            "  Val   Loss: 1.6144 | Val   Acc: 0.3539\n",
            "  Loss Gap:   0.0339 | Acc  Gap:  -0.0201\n",
            "  Loss Ratio: 1.0210 | Acc Ratio: 0.9433\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.5947 | Train Acc: 0.3603\n",
            "  Val   Loss: 1.5583 | Val   Acc: 0.3617\n",
            "  Loss Gap:   0.0364 | Acc  Gap:  -0.0014\n",
            "  Loss Ratio: 1.0234 | Acc Ratio: 0.9961\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.5211 | Train Acc: 0.3928\n",
            "  Val   Loss: 1.5236 | Val   Acc: 0.3588\n",
            "  Loss Gap:   -0.0025 | Acc  Gap:  0.0341\n",
            "  Loss Ratio: 0.9984 | Acc Ratio: 1.0950\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.4447 | Train Acc: 0.4213\n",
            "  Val   Loss: 1.4039 | Val   Acc: 0.4465\n",
            "  Loss Gap:   0.0408 | Acc  Gap:  -0.0252\n",
            "  Loss Ratio: 1.0291 | Acc Ratio: 0.9436\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.3912 | Train Acc: 0.4461\n",
            "  Val   Loss: 1.3626 | Val   Acc: 0.4643\n",
            "  Loss Gap:   0.0286 | Acc  Gap:  -0.0182\n",
            "  Loss Ratio: 1.0210 | Acc Ratio: 0.9608\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.3403 | Train Acc: 0.4737\n",
            "  Val   Loss: 1.3329 | Val   Acc: 0.4761\n",
            "  Loss Gap:   0.0074 | Acc  Gap:  -0.0024\n",
            "  Loss Ratio: 1.0056 | Acc Ratio: 0.9949\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.2941 | Train Acc: 0.4917\n",
            "  Val   Loss: 1.2846 | Val   Acc: 0.4936\n",
            "  Loss Gap:   0.0095 | Acc  Gap:  -0.0019\n",
            "  Loss Ratio: 1.0074 | Acc Ratio: 0.9963\n",
            "Epoch 12/30\n",
            "  Train Loss: 1.2591 | Train Acc: 0.5144\n",
            "  Val   Loss: 1.3178 | Val   Acc: 0.4896\n",
            "  Loss Gap:   -0.0587 | Acc  Gap:  0.0248\n",
            "  Loss Ratio: 0.9555 | Acc Ratio: 1.0507\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 13/30\n",
            "  Train Loss: 1.2154 | Train Acc: 0.5301\n",
            "  Val   Loss: 1.2358 | Val   Acc: 0.5232\n",
            "  Loss Gap:   -0.0204 | Acc  Gap:  0.0069\n",
            "  Loss Ratio: 0.9835 | Acc Ratio: 1.0132\n",
            "Epoch 14/30\n",
            "  Train Loss: 1.1838 | Train Acc: 0.5462\n",
            "  Val   Loss: 1.2086 | Val   Acc: 0.5301\n",
            "  Loss Gap:   -0.0248 | Acc  Gap:  0.0161\n",
            "  Loss Ratio: 0.9795 | Acc Ratio: 1.0304\n",
            "Epoch 15/30\n",
            "  Train Loss: 1.1560 | Train Acc: 0.5613\n",
            "  Val   Loss: 1.2302 | Val   Acc: 0.5296\n",
            "  Loss Gap:   -0.0742 | Acc  Gap:  0.0317\n",
            "  Loss Ratio: 0.9397 | Acc Ratio: 1.0598\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 16/30\n",
            "  Train Loss: 1.1328 | Train Acc: 0.5704\n",
            "  Val   Loss: 1.1910 | Val   Acc: 0.5488\n",
            "  Loss Gap:   -0.0581 | Acc  Gap:  0.0217\n",
            "  Loss Ratio: 0.9512 | Acc Ratio: 1.0395\n",
            "Epoch 17/30\n",
            "  Train Loss: 1.1008 | Train Acc: 0.5837\n",
            "  Val   Loss: 1.1721 | Val   Acc: 0.5549\n",
            "  Loss Gap:   -0.0712 | Acc  Gap:  0.0288\n",
            "  Loss Ratio: 0.9392 | Acc Ratio: 1.0520\n",
            "Epoch 18/30\n",
            "  Train Loss: 1.0852 | Train Acc: 0.5916\n",
            "  Val   Loss: 1.1973 | Val   Acc: 0.5421\n",
            "  Loss Gap:   -0.1121 | Acc  Gap:  0.0494\n",
            "  Loss Ratio: 0.9064 | Acc Ratio: 1.0912\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 19/30\n",
            "  Train Loss: 1.0562 | Train Acc: 0.6045\n",
            "  Val   Loss: 1.2763 | Val   Acc: 0.5284\n",
            "  Loss Gap:   -0.2202 | Acc  Gap:  0.0761\n",
            "  Loss Ratio: 0.8275 | Acc Ratio: 1.1440\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 20/30\n",
            "  Train Loss: 1.0332 | Train Acc: 0.6172\n",
            "  Val   Loss: 1.1966 | Val   Acc: 0.5428\n",
            "  Loss Gap:   -0.1633 | Acc  Gap:  0.0744\n",
            "  Loss Ratio: 0.8635 | Acc Ratio: 1.1370\n",
            "  Early stopping patience: 3/5\n",
            "Epoch 21/30\n",
            "  Train Loss: 1.0123 | Train Acc: 0.6205\n",
            "  Val   Loss: 1.2961 | Val   Acc: 0.5047\n",
            "  Loss Gap:   -0.2837 | Acc  Gap:  0.1158\n",
            "  Loss Ratio: 0.7811 | Acc Ratio: 1.2293\n",
            "  Early stopping patience: 4/5\n",
            "Epoch 22/30\n",
            "  Train Loss: 1.0011 | Train Acc: 0.6266\n",
            "  Val   Loss: 1.2049 | Val   Acc: 0.5521\n",
            "  Loss Gap:   -0.2037 | Acc  Gap:  0.0745\n",
            "  Loss Ratio: 0.8309 | Acc Ratio: 1.1350\n",
            "  Early stopping patience: 5/5\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▄▃▃▃▄▅▃▃▃▄▅▄▄▅▄▅▅▆▆█▆</td></tr><tr><td>acc_ratio</td><td>▁▅▃▄▃▄▆▃▄▄▄▅▅▅▅▅▅▆▇▇█▆</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>loss_gap</td><td>█▆▇▇▆▆▆▇▆▆▆▅▆▅▅▅▅▄▂▃▁▂</td></tr><tr><td>loss_ratio</td><td>█▆▇▇▇▇▆▇▇▆▆▅▆▆▅▅▅▄▂▃▁▂</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▂▂▃▃▄▄▅▅▅▆▆▆▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▇▇▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▂▁▂▂▃▃▃▅▆▆▆▆▇▇▇███▇█▇█</td></tr><tr><td>val_loss</td><td>██▇▇▇▆▅▄▃▃▂▃▂▁▂▁▁▁▂▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.07452</td></tr><tr><td>acc_ratio</td><td>1.13499</td></tr><tr><td>epoch</td><td>22</td></tr><tr><td>loss_gap</td><td>-0.20375</td></tr><tr><td>loss_ratio</td><td>0.8309</td></tr><tr><td>lr</td><td>0.00381</td></tr><tr><td>train_accuracy</td><td>0.62659</td></tr><tr><td>train_loss</td><td>1.00112</td></tr><tr><td>val_accuracy</td><td>0.55207</td></tr><tr><td>val_loss</td><td>1.20487</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Dropout_0.4167672049_LR_0.0038107659_Epochs_30_WD_0.000164257</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/gxzrsne0' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/gxzrsne0</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_123142-gxzrsne0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔁 Trial 8/10\n",
            "   ➤ LR=0.0040638812, Dropout=0.2832641658, Weight_decay = 0.0007435468\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_123256-ts36u95n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/ts36u95n' target=\"_blank\">FER_BatchNorm_Dropout_0.2832641658_LR_0.0040638812_Epochs_30_WD_0.0007435468</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/ts36u95n' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/ts36u95n</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.8182 | Train Acc: 0.2555\n",
            "  Val   Loss: 1.6785 | Val   Acc: 0.3060\n",
            "  Loss Gap:   0.1397 | Acc  Gap:  -0.0505\n",
            "  Loss Ratio: 1.0832 | Acc Ratio: 0.8351\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.6078 | Train Acc: 0.3577\n",
            "  Val   Loss: 1.4836 | Val   Acc: 0.4289\n",
            "  Loss Gap:   0.1242 | Acc  Gap:  -0.0713\n",
            "  Loss Ratio: 1.0837 | Acc Ratio: 0.8339\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.4788 | Train Acc: 0.4224\n",
            "  Val   Loss: 1.4820 | Val   Acc: 0.4253\n",
            "  Loss Gap:   -0.0032 | Acc  Gap:  -0.0029\n",
            "  Loss Ratio: 0.9978 | Acc Ratio: 0.9932\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.3882 | Train Acc: 0.4627\n",
            "  Val   Loss: 1.4452 | Val   Acc: 0.4579\n",
            "  Loss Gap:   -0.0571 | Acc  Gap:  0.0049\n",
            "  Loss Ratio: 0.9605 | Acc Ratio: 1.0106\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.3271 | Train Acc: 0.4898\n",
            "  Val   Loss: 1.5611 | Val   Acc: 0.3588\n",
            "  Loss Gap:   -0.2341 | Acc  Gap:  0.1310\n",
            "  Loss Ratio: 0.8501 | Acc Ratio: 1.3652\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.2894 | Train Acc: 0.5083\n",
            "  Val   Loss: 1.3313 | Val   Acc: 0.5014\n",
            "  Loss Gap:   -0.0419 | Acc  Gap:  0.0069\n",
            "  Loss Ratio: 0.9685 | Acc Ratio: 1.0138\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.2718 | Train Acc: 0.5142\n",
            "  Val   Loss: 1.2675 | Val   Acc: 0.5157\n",
            "  Loss Gap:   0.0042 | Acc  Gap:  -0.0015\n",
            "  Loss Ratio: 1.0033 | Acc Ratio: 0.9971\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.2436 | Train Acc: 0.5299\n",
            "  Val   Loss: 1.3777 | Val   Acc: 0.4753\n",
            "  Loss Gap:   -0.1341 | Acc  Gap:  0.0547\n",
            "  Loss Ratio: 0.9027 | Acc Ratio: 1.1150\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.2246 | Train Acc: 0.5359\n",
            "  Val   Loss: 1.2397 | Val   Acc: 0.5369\n",
            "  Loss Gap:   -0.0151 | Acc  Gap:  -0.0011\n",
            "  Loss Ratio: 0.9878 | Acc Ratio: 0.9980\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.2044 | Train Acc: 0.5450\n",
            "  Val   Loss: 1.2915 | Val   Acc: 0.5068\n",
            "  Loss Gap:   -0.0871 | Acc  Gap:  0.0382\n",
            "  Loss Ratio: 0.9326 | Acc Ratio: 1.0754\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.1963 | Train Acc: 0.5524\n",
            "  Val   Loss: 1.2112 | Val   Acc: 0.5348\n",
            "  Loss Gap:   -0.0150 | Acc  Gap:  0.0176\n",
            "  Loss Ratio: 0.9876 | Acc Ratio: 1.0329\n",
            "Epoch 12/30\n",
            "  Train Loss: 1.1748 | Train Acc: 0.5571\n",
            "  Val   Loss: 1.2369 | Val   Acc: 0.5376\n",
            "  Loss Gap:   -0.0621 | Acc  Gap:  0.0194\n",
            "  Loss Ratio: 0.9498 | Acc Ratio: 1.0362\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 13/30\n",
            "  Train Loss: 1.1620 | Train Acc: 0.5656\n",
            "  Val   Loss: 1.3968 | Val   Acc: 0.4624\n",
            "  Loss Gap:   -0.2348 | Acc  Gap:  0.1032\n",
            "  Loss Ratio: 0.8319 | Acc Ratio: 1.2231\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 14/30\n",
            "  Train Loss: 1.1581 | Train Acc: 0.5677\n",
            "  Val   Loss: 1.2747 | Val   Acc: 0.5158\n",
            "  Loss Gap:   -0.1165 | Acc  Gap:  0.0518\n",
            "  Loss Ratio: 0.9086 | Acc Ratio: 1.1005\n",
            "  Early stopping patience: 3/5\n",
            "Epoch 15/30\n",
            "  Train Loss: 1.1429 | Train Acc: 0.5771\n",
            "  Val   Loss: 1.2698 | Val   Acc: 0.5155\n",
            "  Loss Gap:   -0.1269 | Acc  Gap:  0.0616\n",
            "  Loss Ratio: 0.9001 | Acc Ratio: 1.1196\n",
            "  Early stopping patience: 4/5\n",
            "Epoch 16/30\n",
            "  Train Loss: 1.1378 | Train Acc: 0.5771\n",
            "  Val   Loss: 1.2686 | Val   Acc: 0.5228\n",
            "  Loss Gap:   -0.1307 | Acc  Gap:  0.0543\n",
            "  Loss Ratio: 0.8969 | Acc Ratio: 1.1039\n",
            "  Early stopping patience: 5/5\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▂▁▃▄█▄▃▅▃▅▄▄▇▅▆▅</td></tr><tr><td>acc_ratio</td><td>▁▁▃▃█▃▃▅▃▄▄▄▆▅▅▅</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>loss_gap</td><td>██▅▄▁▅▅▃▅▄▅▄▁▃▃▃</td></tr><tr><td>loss_ratio</td><td>██▆▅▂▅▆▃▅▄▅▄▁▃▃▃</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▅▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▅▆▃▇▇▆█▇██▆▇▇█</td></tr><tr><td>val_loss</td><td>█▅▅▅▆▃▂▃▁▂▁▁▄▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.05432</td></tr><tr><td>acc_ratio</td><td>1.1039</td></tr><tr><td>epoch</td><td>16</td></tr><tr><td>loss_gap</td><td>-0.13074</td></tr><tr><td>loss_ratio</td><td>0.89694</td></tr><tr><td>lr</td><td>0.00406</td></tr><tr><td>train_accuracy</td><td>0.57713</td></tr><tr><td>train_loss</td><td>1.13782</td></tr><tr><td>val_accuracy</td><td>0.52281</td></tr><tr><td>val_loss</td><td>1.26856</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Dropout_0.2832641658_LR_0.0040638812_Epochs_30_WD_0.0007435468</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/ts36u95n' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/ts36u95n</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_123256-ts36u95n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔁 Trial 9/10\n",
            "   ➤ LR=0.0033434952, Dropout=0.3951991415, Weight_decay = 0.0008628863\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_123351-msvneqor</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/msvneqor' target=\"_blank\">FER_BatchNorm_Dropout_0.3951991415_LR_0.0033434952_Epochs_30_WD_0.0008628863</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/msvneqor' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/msvneqor</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.8135 | Train Acc: 0.2559\n",
            "  Val   Loss: 1.7292 | Val   Acc: 0.3110\n",
            "  Loss Gap:   0.0843 | Acc  Gap:  -0.0552\n",
            "  Loss Ratio: 1.0487 | Acc Ratio: 0.8227\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.6593 | Train Acc: 0.3218\n",
            "  Val   Loss: 1.6383 | Val   Acc: 0.3469\n",
            "  Loss Gap:   0.0210 | Acc  Gap:  -0.0251\n",
            "  Loss Ratio: 1.0128 | Acc Ratio: 0.9276\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.5527 | Train Acc: 0.3777\n",
            "  Val   Loss: 1.5472 | Val   Acc: 0.4051\n",
            "  Loss Gap:   0.0054 | Acc  Gap:  -0.0274\n",
            "  Loss Ratio: 1.0035 | Acc Ratio: 0.9323\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.4699 | Train Acc: 0.4155\n",
            "  Val   Loss: 1.5607 | Val   Acc: 0.4065\n",
            "  Loss Gap:   -0.0908 | Acc  Gap:  0.0090\n",
            "  Loss Ratio: 0.9418 | Acc Ratio: 1.0222\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.3912 | Train Acc: 0.4572\n",
            "  Val   Loss: 1.4772 | Val   Acc: 0.4387\n",
            "  Loss Gap:   -0.0861 | Acc  Gap:  0.0185\n",
            "  Loss Ratio: 0.9417 | Acc Ratio: 1.0422\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.3439 | Train Acc: 0.4812\n",
            "  Val   Loss: 1.3093 | Val   Acc: 0.4979\n",
            "  Loss Gap:   0.0346 | Acc  Gap:  -0.0167\n",
            "  Loss Ratio: 1.0264 | Acc Ratio: 0.9665\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.3065 | Train Acc: 0.4926\n",
            "  Val   Loss: 1.3080 | Val   Acc: 0.5164\n",
            "  Loss Gap:   -0.0015 | Acc  Gap:  -0.0238\n",
            "  Loss Ratio: 0.9989 | Acc Ratio: 0.9539\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.2769 | Train Acc: 0.5160\n",
            "  Val   Loss: 1.3236 | Val   Acc: 0.5030\n",
            "  Loss Gap:   -0.0467 | Acc  Gap:  0.0130\n",
            "  Loss Ratio: 0.9647 | Acc Ratio: 1.0259\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.2460 | Train Acc: 0.5258\n",
            "  Val   Loss: 1.3202 | Val   Acc: 0.5099\n",
            "  Loss Gap:   -0.0742 | Acc  Gap:  0.0159\n",
            "  Loss Ratio: 0.9438 | Acc Ratio: 1.0311\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.2287 | Train Acc: 0.5349\n",
            "  Val   Loss: 1.2519 | Val   Acc: 0.5202\n",
            "  Loss Gap:   -0.0231 | Acc  Gap:  0.0147\n",
            "  Loss Ratio: 0.9815 | Acc Ratio: 1.0283\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.2066 | Train Acc: 0.5446\n",
            "  Val   Loss: 1.2861 | Val   Acc: 0.5129\n",
            "  Loss Gap:   -0.0795 | Acc  Gap:  0.0317\n",
            "  Loss Ratio: 0.9382 | Acc Ratio: 1.0618\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 12/30\n",
            "  Train Loss: 1.1952 | Train Acc: 0.5492\n",
            "  Val   Loss: 1.2778 | Val   Acc: 0.5157\n",
            "  Loss Gap:   -0.0826 | Acc  Gap:  0.0335\n",
            "  Loss Ratio: 0.9354 | Acc Ratio: 1.0650\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 13/30\n",
            "  Train Loss: 1.1815 | Train Acc: 0.5597\n",
            "  Val   Loss: 1.2147 | Val   Acc: 0.5430\n",
            "  Loss Gap:   -0.0331 | Acc  Gap:  0.0167\n",
            "  Loss Ratio: 0.9727 | Acc Ratio: 1.0307\n",
            "Epoch 14/30\n",
            "  Train Loss: 1.1704 | Train Acc: 0.5635\n",
            "  Val   Loss: 1.2566 | Val   Acc: 0.5261\n",
            "  Loss Gap:   -0.0862 | Acc  Gap:  0.0373\n",
            "  Loss Ratio: 0.9314 | Acc Ratio: 1.0710\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 15/30\n",
            "  Train Loss: 1.1594 | Train Acc: 0.5649\n",
            "  Val   Loss: 1.2129 | Val   Acc: 0.5449\n",
            "  Loss Gap:   -0.0535 | Acc  Gap:  0.0200\n",
            "  Loss Ratio: 0.9559 | Acc Ratio: 1.0366\n",
            "Epoch 16/30\n",
            "  Train Loss: 1.1533 | Train Acc: 0.5666\n",
            "  Val   Loss: 1.2084 | Val   Acc: 0.5387\n",
            "  Loss Gap:   -0.0550 | Acc  Gap:  0.0279\n",
            "  Loss Ratio: 0.9545 | Acc Ratio: 1.0518\n",
            "Epoch 17/30\n",
            "  Train Loss: 1.1420 | Train Acc: 0.5759\n",
            "  Val   Loss: 1.1679 | Val   Acc: 0.5484\n",
            "  Loss Gap:   -0.0259 | Acc  Gap:  0.0275\n",
            "  Loss Ratio: 0.9778 | Acc Ratio: 1.0501\n",
            "Epoch 18/30\n",
            "  Train Loss: 1.1286 | Train Acc: 0.5816\n",
            "  Val   Loss: 1.2349 | Val   Acc: 0.5294\n",
            "  Loss Gap:   -0.1063 | Acc  Gap:  0.0521\n",
            "  Loss Ratio: 0.9139 | Acc Ratio: 1.0985\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 19/30\n",
            "  Train Loss: 1.1178 | Train Acc: 0.5852\n",
            "  Val   Loss: 1.1954 | Val   Acc: 0.5411\n",
            "  Loss Gap:   -0.0777 | Acc  Gap:  0.0441\n",
            "  Loss Ratio: 0.9350 | Acc Ratio: 1.0815\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 20/30\n",
            "  Train Loss: 1.1106 | Train Acc: 0.5875\n",
            "  Val   Loss: 1.2242 | Val   Acc: 0.5463\n",
            "  Loss Gap:   -0.1137 | Acc  Gap:  0.0412\n",
            "  Loss Ratio: 0.9072 | Acc Ratio: 1.0754\n",
            "  Early stopping patience: 3/5\n",
            "Epoch 21/30\n",
            "  Train Loss: 1.1044 | Train Acc: 0.5910\n",
            "  Val   Loss: 1.2015 | Val   Acc: 0.5435\n",
            "  Loss Gap:   -0.0972 | Acc  Gap:  0.0474\n",
            "  Loss Ratio: 0.9191 | Acc Ratio: 1.0873\n",
            "  Early stopping patience: 4/5\n",
            "Epoch 22/30\n",
            "  Train Loss: 1.0938 | Train Acc: 0.5928\n",
            "  Val   Loss: 1.2335 | Val   Acc: 0.5376\n",
            "  Loss Gap:   -0.1397 | Acc  Gap:  0.0552\n",
            "  Loss Ratio: 0.8867 | Acc Ratio: 1.1027\n",
            "  Early stopping patience: 5/5\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▃▃▅▆▃▃▅▆▅▇▇▆▇▆▆▆█▇▇██</td></tr><tr><td>acc_ratio</td><td>▁▄▄▆▆▅▄▆▆▆▇▇▆▇▆▇▇█▇▇██</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>loss_gap</td><td>█▆▆▃▃▆▅▄▃▅▃▃▄▃▄▄▅▂▃▂▂▁</td></tr><tr><td>loss_ratio</td><td>█▆▆▃▃▇▆▄▃▅▃▃▅▃▄▄▅▂▃▂▂▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▄▄▅▆▆▆▇▇▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>█▇▅▅▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▂▄▄▅▇▇▇▇▇▇▇█▇███▇████</td></tr><tr><td>val_loss</td><td>█▇▆▆▅▃▃▃▃▂▂▂▂▂▂▂▁▂▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.05519</td></tr><tr><td>acc_ratio</td><td>1.10266</td></tr><tr><td>epoch</td><td>22</td></tr><tr><td>loss_gap</td><td>-0.1397</td></tr><tr><td>loss_ratio</td><td>0.88675</td></tr><tr><td>lr</td><td>0.00334</td></tr><tr><td>train_accuracy</td><td>0.59281</td></tr><tr><td>train_loss</td><td>1.09383</td></tr><tr><td>val_accuracy</td><td>0.53762</td></tr><tr><td>val_loss</td><td>1.23352</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Dropout_0.3951991415_LR_0.0033434952_Epochs_30_WD_0.0008628863</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/msvneqor' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/msvneqor</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_123351-msvneqor/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔁 Trial 10/10\n",
            "   ➤ LR=0.0024718449, Dropout=0.4914307678, Weight_decay = 0.0007120901\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_123505-rq37wana</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/rq37wana' target=\"_blank\">FER_BatchNorm_Dropout_0.4914307678_LR_0.0024718449_Epochs_30_WD_0.0007120901</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/rq37wana' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/rq37wana</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.8224 | Train Acc: 0.2462\n",
            "  Val   Loss: 1.7575 | Val   Acc: 0.2541\n",
            "  Loss Gap:   0.0649 | Acc  Gap:  -0.0079\n",
            "  Loss Ratio: 1.0369 | Acc Ratio: 0.9689\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.6597 | Train Acc: 0.3272\n",
            "  Val   Loss: 1.5840 | Val   Acc: 0.3972\n",
            "  Loss Gap:   0.0757 | Acc  Gap:  -0.0701\n",
            "  Loss Ratio: 1.0478 | Acc Ratio: 0.8236\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.5535 | Train Acc: 0.3838\n",
            "  Val   Loss: 1.5159 | Val   Acc: 0.4096\n",
            "  Loss Gap:   0.0376 | Acc  Gap:  -0.0258\n",
            "  Loss Ratio: 1.0248 | Acc Ratio: 0.9370\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.4821 | Train Acc: 0.4114\n",
            "  Val   Loss: 1.4365 | Val   Acc: 0.4528\n",
            "  Loss Gap:   0.0456 | Acc  Gap:  -0.0414\n",
            "  Loss Ratio: 1.0318 | Acc Ratio: 0.9085\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.4336 | Train Acc: 0.4328\n",
            "  Val   Loss: 1.3653 | Val   Acc: 0.4862\n",
            "  Loss Gap:   0.0682 | Acc  Gap:  -0.0534\n",
            "  Loss Ratio: 1.0500 | Acc Ratio: 0.8901\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.3890 | Train Acc: 0.4608\n",
            "  Val   Loss: 1.3603 | Val   Acc: 0.4843\n",
            "  Loss Gap:   0.0287 | Acc  Gap:  -0.0235\n",
            "  Loss Ratio: 1.0211 | Acc Ratio: 0.9515\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.3341 | Train Acc: 0.4829\n",
            "  Val   Loss: 1.4007 | Val   Acc: 0.4767\n",
            "  Loss Gap:   -0.0666 | Acc  Gap:  0.0062\n",
            "  Loss Ratio: 0.9525 | Acc Ratio: 1.0131\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.2960 | Train Acc: 0.5008\n",
            "  Val   Loss: 1.3634 | Val   Acc: 0.4671\n",
            "  Loss Gap:   -0.0675 | Acc  Gap:  0.0338\n",
            "  Loss Ratio: 0.9505 | Acc Ratio: 1.0723\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.2676 | Train Acc: 0.5183\n",
            "  Val   Loss: 1.2852 | Val   Acc: 0.5129\n",
            "  Loss Gap:   -0.0175 | Acc  Gap:  0.0054\n",
            "  Loss Ratio: 0.9864 | Acc Ratio: 1.0106\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.2316 | Train Acc: 0.5305\n",
            "  Val   Loss: 1.2240 | Val   Acc: 0.5413\n",
            "  Loss Gap:   0.0076 | Acc  Gap:  -0.0108\n",
            "  Loss Ratio: 1.0062 | Acc Ratio: 0.9800\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.2085 | Train Acc: 0.5400\n",
            "  Val   Loss: 1.2087 | Val   Acc: 0.5263\n",
            "  Loss Gap:   -0.0002 | Acc  Gap:  0.0137\n",
            "  Loss Ratio: 0.9998 | Acc Ratio: 1.0261\n",
            "Epoch 12/30\n",
            "  Train Loss: 1.1804 | Train Acc: 0.5505\n",
            "  Val   Loss: 1.3171 | Val   Acc: 0.4782\n",
            "  Loss Gap:   -0.1367 | Acc  Gap:  0.0723\n",
            "  Loss Ratio: 0.8962 | Acc Ratio: 1.1511\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 13/30\n",
            "  Train Loss: 1.1602 | Train Acc: 0.5607\n",
            "  Val   Loss: 1.2057 | Val   Acc: 0.5364\n",
            "  Loss Gap:   -0.0455 | Acc  Gap:  0.0243\n",
            "  Loss Ratio: 0.9623 | Acc Ratio: 1.0453\n",
            "Epoch 14/30\n",
            "  Train Loss: 1.1411 | Train Acc: 0.5662\n",
            "  Val   Loss: 1.2174 | Val   Acc: 0.5359\n",
            "  Loss Gap:   -0.0763 | Acc  Gap:  0.0303\n",
            "  Loss Ratio: 0.9373 | Acc Ratio: 1.0566\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 15/30\n",
            "  Train Loss: 1.1233 | Train Acc: 0.5762\n",
            "  Val   Loss: 1.2462 | Val   Acc: 0.5200\n",
            "  Loss Gap:   -0.1228 | Acc  Gap:  0.0561\n",
            "  Loss Ratio: 0.9014 | Acc Ratio: 1.1080\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 16/30\n",
            "  Train Loss: 1.1016 | Train Acc: 0.5828\n",
            "  Val   Loss: 1.1669 | Val   Acc: 0.5540\n",
            "  Loss Gap:   -0.0654 | Acc  Gap:  0.0288\n",
            "  Loss Ratio: 0.9440 | Acc Ratio: 1.0520\n",
            "Epoch 17/30\n",
            "  Train Loss: 1.0912 | Train Acc: 0.5889\n",
            "  Val   Loss: 1.1876 | Val   Acc: 0.5486\n",
            "  Loss Gap:   -0.0965 | Acc  Gap:  0.0403\n",
            "  Loss Ratio: 0.9188 | Acc Ratio: 1.0735\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 18/30\n",
            "  Train Loss: 1.0713 | Train Acc: 0.5987\n",
            "  Val   Loss: 1.1600 | Val   Acc: 0.5580\n",
            "  Loss Gap:   -0.0887 | Acc  Gap:  0.0407\n",
            "  Loss Ratio: 0.9235 | Acc Ratio: 1.0730\n",
            "Epoch 19/30\n",
            "  Train Loss: 1.0583 | Train Acc: 0.6002\n",
            "  Val   Loss: 1.1897 | Val   Acc: 0.5458\n",
            "  Loss Gap:   -0.1314 | Acc  Gap:  0.0544\n",
            "  Loss Ratio: 0.8896 | Acc Ratio: 1.0996\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 20/30\n",
            "  Train Loss: 1.0399 | Train Acc: 0.6092\n",
            "  Val   Loss: 1.1631 | Val   Acc: 0.5625\n",
            "  Loss Gap:   -0.1232 | Acc  Gap:  0.0467\n",
            "  Loss Ratio: 0.8941 | Acc Ratio: 1.0830\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 21/30\n",
            "  Train Loss: 1.0191 | Train Acc: 0.6211\n",
            "  Val   Loss: 1.1715 | Val   Acc: 0.5655\n",
            "  Loss Gap:   -0.1524 | Acc  Gap:  0.0556\n",
            "  Loss Ratio: 0.8699 | Acc Ratio: 1.0984\n",
            "  Early stopping patience: 3/5\n",
            "Epoch 22/30\n",
            "  Train Loss: 1.0136 | Train Acc: 0.6247\n",
            "  Val   Loss: 1.2359 | Val   Acc: 0.5298\n",
            "  Loss Gap:   -0.2224 | Acc  Gap:  0.0949\n",
            "  Loss Ratio: 0.8201 | Acc Ratio: 1.1791\n",
            "  Early stopping patience: 4/5\n",
            "Epoch 23/30\n",
            "  Train Loss: 0.9958 | Train Acc: 0.6302\n",
            "  Val   Loss: 1.1785 | Val   Acc: 0.5669\n",
            "  Loss Gap:   -0.1826 | Acc  Gap:  0.0633\n",
            "  Loss Ratio: 0.8450 | Acc Ratio: 1.1116\n",
            "  Early stopping patience: 5/5\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▄▁▃▂▂▃▄▅▄▄▅▇▅▅▆▅▆▆▆▆▆█▇</td></tr><tr><td>acc_ratio</td><td>▄▁▃▃▂▄▅▆▅▄▅▇▅▆▇▅▆▆▆▆▆█▇</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>loss_gap</td><td>██▇▇█▇▅▅▆▆▆▃▅▄▃▅▄▄▃▃▃▁▂</td></tr><tr><td>loss_ratio</td><td>██▇▇█▇▅▅▆▇▆▃▅▅▃▅▄▄▃▃▃▁▂</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▄▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▄▅▆▆▆▆▇▇▇▆▇▇▇██████▇█</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▄▃▂▂▂▃▂▂▂▁▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.06329</td></tr><tr><td>acc_ratio</td><td>1.11165</td></tr><tr><td>epoch</td><td>23</td></tr><tr><td>loss_gap</td><td>-0.18265</td></tr><tr><td>loss_ratio</td><td>0.84501</td></tr><tr><td>lr</td><td>0.00247</td></tr><tr><td>train_accuracy</td><td>0.63017</td></tr><tr><td>train_loss</td><td>0.99581</td></tr><tr><td>val_accuracy</td><td>0.56688</td></tr><tr><td>val_loss</td><td>1.17846</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Dropout_0.4914307678_LR_0.0024718449_Epochs_30_WD_0.0007120901</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/rq37wana' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/rq37wana</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_123505-rq37wana/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🏆 Best Model Found:\n",
            "   ➤ Val Acc: 0.5742\n",
            "   ➤ LR: 0.0005977104, Dropout: 0.4092261198, Patience: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 10 - final cnn"
      ],
      "metadata": {
        "id": "y5vRuGwE-Ifx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "import numpy as np\n",
        "\n",
        "class FERCNN_With_BatchNorm(nn.Module):\n",
        "    def __init__(self, num_classes=7, dropout_rate=0.3):\n",
        "        super(FERCNN_With_BatchNorm, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Calculate flattened size dynamically\n",
        "        dummy_input = torch.zeros(1, 1, 48, 48)\n",
        "        with torch.no_grad():\n",
        "            x = self.pool(torch.relu(self.bn1(self.conv1(dummy_input))))\n",
        "            x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "            x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "            flattened_size = x.view(1, -1).shape[1]\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.dropout(torch.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ey-L2bPK-Lk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 10 - Try different model architectures"
      ],
      "metadata": {
        "id": "9B5dNmhp___a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "\n",
        "class FERCNN_With_BatchNorm(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes=7,\n",
        "                 channels=[32, 64, 128],\n",
        "                 kernel_sizes=3,\n",
        "                 strides=1,\n",
        "                 paddings=1,\n",
        "                 dropout_rate=0.3):\n",
        "        super(FERCNN_With_BatchNorm, self).__init__()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "        # Make sure kernel_sizes, strides, paddings are lists with length 3\n",
        "        if isinstance(kernel_sizes, int):\n",
        "            kernel_sizes = [kernel_sizes] * 3\n",
        "        if isinstance(strides, int):\n",
        "            strides = [strides] * 3\n",
        "        if isinstance(paddings, int):\n",
        "            paddings = [paddings] * 3\n",
        "\n",
        "        # Conv layers and batch norms\n",
        "        self.conv1 = nn.Conv2d(1, channels[0], kernel_size=kernel_sizes[0], stride=strides[0], padding=paddings[0])\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])\n",
        "        self.conv2 = nn.Conv2d(channels[0], channels[1], kernel_size=kernel_sizes[1], stride=strides[1], padding=paddings[1])\n",
        "        self.bn2 = nn.BatchNorm2d(channels[1])\n",
        "        self.conv3 = nn.Conv2d(channels[1], channels[2], kernel_size=kernel_sizes[2], stride=strides[2], padding=paddings[2])\n",
        "        self.bn3 = nn.BatchNorm2d(channels[2])\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Calculate flattened size dynamically\n",
        "        dummy_input = torch.zeros(1, 1, 48, 48)\n",
        "        with torch.no_grad():\n",
        "            x = self.pool(torch.relu(self.bn1(self.conv1(dummy_input))))\n",
        "            x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "            x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "            flattened_size = x.view(1, -1).shape[1]\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(x))))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(torch.relu(self.fc1(x)))\n",
        "        x = self.dropout(torch.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7FByFuqpAE06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_with_batchnorm(x_train, y_train, x_val, y_val,\n",
        "                               epochs=20,\n",
        "                               lr=0.001,\n",
        "                               dropout_rate=0.3,\n",
        "                               weight_decay=1e-5,\n",
        "                               patience=5,\n",
        "                               batch_size=64,\n",
        "                               channels=[32, 64, 128],\n",
        "                               kernel_sizes=3,\n",
        "                               strides=1,\n",
        "                               paddings=1):\n",
        "    run = wandb.init(\n",
        "        project=\"ML_4\",\n",
        "        entity=\"nkhar21-student\",\n",
        "        name=f\"FER_BatchNorm_Channels_{channels}_Dropout_{dropout_rate}_LR_{lr}_Epochs_{epochs}_WD_{weight_decay}\",\n",
        "        config={\n",
        "            \"dropout_rate\": dropout_rate,\n",
        "            \"pool_kernel\": (2, 2),\n",
        "            \"epochs\": epochs,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"early_stopping_patience\": patience,\n",
        "            \"channels\": channels,\n",
        "            \"kernel_sizes\": kernel_sizes,\n",
        "            \"strides\": strides,\n",
        "            \"paddings\": paddings,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = FERCNN_With_BatchNorm(\n",
        "        num_classes=7,\n",
        "        dropout_rate=dropout_rate,\n",
        "        channels=channels,\n",
        "        kernel_sizes=kernel_sizes,\n",
        "        strides=strides,\n",
        "        paddings=paddings\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience, verbose=True)\n",
        "    wandb.watch(model)\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = evaluate_model(\n",
        "            x_train, y_train, model, loss_fn, device,\n",
        "            train_mode=True, optimizer=optimizer, batch_size=batch_size\n",
        "        )\n",
        "        val_loss, val_acc = evaluate_model(\n",
        "            x_val, y_val, model, loss_fn, device,\n",
        "            train_mode=False, optimizer=None, batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        loss_gap = train_loss - val_loss\n",
        "        acc_gap = train_acc - val_acc\n",
        "        loss_ratio = train_loss / val_loss if val_loss != 0 else float(\"inf\")\n",
        "        acc_ratio = train_acc / val_acc if val_acc != 0 else float(\"inf\")\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"  Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
        "        print(f\"  Loss Gap:   {loss_gap:.4f} | Acc  Gap:  {acc_gap:.4f}\")\n",
        "        print(f\"  Loss Ratio: {loss_ratio:.4f} | Acc Ratio: {acc_ratio:.4f}\")\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_accuracy\": val_acc,\n",
        "            \"loss_gap\": loss_gap,\n",
        "            \"acc_gap\": acc_gap,\n",
        "            \"loss_ratio\": loss_ratio,\n",
        "            \"acc_ratio\": acc_ratio,\n",
        "            \"lr\": optimizer.param_groups[0][\"lr\"]\n",
        "        })\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            print(f\"  Early stopping patience: {epochs_without_improvement}/{patience}\")\n",
        "            if epochs_without_improvement >= patience:\n",
        "                print(\"  ⛔ Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    run.finish()\n",
        "    return {\n",
        "        \"val_accuracy\": val_acc,\n",
        "        \"train_accuracy\": train_acc,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"dropout_rate\": dropout_rate,\n",
        "        \"lr\": lr,\n",
        "        \"patience\": patience\n",
        "    }"
      ],
      "metadata": {
        "id": "TtNomBc7AF_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_with_batchnorm(\n",
        "    x_train, y_train, x_val, y_val,\n",
        "    epochs=30,\n",
        "    lr=0.0015,\n",
        "    dropout_rate=0.25,\n",
        "    weight_decay=1e-5,\n",
        "    patience=5,\n",
        "    batch_size=128,\n",
        "    channels=[32, 64, 256],      # custom channels\n",
        "    kernel_sizes=[3, 5, 3],      # custom kernel sizes\n",
        "    strides=[1, 2, 1],           # custom strides\n",
        "    paddings=[1, 2, 1]           # custom paddings\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KH7pqmTzAKfd",
        "outputId": "26758337-4c44-4882-eed8-377b52c3c83a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_124931-1nz20yst</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/1nz20yst' target=\"_blank\">FER_BatchNorm_Channels_[32, 64, 256]_Dropout_0.25_LR_0.0015_Epochs_30_WD_1e-05</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/1nz20yst' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/1nz20yst</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.7007 | Train Acc: 0.3081\n",
            "  Val   Loss: 1.6483 | Val   Acc: 0.3514\n",
            "  Loss Gap:   0.0524 | Acc  Gap:  -0.0434\n",
            "  Loss Ratio: 1.0318 | Acc Ratio: 0.8766\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.4589 | Train Acc: 0.4391\n",
            "  Val   Loss: 1.4099 | Val   Acc: 0.4580\n",
            "  Loss Gap:   0.0490 | Acc  Gap:  -0.0189\n",
            "  Loss Ratio: 1.0348 | Acc Ratio: 0.9587\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.3336 | Train Acc: 0.4898\n",
            "  Val   Loss: 1.4074 | Val   Acc: 0.4619\n",
            "  Loss Gap:   -0.0739 | Acc  Gap:  0.0280\n",
            "  Loss Ratio: 0.9475 | Acc Ratio: 1.0606\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.2514 | Train Acc: 0.5231\n",
            "  Val   Loss: 1.2858 | Val   Acc: 0.5084\n",
            "  Loss Gap:   -0.0344 | Acc  Gap:  0.0148\n",
            "  Loss Ratio: 0.9732 | Acc Ratio: 1.0291\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.1715 | Train Acc: 0.5511\n",
            "  Val   Loss: 1.3852 | Val   Acc: 0.4835\n",
            "  Loss Gap:   -0.2136 | Acc  Gap:  0.0676\n",
            "  Loss Ratio: 0.8458 | Acc Ratio: 1.1398\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.1026 | Train Acc: 0.5830\n",
            "  Val   Loss: 1.2583 | Val   Acc: 0.5192\n",
            "  Loss Gap:   -0.1556 | Acc  Gap:  0.0638\n",
            "  Loss Ratio: 0.8763 | Acc Ratio: 1.1229\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.0340 | Train Acc: 0.6057\n",
            "  Val   Loss: 1.1699 | Val   Acc: 0.5599\n",
            "  Loss Gap:   -0.1359 | Acc  Gap:  0.0457\n",
            "  Loss Ratio: 0.8838 | Acc Ratio: 1.0817\n",
            "Epoch 8/30\n",
            "  Train Loss: 0.9505 | Train Acc: 0.6380\n",
            "  Val   Loss: 1.2236 | Val   Acc: 0.5470\n",
            "  Loss Gap:   -0.2731 | Acc  Gap:  0.0910\n",
            "  Loss Ratio: 0.7768 | Acc Ratio: 1.1663\n",
            "  Early stopping patience: 1/5\n",
            "Epoch 9/30\n",
            "  Train Loss: 0.8828 | Train Acc: 0.6665\n",
            "  Val   Loss: 1.2462 | Val   Acc: 0.5538\n",
            "  Loss Gap:   -0.3634 | Acc  Gap:  0.1127\n",
            "  Loss Ratio: 0.7084 | Acc Ratio: 1.2034\n",
            "  Early stopping patience: 2/5\n",
            "Epoch 10/30\n",
            "  Train Loss: 0.8067 | Train Acc: 0.6929\n",
            "  Val   Loss: 1.3096 | Val   Acc: 0.5479\n",
            "  Loss Gap:   -0.5029 | Acc  Gap:  0.1450\n",
            "  Loss Ratio: 0.6160 | Acc Ratio: 1.2646\n",
            "  Early stopping patience: 3/5\n",
            "Epoch 11/30\n",
            "  Train Loss: 0.7510 | Train Acc: 0.7134\n",
            "  Val   Loss: 1.4046 | Val   Acc: 0.5597\n",
            "  Loss Gap:   -0.6537 | Acc  Gap:  0.1537\n",
            "  Loss Ratio: 0.5346 | Acc Ratio: 1.2746\n",
            "  Early stopping patience: 4/5\n",
            "Epoch 12/30\n",
            "  Train Loss: 0.6589 | Train Acc: 0.7526\n",
            "  Val   Loss: 1.4553 | Val   Acc: 0.5592\n",
            "  Loss Gap:   -0.7964 | Acc  Gap:  0.1934\n",
            "  Loss Ratio: 0.4528 | Acc Ratio: 1.3458\n",
            "  Early stopping patience: 5/5\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▂▃▃▄▄▄▅▆▇▇█</td></tr><tr><td>acc_ratio</td><td>▁▂▄▃▅▅▄▅▆▇▇█</td></tr><tr><td>epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>loss_gap</td><td>██▇▇▆▆▆▅▅▃▂▁</td></tr><tr><td>loss_ratio</td><td>██▇▇▆▆▆▅▄▃▂▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▄▅▅▆▆▇▇▇█</td></tr><tr><td>train_loss</td><td>█▆▆▅▄▄▄▃▃▂▂▁</td></tr><tr><td>val_accuracy</td><td>▁▅▅▆▅▇██████</td></tr><tr><td>val_loss</td><td>█▅▄▃▄▂▁▂▂▃▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.19339</td></tr><tr><td>acc_ratio</td><td>1.34582</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>loss_gap</td><td>-0.79636</td></tr><tr><td>loss_ratio</td><td>0.45277</td></tr><tr><td>lr</td><td>0.0015</td></tr><tr><td>train_accuracy</td><td>0.7526</td></tr><tr><td>train_loss</td><td>0.65891</td></tr><tr><td>val_accuracy</td><td>0.55921</td></tr><tr><td>val_loss</td><td>1.45527</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Channels_[32, 64, 256]_Dropout_0.25_LR_0.0015_Epochs_30_WD_1e-05</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/1nz20yst' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/1nz20yst</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_124931-1nz20yst/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_with_batchnorm(\n",
        "    x_train, y_train, x_val, y_val,\n",
        "    epochs=30,\n",
        "    lr=0.0015,\n",
        "    dropout_rate=0.25,\n",
        "    weight_decay=1e-5,\n",
        "    patience=8,\n",
        "    batch_size=128,\n",
        "    channels=[64, 128, 256],      # custom channels\n",
        "    kernel_sizes=[3, 5, 3],      # custom kernel sizes\n",
        "    strides=[1, 2, 1],           # custom strides\n",
        "    paddings=[1, 2, 1]           # custom paddings\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mpoh58KdAdQy",
        "outputId": "d0683c4c-c463-485e-bada-4b2df480d60d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_125130-wd46j7hj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/wd46j7hj' target=\"_blank\">FER_BatchNorm_Channels_[64, 128, 256]_Dropout_0.25_LR_0.0015_Epochs_30_WD_1e-05</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/wd46j7hj' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/wd46j7hj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.7524 | Train Acc: 0.2767\n",
            "  Val   Loss: 1.5698 | Val   Acc: 0.3734\n",
            "  Loss Gap:   0.1826 | Acc  Gap:  -0.0967\n",
            "  Loss Ratio: 1.1163 | Acc Ratio: 0.7411\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.5021 | Train Acc: 0.4096\n",
            "  Val   Loss: 1.3937 | Val   Acc: 0.4436\n",
            "  Loss Gap:   0.1084 | Acc  Gap:  -0.0340\n",
            "  Loss Ratio: 1.0778 | Acc Ratio: 0.9234\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.3638 | Train Acc: 0.4726\n",
            "  Val   Loss: 1.3611 | Val   Acc: 0.4650\n",
            "  Loss Gap:   0.0026 | Acc  Gap:  0.0076\n",
            "  Loss Ratio: 1.0019 | Acc Ratio: 1.0163\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.2676 | Train Acc: 0.5108\n",
            "  Val   Loss: 1.2915 | Val   Acc: 0.5134\n",
            "  Loss Gap:   -0.0238 | Acc  Gap:  -0.0026\n",
            "  Loss Ratio: 0.9815 | Acc Ratio: 0.9950\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.1879 | Train Acc: 0.5433\n",
            "  Val   Loss: 1.2354 | Val   Acc: 0.5287\n",
            "  Loss Gap:   -0.0476 | Acc  Gap:  0.0146\n",
            "  Loss Ratio: 0.9615 | Acc Ratio: 1.0276\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.1037 | Train Acc: 0.5716\n",
            "  Val   Loss: 1.2168 | Val   Acc: 0.5397\n",
            "  Loss Gap:   -0.1130 | Acc  Gap:  0.0319\n",
            "  Loss Ratio: 0.9071 | Acc Ratio: 1.0592\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.0241 | Train Acc: 0.6046\n",
            "  Val   Loss: 1.2542 | Val   Acc: 0.5312\n",
            "  Loss Gap:   -0.2301 | Acc  Gap:  0.0734\n",
            "  Loss Ratio: 0.8165 | Acc Ratio: 1.1382\n",
            "  Early stopping patience: 1/8\n",
            "Epoch 8/30\n",
            "  Train Loss: 0.9418 | Train Acc: 0.6373\n",
            "  Val   Loss: 1.3623 | Val   Acc: 0.5118\n",
            "  Loss Gap:   -0.4205 | Acc  Gap:  0.1255\n",
            "  Loss Ratio: 0.6913 | Acc Ratio: 1.2452\n",
            "  Early stopping patience: 2/8\n",
            "Epoch 9/30\n",
            "  Train Loss: 0.8533 | Train Acc: 0.6692\n",
            "  Val   Loss: 1.2197 | Val   Acc: 0.5674\n",
            "  Loss Gap:   -0.3664 | Acc  Gap:  0.1018\n",
            "  Loss Ratio: 0.6996 | Acc Ratio: 1.1794\n",
            "  Early stopping patience: 3/8\n",
            "Epoch 10/30\n",
            "  Train Loss: 0.7659 | Train Acc: 0.7058\n",
            "  Val   Loss: 1.3085 | Val   Acc: 0.5601\n",
            "  Loss Gap:   -0.5426 | Acc  Gap:  0.1457\n",
            "  Loss Ratio: 0.5853 | Acc Ratio: 1.2602\n",
            "  Early stopping patience: 4/8\n",
            "Epoch 11/30\n",
            "  Train Loss: 0.6795 | Train Acc: 0.7406\n",
            "  Val   Loss: 1.4311 | Val   Acc: 0.5479\n",
            "  Loss Gap:   -0.7516 | Acc  Gap:  0.1927\n",
            "  Loss Ratio: 0.4748 | Acc Ratio: 1.3518\n",
            "  Early stopping patience: 5/8\n",
            "Epoch 12/30\n",
            "  Train Loss: 0.5864 | Train Acc: 0.7776\n",
            "  Val   Loss: 1.7100 | Val   Acc: 0.5037\n",
            "  Loss Gap:   -1.1236 | Acc  Gap:  0.2739\n",
            "  Loss Ratio: 0.3429 | Acc Ratio: 1.5438\n",
            "  Early stopping patience: 6/8\n",
            "Epoch 13/30\n",
            "  Train Loss: 0.5252 | Train Acc: 0.8014\n",
            "  Val   Loss: 1.6394 | Val   Acc: 0.5357\n",
            "  Loss Gap:   -1.1142 | Acc  Gap:  0.2657\n",
            "  Loss Ratio: 0.3204 | Acc Ratio: 1.4960\n",
            "  Early stopping patience: 7/8\n",
            "Epoch 14/30\n",
            "  Train Loss: 0.4519 | Train Acc: 0.8320\n",
            "  Val   Loss: 1.7155 | Val   Acc: 0.5357\n",
            "  Loss Gap:   -1.2635 | Acc  Gap:  0.2963\n",
            "  Loss Ratio: 0.2634 | Acc Ratio: 1.5531\n",
            "  Early stopping patience: 8/8\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▂▃▃▃▃▄▅▅▅▆█▇█</td></tr><tr><td>acc_ratio</td><td>▁▃▃▃▃▄▄▅▅▅▆███</td></tr><tr><td>epoch</td><td>▁▂▂▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>loss_gap</td><td>██▇▇▇▇▆▅▅▄▃▂▂▁</td></tr><tr><td>loss_ratio</td><td>██▇▇▇▆▆▅▅▄▃▂▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▄▆▇▇▇▆██▇▆▇▇</td></tr><tr><td>val_loss</td><td>▆▃▃▂▁▁▂▃▁▂▄█▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.29627</td></tr><tr><td>acc_ratio</td><td>1.55306</td></tr><tr><td>epoch</td><td>14</td></tr><tr><td>loss_gap</td><td>-1.26354</td></tr><tr><td>loss_ratio</td><td>0.26345</td></tr><tr><td>lr</td><td>0.0015</td></tr><tr><td>train_accuracy</td><td>0.83198</td></tr><tr><td>train_loss</td><td>0.45194</td></tr><tr><td>val_accuracy</td><td>0.5357</td></tr><tr><td>val_loss</td><td>1.71548</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Channels_[64, 128, 256]_Dropout_0.25_LR_0.0015_Epochs_30_WD_1e-05</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/wd46j7hj' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/wd46j7hj</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_125130-wd46j7hj/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_with_batchnorm(\n",
        "    x_train, y_train, x_val, y_val,\n",
        "    epochs=30,\n",
        "    lr=0.001,\n",
        "    dropout_rate=0.4,\n",
        "    weight_decay=1e-4,  # more regularization\n",
        "    patience=8,\n",
        "    batch_size=128,\n",
        "    channels=[32, 64, 128],\n",
        "    kernel_sizes=[3, 3, 3],\n",
        "    strides=[1, 1, 1],\n",
        "    paddings=[1, 1, 1]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UeZnV8nHBeUo",
        "outputId": "b1926dca-846e-4196-d9e8-7d8066998c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_125447-o8jb3u4z</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/o8jb3u4z' target=\"_blank\">FER_BatchNorm_Channels_[32, 64, 128]_Dropout_0.4_LR_0.001_Epochs_30_WD_0.0001</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/o8jb3u4z' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/o8jb3u4z</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.7395 | Train Acc: 0.2954\n",
            "  Val   Loss: 1.5212 | Val   Acc: 0.4101\n",
            "  Loss Gap:   0.2183 | Acc  Gap:  -0.1148\n",
            "  Loss Ratio: 1.1435 | Acc Ratio: 0.7202\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.5247 | Train Acc: 0.4091\n",
            "  Val   Loss: 1.3925 | Val   Acc: 0.4617\n",
            "  Loss Gap:   0.1321 | Acc  Gap:  -0.0526\n",
            "  Loss Ratio: 1.0949 | Acc Ratio: 0.8861\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.4227 | Train Acc: 0.4516\n",
            "  Val   Loss: 1.3980 | Val   Acc: 0.4518\n",
            "  Loss Gap:   0.0246 | Acc  Gap:  -0.0002\n",
            "  Loss Ratio: 1.0176 | Acc Ratio: 0.9996\n",
            "  Early stopping patience: 1/8\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.3471 | Train Acc: 0.4789\n",
            "  Val   Loss: 1.3045 | Val   Acc: 0.5059\n",
            "  Loss Gap:   0.0427 | Acc  Gap:  -0.0270\n",
            "  Loss Ratio: 1.0327 | Acc Ratio: 0.9466\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.2925 | Train Acc: 0.5047\n",
            "  Val   Loss: 1.2411 | Val   Acc: 0.5280\n",
            "  Loss Gap:   0.0514 | Acc  Gap:  -0.0234\n",
            "  Loss Ratio: 1.0414 | Acc Ratio: 0.9558\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.2548 | Train Acc: 0.5195\n",
            "  Val   Loss: 1.2130 | Val   Acc: 0.5437\n",
            "  Loss Gap:   0.0418 | Acc  Gap:  -0.0242\n",
            "  Loss Ratio: 1.0345 | Acc Ratio: 0.9554\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.2136 | Train Acc: 0.5358\n",
            "  Val   Loss: 1.2037 | Val   Acc: 0.5352\n",
            "  Loss Gap:   0.0099 | Acc  Gap:  0.0006\n",
            "  Loss Ratio: 1.0082 | Acc Ratio: 1.0011\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.1666 | Train Acc: 0.5535\n",
            "  Val   Loss: 1.1992 | Val   Acc: 0.5430\n",
            "  Loss Gap:   -0.0326 | Acc  Gap:  0.0105\n",
            "  Loss Ratio: 0.9728 | Acc Ratio: 1.0194\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.1214 | Train Acc: 0.5695\n",
            "  Val   Loss: 1.2494 | Val   Acc: 0.5331\n",
            "  Loss Gap:   -0.1280 | Acc  Gap:  0.0364\n",
            "  Loss Ratio: 0.8976 | Acc Ratio: 1.0682\n",
            "  Early stopping patience: 1/8\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.1016 | Train Acc: 0.5829\n",
            "  Val   Loss: 1.2774 | Val   Acc: 0.5273\n",
            "  Loss Gap:   -0.1758 | Acc  Gap:  0.0555\n",
            "  Loss Ratio: 0.8624 | Acc Ratio: 1.1053\n",
            "  Early stopping patience: 2/8\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.0530 | Train Acc: 0.5987\n",
            "  Val   Loss: 1.1418 | Val   Acc: 0.5731\n",
            "  Loss Gap:   -0.0888 | Acc  Gap:  0.0256\n",
            "  Loss Ratio: 0.9222 | Acc Ratio: 1.0446\n",
            "Epoch 12/30\n",
            "  Train Loss: 1.0090 | Train Acc: 0.6137\n",
            "  Val   Loss: 1.2126 | Val   Acc: 0.5606\n",
            "  Loss Gap:   -0.2036 | Acc  Gap:  0.0531\n",
            "  Loss Ratio: 0.8321 | Acc Ratio: 1.0946\n",
            "  Early stopping patience: 1/8\n",
            "Epoch 13/30\n",
            "  Train Loss: 0.9735 | Train Acc: 0.6318\n",
            "  Val   Loss: 1.1456 | Val   Acc: 0.5805\n",
            "  Loss Gap:   -0.1721 | Acc  Gap:  0.0514\n",
            "  Loss Ratio: 0.8498 | Acc Ratio: 1.0885\n",
            "  Early stopping patience: 2/8\n",
            "Epoch 14/30\n",
            "  Train Loss: 0.9428 | Train Acc: 0.6426\n",
            "  Val   Loss: 1.2233 | Val   Acc: 0.5406\n",
            "  Loss Gap:   -0.2805 | Acc  Gap:  0.1020\n",
            "  Loss Ratio: 0.7707 | Acc Ratio: 1.1887\n",
            "  Early stopping patience: 3/8\n",
            "Epoch 15/30\n",
            "  Train Loss: 0.8963 | Train Acc: 0.6618\n",
            "  Val   Loss: 1.1685 | Val   Acc: 0.5726\n",
            "  Loss Gap:   -0.2722 | Acc  Gap:  0.0892\n",
            "  Loss Ratio: 0.7671 | Acc Ratio: 1.1558\n",
            "  Early stopping patience: 4/8\n",
            "Epoch 16/30\n",
            "  Train Loss: 0.8681 | Train Acc: 0.6684\n",
            "  Val   Loss: 1.2811 | Val   Acc: 0.5371\n",
            "  Loss Gap:   -0.4130 | Acc  Gap:  0.1313\n",
            "  Loss Ratio: 0.6776 | Acc Ratio: 1.2444\n",
            "  Early stopping patience: 5/8\n",
            "Epoch 17/30\n",
            "  Train Loss: 0.8110 | Train Acc: 0.6936\n",
            "  Val   Loss: 1.2780 | Val   Acc: 0.5702\n",
            "  Loss Gap:   -0.4670 | Acc  Gap:  0.1234\n",
            "  Loss Ratio: 0.6346 | Acc Ratio: 1.2165\n",
            "  Early stopping patience: 6/8\n",
            "Epoch 18/30\n",
            "  Train Loss: 0.7837 | Train Acc: 0.7050\n",
            "  Val   Loss: 1.2383 | Val   Acc: 0.5752\n",
            "  Loss Gap:   -0.4546 | Acc  Gap:  0.1297\n",
            "  Loss Ratio: 0.6329 | Acc Ratio: 1.2255\n",
            "  Early stopping patience: 7/8\n",
            "Epoch 19/30\n",
            "  Train Loss: 0.7583 | Train Acc: 0.7132\n",
            "  Val   Loss: 1.2771 | Val   Acc: 0.5848\n",
            "  Loss Gap:   -0.5188 | Acc  Gap:  0.1283\n",
            "  Loss Ratio: 0.5937 | Acc Ratio: 1.2195\n",
            "  Early stopping patience: 8/8\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▁▃▄▃▄▄▄▅▅▆▅▆▆▇▇████</td></tr><tr><td>acc_ratio</td><td>▁▃▅▄▄▄▅▅▆▆▅▆▆▇▇████</td></tr><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>loss_gap</td><td>█▇▆▆▆▆▆▆▅▄▅▄▄▃▃▂▁▂▁</td></tr><tr><td>loss_ratio</td><td>█▇▆▇▇▇▆▆▅▄▅▄▄▃▃▂▂▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▃▅▆▆▆▆▆▆█▇█▆█▆▇██</td></tr><tr><td>val_loss</td><td>█▆▆▄▃▂▂▂▃▄▁▂▁▃▁▄▄▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.12834</td></tr><tr><td>acc_ratio</td><td>1.21945</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>loss_gap</td><td>-0.51884</td></tr><tr><td>loss_ratio</td><td>0.59375</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>train_accuracy</td><td>0.71315</td></tr><tr><td>train_loss</td><td>0.7583</td></tr><tr><td>val_accuracy</td><td>0.58481</td></tr><tr><td>val_loss</td><td>1.27714</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Channels_[32, 64, 128]_Dropout_0.4_LR_0.001_Epochs_30_WD_0.0001</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/o8jb3u4z' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/o8jb3u4z</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_125447-o8jb3u4z/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model_with_batchnorm(\n",
        "    x_train, y_train, x_val, y_val,\n",
        "    epochs=30,\n",
        "    lr=0.0015,\n",
        "    dropout_rate=0.4,\n",
        "    weight_decay=1e-4,\n",
        "    patience=8,\n",
        "    batch_size=256,\n",
        "    channels=[64, 128, 256],\n",
        "    kernel_sizes=[3, 3, 3],\n",
        "    strides=[1, 1, 1],\n",
        "    paddings=[1, 1, 1]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aOH6TuucB_x8",
        "outputId": "c20f8818-1345-4f11-88b0-597e4be71019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250604_125723-kxtu31s6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nkhar21-student/ML_4/runs/kxtu31s6' target=\"_blank\">FER_BatchNorm_Channels_[64, 128, 256]_Dropout_0.4_LR_0.0015_Epochs_30_WD_0.0001</a></strong> to <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nkhar21-student/ML_4/runs/kxtu31s6' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/kxtu31s6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  Train Loss: 1.9055 | Train Acc: 0.2363\n",
            "  Val   Loss: 1.7441 | Val   Acc: 0.2727\n",
            "  Loss Gap:   0.1613 | Acc  Gap:  -0.0364\n",
            "  Loss Ratio: 1.0925 | Acc Ratio: 0.8666\n",
            "Epoch 2/30\n",
            "  Train Loss: 1.7133 | Train Acc: 0.3054\n",
            "  Val   Loss: 1.6393 | Val   Acc: 0.3622\n",
            "  Loss Gap:   0.0740 | Acc  Gap:  -0.0569\n",
            "  Loss Ratio: 1.0451 | Acc Ratio: 0.8429\n",
            "Epoch 3/30\n",
            "  Train Loss: 1.5743 | Train Acc: 0.3819\n",
            "  Val   Loss: 1.4837 | Val   Acc: 0.4370\n",
            "  Loss Gap:   0.0906 | Acc  Gap:  -0.0551\n",
            "  Loss Ratio: 1.0611 | Acc Ratio: 0.8739\n",
            "Epoch 4/30\n",
            "  Train Loss: 1.4837 | Train Acc: 0.4237\n",
            "  Val   Loss: 1.4445 | Val   Acc: 0.4394\n",
            "  Loss Gap:   0.0393 | Acc  Gap:  -0.0157\n",
            "  Loss Ratio: 1.0272 | Acc Ratio: 0.9644\n",
            "Epoch 5/30\n",
            "  Train Loss: 1.4230 | Train Acc: 0.4488\n",
            "  Val   Loss: 1.3780 | Val   Acc: 0.4906\n",
            "  Loss Gap:   0.0449 | Acc  Gap:  -0.0418\n",
            "  Loss Ratio: 1.0326 | Acc Ratio: 0.9148\n",
            "Epoch 6/30\n",
            "  Train Loss: 1.3796 | Train Acc: 0.4648\n",
            "  Val   Loss: 1.3626 | Val   Acc: 0.4807\n",
            "  Loss Gap:   0.0170 | Acc  Gap:  -0.0159\n",
            "  Loss Ratio: 1.0125 | Acc Ratio: 0.9670\n",
            "Epoch 7/30\n",
            "  Train Loss: 1.3537 | Train Acc: 0.4752\n",
            "  Val   Loss: 1.3008 | Val   Acc: 0.5049\n",
            "  Loss Gap:   0.0528 | Acc  Gap:  -0.0297\n",
            "  Loss Ratio: 1.0406 | Acc Ratio: 0.9412\n",
            "Epoch 8/30\n",
            "  Train Loss: 1.2945 | Train Acc: 0.4968\n",
            "  Val   Loss: 1.3194 | Val   Acc: 0.5045\n",
            "  Loss Gap:   -0.0249 | Acc  Gap:  -0.0077\n",
            "  Loss Ratio: 0.9812 | Acc Ratio: 0.9848\n",
            "  Early stopping patience: 1/8\n",
            "Epoch 9/30\n",
            "  Train Loss: 1.2612 | Train Acc: 0.5180\n",
            "  Val   Loss: 1.2786 | Val   Acc: 0.5319\n",
            "  Loss Gap:   -0.0174 | Acc  Gap:  -0.0138\n",
            "  Loss Ratio: 0.9864 | Acc Ratio: 0.9740\n",
            "Epoch 10/30\n",
            "  Train Loss: 1.2351 | Train Acc: 0.5258\n",
            "  Val   Loss: 1.2312 | Val   Acc: 0.5481\n",
            "  Loss Gap:   0.0039 | Acc  Gap:  -0.0223\n",
            "  Loss Ratio: 1.0032 | Acc Ratio: 0.9594\n",
            "Epoch 11/30\n",
            "  Train Loss: 1.2131 | Train Acc: 0.5374\n",
            "  Val   Loss: 1.2387 | Val   Acc: 0.5388\n",
            "  Loss Gap:   -0.0256 | Acc  Gap:  -0.0015\n",
            "  Loss Ratio: 0.9793 | Acc Ratio: 0.9973\n",
            "  Early stopping patience: 1/8\n",
            "Epoch 12/30\n",
            "  Train Loss: 1.1766 | Train Acc: 0.5457\n",
            "  Val   Loss: 1.2139 | Val   Acc: 0.5441\n",
            "  Loss Gap:   -0.0373 | Acc  Gap:  0.0017\n",
            "  Loss Ratio: 0.9693 | Acc Ratio: 1.0031\n",
            "Epoch 13/30\n",
            "  Train Loss: 1.1189 | Train Acc: 0.5689\n",
            "  Val   Loss: 1.1833 | Val   Acc: 0.5557\n",
            "  Loss Gap:   -0.0644 | Acc  Gap:  0.0132\n",
            "  Loss Ratio: 0.9456 | Acc Ratio: 1.0238\n",
            "Epoch 14/30\n",
            "  Train Loss: 1.0824 | Train Acc: 0.5824\n",
            "  Val   Loss: 1.1998 | Val   Acc: 0.5529\n",
            "  Loss Gap:   -0.1174 | Acc  Gap:  0.0294\n",
            "  Loss Ratio: 0.9021 | Acc Ratio: 1.0532\n",
            "  Early stopping patience: 1/8\n",
            "Epoch 15/30\n",
            "  Train Loss: 1.0560 | Train Acc: 0.5970\n",
            "  Val   Loss: 1.1589 | Val   Acc: 0.5580\n",
            "  Loss Gap:   -0.1030 | Acc  Gap:  0.0390\n",
            "  Loss Ratio: 0.9111 | Acc Ratio: 1.0699\n",
            "Epoch 16/30\n",
            "  Train Loss: 1.0257 | Train Acc: 0.6060\n",
            "  Val   Loss: 1.1861 | Val   Acc: 0.5636\n",
            "  Loss Gap:   -0.1604 | Acc  Gap:  0.0425\n",
            "  Loss Ratio: 0.8648 | Acc Ratio: 1.0754\n",
            "  Early stopping patience: 1/8\n",
            "Epoch 17/30\n",
            "  Train Loss: 0.9867 | Train Acc: 0.6202\n",
            "  Val   Loss: 1.2166 | Val   Acc: 0.5486\n",
            "  Loss Gap:   -0.2299 | Acc  Gap:  0.0716\n",
            "  Loss Ratio: 0.8110 | Acc Ratio: 1.1306\n",
            "  Early stopping patience: 2/8\n",
            "Epoch 18/30\n",
            "  Train Loss: 0.9582 | Train Acc: 0.6313\n",
            "  Val   Loss: 1.2899 | Val   Acc: 0.5284\n",
            "  Loss Gap:   -0.3317 | Acc  Gap:  0.1029\n",
            "  Loss Ratio: 0.7429 | Acc Ratio: 1.1947\n",
            "  Early stopping patience: 3/8\n",
            "Epoch 19/30\n",
            "  Train Loss: 0.9058 | Train Acc: 0.6517\n",
            "  Val   Loss: 1.1684 | Val   Acc: 0.5744\n",
            "  Loss Gap:   -0.2626 | Acc  Gap:  0.0774\n",
            "  Loss Ratio: 0.7752 | Acc Ratio: 1.1347\n",
            "  Early stopping patience: 4/8\n",
            "Epoch 20/30\n",
            "  Train Loss: 0.8596 | Train Acc: 0.6718\n",
            "  Val   Loss: 1.2137 | Val   Acc: 0.5594\n",
            "  Loss Gap:   -0.3542 | Acc  Gap:  0.1124\n",
            "  Loss Ratio: 0.7082 | Acc Ratio: 1.2009\n",
            "  Early stopping patience: 5/8\n",
            "Epoch 21/30\n",
            "  Train Loss: 0.8045 | Train Acc: 0.6889\n",
            "  Val   Loss: 1.2327 | Val   Acc: 0.5662\n",
            "  Loss Gap:   -0.4283 | Acc  Gap:  0.1227\n",
            "  Loss Ratio: 0.6526 | Acc Ratio: 1.2167\n",
            "  Early stopping patience: 6/8\n",
            "Epoch 22/30\n",
            "  Train Loss: 0.7820 | Train Acc: 0.6992\n",
            "  Val   Loss: 1.3112 | Val   Acc: 0.5120\n",
            "  Loss Gap:   -0.5292 | Acc  Gap:  0.1872\n",
            "  Loss Ratio: 0.5964 | Acc Ratio: 1.3656\n",
            "  Early stopping patience: 7/8\n",
            "Epoch 23/30\n",
            "  Train Loss: 0.7539 | Train Acc: 0.7101\n",
            "  Val   Loss: 1.2692 | Val   Acc: 0.5637\n",
            "  Loss Gap:   -0.5153 | Acc  Gap:  0.1464\n",
            "  Loss Ratio: 0.5940 | Acc Ratio: 1.2597\n",
            "  Early stopping patience: 8/8\n",
            "  ⛔ Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>▂▁▁▂▁▂▂▂▂▂▃▃▃▃▄▄▅▆▅▆▆█▇</td></tr><tr><td>acc_ratio</td><td>▁▁▁▃▂▃▂▃▃▃▃▃▃▄▄▄▅▆▅▆▆█▇</td></tr><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>loss_gap</td><td>█▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▄▃▄▃▂▁▁</td></tr><tr><td>loss_ratio</td><td>█▇█▇▇▇▇▆▇▇▆▆▆▅▅▅▄▃▄▃▂▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▂▃▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▅▅▆▆▆▆▇▇▇▇████▇▇███▇█</td></tr><tr><td>val_loss</td><td>█▇▅▄▄▃▃▃▂▂▂▂▁▁▁▁▂▃▁▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_gap</td><td>0.14641</td></tr><tr><td>acc_ratio</td><td>1.25971</td></tr><tr><td>epoch</td><td>23</td></tr><tr><td>loss_gap</td><td>-0.51527</td></tr><tr><td>loss_ratio</td><td>0.59402</td></tr><tr><td>lr</td><td>0.0015</td></tr><tr><td>train_accuracy</td><td>0.71015</td></tr><tr><td>train_loss</td><td>0.75394</td></tr><tr><td>val_accuracy</td><td>0.56374</td></tr><tr><td>val_loss</td><td>1.26921</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">FER_BatchNorm_Channels_[64, 128, 256]_Dropout_0.4_LR_0.0015_Epochs_30_WD_0.0001</strong> at: <a href='https://wandb.ai/nkhar21-student/ML_4/runs/kxtu31s6' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4/runs/kxtu31s6</a><br> View project at: <a href='https://wandb.ai/nkhar21-student/ML_4' target=\"_blank\">https://wandb.ai/nkhar21-student/ML_4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250604_125723-kxtu31s6/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}